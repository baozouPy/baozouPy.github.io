<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[第四周：深层神经网络(Deep Neural Networks)(Course 1)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E5%9B%9B%E5%91%A8%EF%BC%9A%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Deep-Neural-Networks%2F</url>
    <content type="text"><![CDATA[4.1 深层神经网络（Deep L-layer neural network） $L-layer\quad NN$，则包含了L-1个隐藏层，最后的L层是输出层 $a^{[l]}$和W^{[l]}中的上标l都是从1开始的，l=1,\cdots,L 输入x记为a^{[0]}​​，把输出层\hat y记为a^{[L]} $X$：(12288, 209)(with m=209 examples) Shape of W Shape of b Activation Shape of Activation Layer 1 (n^{[1]},12288) (n^{[1]},1) Z^{[1]} = W^{[1]} X + b^{[1]} (n^{[1]},209) Layer 2 (n^{[2]}, n^{[1]}) (n^{[2]},1) Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} (n^{[2]},209) \vdots \vdots \vdots \vdots \vdots Layer L-1 (n^{[L-1]}, n^{[L-2]}) (n^{[L-1]}, 1) Z^{[L-1]} = W^{[L-1]} A^{[L-2]} + b^{[L-1]} (n^{[L-1]}, 209) Layer L (n^{[L]}, n^{[L-1]}) (n^{[L]}, 1) Z^{[L]} = W^{[L]} A^{[L-1]} + b^{[L]} (n^{[L]}, 209) 4.2 前向传播和反向传播（Forward and backward propagation）正向传播过程 z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]} a^{[l]}=g^{[l]}(z^{[l]})$m$个训练样本，向量化形式为： Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} A^{[l]}=g^{[l]}(Z^{[l]})反向传播过程 dz^{[l]}=da^{[l]}\ast g^{[l]'}(z^{[l]}) dW^{[l]}=dz^{[l]}\cdot {a^{[l-1]}}^T db^{[l]}=dz^{[l]} da^{[l-1]}=W^{[l]T}\cdot dz^{[l]}得到： dz^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}\ast g^{[l]'}(z^{[l]})$m$个训练样本，向量化形式为： dZ^{[l]}=dA^{[l]}\ast g^{[l]'}(Z^{[l]}) dW^{[l]}=\frac1mdZ^{[l]}\cdot A^{[l-1]T} db^{[l]}=\frac1mnp.sum(dZ^{[l]},axis=1,keepdim=True) dA^{[l-1]}=W^{[l]T}\cdot dZ^{[l]} dZ^{[l]}=W^{[l+1]T}\cdot dZ^{[l+1]}\ast g^{[l]'}(Z^{[l]}) 4.3 深层网络中的前向传播（Forward propagation in a Deep Network ）对于第l层，其正向传播过程的Z^{[l]}和A^{[l]}可以表示为： Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} A^{[l]}=g^{[l]}(Z^{[l]})其中l=1,\cdots,L 4.4 为什么使用深层表示？（Why deep representations?）人脸识别经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。 随着层数由浅到深，神经网络提取的特征也是从边缘到局部特征到整体，由简单到复杂。如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。 语音识别模型浅层的神经元能够检测一些简单的音调，较深的神经元能够检测出基本的音素，更深的神经元就能够检测出单词信息。如果网络够深，还能对短语、句子进行检测。 神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大 深层网络另外一个优点:减少神经元个数，从而减少计算量 使用电路理论，计算逻辑输出： y=x_1\oplus x_2\oplus x_3\oplus\cdots\oplus x_n对于这个逻辑运算，深度网络的结构是每层将前一层的两两单元进行异或，最后得到一个输出 整个深度网络的层数是log_2(n)，不包含输入层。总共使用的神经元个数为： 1+2+\cdots+2^{log_2(n)-1}=1\cdot\frac{1-2^{log_2(n)}}{1-2}=2^{log_2(n)}-1=n-1输入个数是n，这种深层网络所需的神经元个数仅仅是n-1个 如果不用深层网络，使用单个隐藏层，需要的神经元个数将是指数级别那么大。由于包含了所有的逻辑位（0和1），则需要2^{n-1}个神经元 处理同一逻辑问题，深层网络所需的神经元个数比浅层网络要少很多 4.5 搭建神经网络块（Building blocks of deep neural networks）第l层的流程块图 对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示： 4.6 参数 VS 超参数（Parameters vs Hyperparameters）神经网络中的参数是W^{[l]}和b^{[l]} 超参数则是例如学习速率\alpha，训练迭代次数N，神经网络层数L，各层神经元个数n^{[l]}，激活函数g(z)等 叫做超参数的原因是它们决定了参数W^{[l]}和b^{[l]}的值 如何设置最优的超参数： 通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第三周：浅层神经网络(Shallow neural networks)(Course 1)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%B8%89%E5%91%A8%EF%BC%9A%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Shallow-neural-networks%2F</url>
    <content type="text"><![CDATA[3.1 神经网络概述（Neural Network Overview） 3.2 神经网络的表示（Neural Network Representation ）单隐藏层神经网络就是典型的浅层（shallow）神经网络 单隐藏层神经网络也被称为两层神经网络（2 layer NN） 第l层的权重W^{[l]}维度的行等于l层神经元的个数，列等于l-1层神经元的个数；第i层常数项b^{[l]}维度的行等于l层神经元的个数，列始终为1 3.3 计算一个神经网络的输出（Computing a Neural Network’s output ）两层神经网络可以看成是逻辑回归再重复计算一次 逻辑回归的正向计算可以分解成计算z和a的两部分： z=w^Tx+b a=\sigma(z) 两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算 z^{[1]}=W^{[1]}x+b^{[1]} a^{[1]}=\sigma(z^{[1]}) z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} a^{[2]}=\sigma(z^{[2]}) 3.4 多样本向量化（Vectorizing across multiple examples ）for循环来求解其正向输出： for i = 1 to m: \begin{aligned}&z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]}\\&a^{[1](i)}=\sigma(z^{[1](i)})\\&z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]} \\&a^{[2](i)}=\sigma(z^{[2](i)})\end{aligned}矩阵运算的形式： Z^{[1]}=W^{[1]}X+b^{[1]} A^{[1]}=\sigma(Z^{[1]}) Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]} A^{[2]}=\sigma(Z^{[2]})行表示神经元个数，列表示样本数目m 3.5 激活函数（Activation functions） sigmoid函数 tanh函数 ReLU函数 Leaky ReLU函数 对于隐藏层的激活函数，tanh函数要比sigmoid函数表现更好一些。因为tanh函数的取值范围在[-1,+1]之间，隐藏层的输出被限定在[-1,+1]之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化（均值为0）的效果 对于输出层的激活函数，因为二分类问题的输出取值为\{0,+1\}，所以一般会选择sigmoid作为激活函数 选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点 Leaky$$ $$ReLU$$激活函数，能够保证$$z$$小于零时梯度不为$$03.6 为什么需要（ 非线性激活函数？（why need a nonlinear activation function?）假设所有的激活函数都是线性的，直接令激活函数g(z)=z，即a=z z^{[1]}=W^{[1]}x+b^{[1]} a^{[1]}=z^{[1]} z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} a^{[2]}=z^{[2]} a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}=(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})=W'x+b'多层隐藏层的神经网络，如果使用线性函数作为激活函数，最终的输出仍然是输入x的线性模型。这样的话神经网络就没有任何作用了。因此，隐藏层的激活函数必须要是非线性的 如果是预测问题而不是分类问题，输出y是连续的情况下，输出层的激活函数可以使用线性函数。如果输出y恒为正值，则也可以使用ReLU激活函数 3.7 激活函数的导数（Derivatives of activation functions ）$sigmoid$函数的导数： g(z)=\frac{1}{1+e^{(-z)}} g'(z)=\frac{d}{dz}g(z)=g(z)(1-g(z))=a(1-a)$tanh$函数的导数： g(z)=\frac{e^{(z)}-e^{(-z)}}{e^{(z)}+e^{(-z)}} g'(z)=\frac{d}{dz}g(z)=1-(g(z))^2=1-a^2$ReLU$函数的导数： g(z)=max(0,z) x = \begin{cases} 0 &\text{if } z < 0 \\ 1 &\text{if } z \geq 0 \end{cases}$Leaky ReLU$函数： g(z)=max(0.01z,z) g'(z) = \begin{cases} 0.01 &\text{if } z < 0 \\ 1 &\text{if } z \geq 0 \end{cases}3.8 神经网络的梯度下降（Gradient descent for neural networks） dZ^{[2]}=A^{[2]}-Y dW^{[2]}=\frac1mdZ^{[2]}A^{[1]T} db^{[2]}=\frac1mnp.sum(dZ^{[2]},axis=1,keepdim=True) dZ^{[1]}=W^{[2]T}dZ^{[2]}\ast g'(Z^{[1]}) dW^{[1]}=\frac1mdZ^{[1]}X^T db^{[1]}=\frac1mnp.sum(dZ^{[1]},axis=1,keepdim=True)3.9 （选修）直观理解反向传播（Backpropagation intuition ）单个训练样本反向过程可以根据梯度计算方法逐一推导： dz^{[2]}=a^{[2]}-y dW^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial W^{[2]}}=dz^{[2]}a^{[1]T} db^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial b^{[2]}}=dz^{[2]}\cdot 1=dz^{[2]} dz^{[1]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial a^{[1]}}\cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}=W^{[2]T}dz^{[2]}\ast g^{[1]'}(z^{[1]}) dW^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial W^{[1]}}=dz^{[1]}x^T db^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}=dz^{[1]}\cdot 1=dz^{[1]} 浅层神经网络（包含一个隐藏层），m个训练样本的正向传播过程和反向传播过程分别包含了6个表达式，其向量化矩阵形式如下图所示： 3.10 随机初始化（Random Initialization）神经网络模型中的参数权重W不能全部初始化为零 如果权重W^{[1]}和W^{[2]}都初始化为零，即： W^{[1]}= \left[ \begin{matrix} 0 & 0 \\ 0 & 0 \end{matrix} \right] W^{[2]}= \left[ \begin{matrix} 0 & 0 \end{matrix} \right]这样使得隐藏层第一个神经元的输出等于第二个神经元的输出，即a_1^{[1]}=a_2^{[1]}。经过推导得到dz_1^{[1]}=dz_2^{[1]}，dW_1^{[1]}=dW_2^{[1]}，这样的结果是隐藏层两个神经元对应的权重行向量W_1^{[1]}和W_2^{[1]}每次迭代更新都会得到完全相同的结果，W_1^{[1]}始终等于W_2^{[1]}，完全对称。这样隐藏层设置多个神经元就没有任何意义 权重W全部初始化为零带来的问题称为symmetry breaking problem 随机初始化： 1234W_1 = np.random.randn((2,2))*0.01b_1 = np.zero((2,1))W_2 = np.random.randn((1,2))*0.01b_2 = 0 让W比较小，是因为如果使用sigmoid函数或者tanh函数作为激活函数的话，W比较小，得到的|z|也比较小（靠近零点），而零点区域的梯度比较大，这样能大大提高梯度下降算法的更新速度，尽快找到全局最优解 如果W较大，得到的|z|也比较大，附近曲线平缓，梯度较小，训练过程会慢很多 如果激活函数是ReLU或者Leaky ReLU函数，则不需要考虑这个问题 如果输出层是sigmoid函数，则对应的权重W最好初始化到比较小的值]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning.ai深度学习课程笔记]]></title>
    <url>%2F2019%2F02%2F27%2FDeepLearning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[笔记中没有涵盖所有的视频内容，主要是我不懂或者觉得比较重要的内容，学生我水平有限，如笔记中有知识点、公式、代码错误，还烦请指出 Andrew Ng（吴恩达）的公开信： 朋友们， 我在做三个全新的AI项目。现在，我十分兴奋地宣布其中的第一个：deeplearning.ai，一个立志于扩散AI知识的项目。该项目在Coursera上发布了一系列深度学习课程，这些课程将帮助你掌握深度学习、对它高效地应用，并打造属于你自己的AI事业。 AI是新一轮电力革命 就像一百年前电力改造了每个主流行业，当今的AI技术在做着相同的事。好几个大型科技公司都设立了AI部门，用AI革新他们的业务。接下来的几年里，各个行业、规模大小各不相同的公司也都会意识到——-在由AI驱动的未来，他们必须成为其中的一份子。 创建由AI驱动的社会 我希望，我们可以建立一个由AI驱动的社会：让每个人看得起病，给每个孩子个性化的教育，让所有人都能坐上价格亲民的自动驾驶汽车，并向男人和女人提供有意义的工作。总而言之，是一个让每个人的生活变得更好的社会。 但是，任何一个公司都不可能单独完成这些任务。就像现在每一个计算机专业的毕业生都知道怎么用云，将来，每个程序员也必须懂得怎么用AI。用深度学习改善人类生活的方法有数百万种，社会也需要数百万个人——即来自世界各国的你们，来创造出了不起的AI系统。不管你是加州的一个软件工程师，一名中国的研究员，还是印度的ML工程师，我希望都能用深度学习来解决世界上的各种挑战。 你会学到什么 任何一个掌握了机器学习基础知识的人，都可以学习这五门系列课程，它们组成了Coursera的全新深度学习专业。 你会学到深度学习的基础，理解如何创建神经网络，学习怎么成功地领导机器学习项目。你会学习卷积神经网络、RNNs、LSTM、Adam、Dropout、BatchNorm、Xavier/He initialization以及更多。学习过程中，你会接触到医疗、自动驾驶、读手语、音乐生成、自然语言处理的案例。 你不仅会掌握深度学习理论，还会看到它是怎样在行业应用落地的。你会在Python和TensorFlow里试验这些想法，你还会听到各位深度学习领袖人物的意见，他们会分享各自的学习经历，并提供职业规划建议。 当你拿到Coursera的深度学习专业证书，就可以自信得把“深度学习”四个字写进你的简历。 加入我，建立一个由AI驱动的社会 从2011年到现在，已经有180万人加入了我的机器学习课程。当时，我和四名斯坦福的学生发布了这门课程，它随即成为了Coursera的第一门公开课。那之后，我受到你们之中许多人的启发——当我看到你们是如何努力地理解机器学习，开发优秀的AI系统，并开启令人惊艳的事业。 我希望深度学习专业能帮助你们实现更了不起的事，让你们为社会贡献更多，在职业道路上走得更远。 我希望大家和我一道，建立一个由AI驱动的社会。 我会通知大家另外两个项目的进展，并不断探索，为全世界AI社区的每一个人提供更多支持的途径。 Sincerely， 吴恩达 吴恩达与deeplearning.ai团队]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周：神经网络的编程基础(Basics of Neural Network programming)(Course 1)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-Basics-of-Neural-Network-programming%2F</url>
    <content type="text"><![CDATA[2.1 二分类(Binary Classification)逻辑回归模型一般用来解决二分类（Binary Classification）问题 二分类就是输出y只有{0,1}两个离散值（也有{-1,1}的情况） 彩色图片包含RGB三个通道。例如该cat图片的尺寸为（64，64，3） 在神经网络模型中，首先要将图片输入x（维度是（64，64，3））转化为一维的特征向量（featurevector）。方法是每个通道一行一行取，再连接起来。则转化后的输入特征向量维度为（12288，1）。此特征向量x是列向量，维度一般记为n_x 如果训练样本共有m张图片，那么整个训练样本X组成了矩阵，维度是（n_x,m), n_x代表了每个样本X^{(i)}特征个数，列m代表了样本个数,输出Y组成了一维的行向量，维度是（1，m） ) 2.2 逻辑回归(Logistic Regression)逻辑回归中，预测值\hat y=P(y=1 | x)表示为1的概率，取值范围在[0,1]之间 使用线性模型，引入参数w和b。权重w的维度是（n_x，1），b是一个常数项 逻辑回归的预测输出可以完整写成： \hat y = Sigmoid(w^Tx+b)=\sigma(w^Tx+b)Sigmoid函数的一阶导数可以用其自身表示： \sigma'(z)=\sigma(z)(1-\sigma(z)) 2.3 逻辑回归的代价函数（Logistic Regression Cost Function）单个样本的cost function用Loss function来表示，使用平方误差（squared error）： L(\hat y,y)=\frac12(\hat y-y)^2逻辑回归一般不使用平方误差来作为Loss function。原因是这种Loss function一般是non-convex的。 non-convex函数在使用梯度下降算法时，容易得到局部最小值（localminimum），即局部最优化。而最优化的目标是计算得到全局最优化（Global optimization），因此一般选择的Loss function应该是convex的 构建另外一种Loss function(针对单个样本)，且是convex的： L(\hat y,y)=-(ylog\ \hat y+(1-y)log\ (1-\hat y))当y=1时，L(\hat y,y)=-\log \hat y，如果\hat y越接近1，L(\hat y,y)\approx 0，表示预测效果越好；如果\hat y越接近0，L(\hat y,y)\approx +\infty，表示预测效果越差 当y=0时，L(\hat y,y)=-\log(1- \hat y)，如果\hat y越接近0，L(\hat y,y)\approx 0，表示预测效果越好；如果\hat y越接近1，L(\hat y,y)\approx +\infty，表示预测效果越差 Cost function是m个样本的Loss function的平均值，反映了m个样本的预测输出\hat y与真实样本输出y的平均接近程度: J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac1m\sum_{i=1}^m[y^{(i)}log\ \hat y^{(i)}+(1-y^{(i)})log\ (1-\hat y^{(i)})] 2.4 逻辑回归的梯度下降（Logistic Regression Gradient Descent）对单个样本而言，逻辑回归Loss function表达式如下： z=w^Tx+b \hat y=a=\sigma(z) L(a,y)=-(y\log(a)+(1-y)\log(1-a)) 计算该逻辑回归的反向传播过程: da=\frac{\partial L}{\partial a}=-\frac ya+\frac{1-y}{1-a} dz=\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a}\cdot \frac{\partial a}{\partial z}=(-\frac ya+\frac{1-y}{1-a})\cdot a(1-a)=a-y dw_1=\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w_1}=x_1\cdot dz=x_1(a-y) dw_2=\frac{\partial L}{\partial w_2}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w_2}=x_2\cdot dz=x_2(a-y) db=\frac{\partial L}{\partial b}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial b}=1\cdot dz=a-y则梯度下降算法可表示为： w_1:=w_1-\alpha\ dw_1 w_2:=w_2-\alpha\ dw_2 b:=b-\alpha\ db 2.5 梯度下降的例子(Gradient Descent on m Examples)m个样本的Cost function表达式如下： z^{(i)}=w^Tx^{(i)}+b \hat y^{(i)}=a^{(i)}=\sigma(z^{(i)}) J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac1m\sum_{i=1}^m[y^{(i)}log\ \hat y^{(i)}+(1-y^{(i)})log\ (1-\hat y^{(i)})]Cost function关于w和b的偏导数可以写成和平均的形式： dw_1=\frac1m\sum_{i=1}^mx_1^{(i)}(a^{(i)}-y^{(i)}) dw_2=\frac1m\sum_{i=1}^mx_2^{(i)}(a^{(i)}-y^{(i)}) dw_m=\frac1m\sum_{i=1}^mx_m^{(i)}(a^{(i)}-y^{(i)}) db=\frac1m\sum_{i=1}^m(a^{(i)}-y^{(i)})算法流程如下所示： 12345678910111213J=0; dw1=0; dw2=0; db=0;for i = 1 to mz(i) = wx(i)+b;a(i) = sigmoid(z(i));J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i));dz(i) = a(i)-y(i);dw1 += x1(i)dz(i);dw2 += x2(i)dz(i);db += dz(i);J /= m;dw1 /= m;dw2 /= m;db /= m; 经过每次迭代后，根据梯度下降算法，w和b都进行更新： w_1:=w_1-\alpha\ dw_1 w_2:=w_2-\alpha\ dw_2 w_m:=w_m-\alpha\ dw_m b:=b-\alpha\ db在深度学习中，样本数量m通常很大，使用for循环会让神经网络程序运行得很慢。应该尽量避免使用for循环操作，而使用矩阵运算，能够大大提高程序运行速度 2.6 向量化 logistic 回归的梯度输出（Vectorizing Logistic Regression’s Gradient Output）db可表示为： db=\frac1m \sum_{i=1}^mdz^{(i)}dw可表示为： dw=\frac1m X\cdot dZ^T单次迭代，梯度下降算法流程如下所示： 12345678Z = np.dot(w.T,X) + bA = sigmoid(Z)dZ = A-Ydw = 1/m*np.dot(X,dZ.T)db = 1/m*np.sum(dZ)w = w - alpha*dwb = b - alpha*db 2.7 （选修）logistic 损失函数的解释（Explanation of logistic regression cost function ）$\hat y$可以看成是预测输出为正类（+1）的概率： \hat y=P(y=1|x)当y=1时： p(y|x)=\hat y当y=0时： p(y|x)=1-\hat y整合到一个式子: P(y|x)=\hat y^y(1-\hat y)^{(1-y)}进行log处理： log\ P(y|x)=log\ \hat y^y(1-\hat y)^{(1-y)}=y\ log\ \hat y+(1-y)log(1-\hat y)上述概率P(y|x)越大越好，加上负号，则转化成了单个样本的Loss function，越小越好: L=-(y\ log\ \hat y+(1-y)log(1-\hat y))对于所有m个训练样本，假设样本之间是独立同分布的，总的概率越大越好： max\ \prod_{i=1}^m\ P(y^{(i)}|x^{(i)})引入log函数，加上负号，将上式转化为Cost function： J(w,b)=-\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac 1m\sum_{i=1}^m[y^{(i)}\ log\ \hat y^{(i)}+(1-y^{(i)})log(1-\hat y^{(i)})]]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周：深度学习引言(Introduction to Deep Learning)(Course 1)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%B8%80%E9%97%A8%E8%AF%BE-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Neural-Networks-and-Deep-Learning%2F</url>
    <content type="text"><![CDATA[1.1 神经网络的监督学习(Supervised Learning with Neural Networks) 一般的监督式学习（房价预测和线上广告问题），只要使用标准的神经网络模型就可以图像识别处理问题，则要使用卷积神经网络（Convolution Neural Network），即CNN 处理类似语音这样的序列信号时，则要使用循环神经网络（Recurrent Neural Network），即RNN 自动驾驶这样的复杂问题则需要更加复杂的混合神经网络模型 CNN一般处理图像问题，RNN一般处理语音信号 数据类型一般分为两种：Structured Data和Unstructured Data Structured Data通常指的是有实际意义的数据，例如房价预测中的size，#bedrooms，price等；例如在线广告中的User Age，Ad ID等 Unstructured Data通常指的是比较抽象的数据，例如Audio，Image或者Text Why is Deep Learning taking off？ 红色曲线代表了传统机器学习算法的表现，例如是SVM，logistic regression，decision tree等。当数据量比较小的时候，传统学习模型的表现是比较好的。当数据量很大的时候，其性能基本趋于水平 构建一个深度学习的流程是首先产生Idea，然后将Idea转化为Code，最后进行Experiment。接着根据结果修改Idea，继续这种Idea-&gt;Code-&gt;Experiment的循环，直到最终训练得到表现不错的深度学习网络模型 ![]]]></content>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning]]></title>
    <url>%2F2019%2F02%2F23%2Fmachine%20learning%2F</url>
    <content type="text"><![CDATA[A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience EA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>

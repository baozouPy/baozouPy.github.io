<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[大话数据结构 第五章 串]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%BA%94%E7%AB%A0%20%20%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[5.1 串的定义串（string）是由零个或多个字符组成的有限序列，又名叫字符串 一般记为s="a_1a_2\cdots a_n"(n\geq0)，s是串的名称，用双引号（有些书中也用单引号）括起来的字符序列是串的值，a_i(1\leq i\leq n)可以是字母、数字或其他字符，i是该字符在串中的位置。串中的字符数目n称为串的长度，“有限”是指长度n是一个有限的数值。零个字符的串称为空串（nullstring），它的长度为零，可以直接用两双引号“”””表示，也可以用希腊字母\Phi来表示。所谓的序列，说明串的相邻字符之间具有前驱和后继的关系 空格串，是只包含空格的串。空格串是有内容有长度的，而且可以不止一个空格。 串中任意个数的连续字符组成的子序列称为该串的子串，相应地，包含子串的串称为主串。子串在主串中的位置就是子串的第一个字符在主串中的序号 5.2 串的比较串的比较是通过组成串的字符之间的编码来进行的，而字符的编码指的是字符在对应字符集中的序号 计算机中的常用字符是使用标准的ASCII编码，由7位二进制数表示一个字符，总共可以表示128个字符,后来扩展ASCII码由8位二进制数表示一个字符，总共可以表示256个字符 Unicode编码，由16位的二进制数表示一个字符，这样总共就可以表示2^{16}个字符，约是6.5万多个字符，为了和ASCII码兼容，Unicode的前256个字符与ASCII码完全相同 在C语言中比较两个串是否相等，必须是它们串的长度以及它们各个对应位置的字符都相等时，才算是相等。即给定两个串：s="a_1a_2\cdots a_n",t="b_1b_2\cdots b_m"，当且仅当n=m，且a_1=b_1,a_2=b_2,\cdots,a_n=b_m时，认为s=t 两个串不相等时，判定大小： 给定两个串：s="a_1a_2\cdots a_n",t="b_1b_2\cdots b_m"，当满足以下条件之一时，s]]></content>
      <categories>
        <category>数据结构</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构 第九章 排序]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[9.1 排序的基本概念与分类假设含有n个记录的序列为\{r_1,r_2,\cdots,r_n\}，其相应的关键字分别为\{k_1,k_2,\cdots,k_n\}，需确定1,2,……,n的一种排列p_1,p_2,\cdots,p_n，使其相应的关键字满足k_{p_1}\le k_{p_2}\le \cdots \le k_{p_n}（非递减或非递增）关系，即使得序列成为一个按关键字有序的序列\{r_{p_1},r_{p_2},\cdots,r_{p_n}\}，这样的操作称为排序 在排序问题中，通常将数据元素称为记录。输入的是一个记录集合，输出的也是一个记录集合，可以将排序看成是线性表的一种操作 排序的依据是关键字之间的大小关系，对同一个记录集合，针对不同的关键字进行排序，可以得到不同序列 关键字k_i可以是记录r的主关键字，也可以是次关键字，甚至是若干数据项的组合。 排序的稳定性假设k_i=k_j(1 \le i\le n,1\le j\le n,i\ne j)，且在排序前的序列中r_i领先于r_j（即i]]></content>
      <categories>
        <category>数据结构</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构 第一章 数据结构概论]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%B8%80%E7%AB%A0%20%20%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%A6%82%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[1.1 基本概念和术语数据：是描述客观事物的符号，是计算机中可以操作的对象，是能被计算机识别，并输入给计算机处理的符号集合。数据不仅仅包括整型、实型等数值类型，还包括字符及声音、图像、视频等非数值类型 这里说的数据就是符号，必须具备两个前提： 可以输入到计算机中 能被计算机程序处理 数据元素：是组成数据的、有一定意义的基本单位，在计算机中通常作为整体处理。也被称为记录。 数据项：一个数据元素可以由若干个数据项组成。数据项是数据不可分割的最小单位。但真正讨论问题时，数据元素才是数据结构中建立数据模型的着眼点 数据对象：是性质相同的数据元素的集合，是数据的子集。 性质相同：是指数据元素具有相同数量和类型的数据项 数据结构：是相互之间存在一种或多种特定关系的数据元素的集合。 在计算机中，数据元素并不是孤立、杂乱无序的，而是具有内在联系的数据集合。数据元素之间存在的一种或多种特定关系，也就是数据的组织形式 结构：指各个组成部分相互搭配和排列的方式。简单的理解就是关系，比如分子结构，就是说组成分子的原子之间的排列方式 1.2 逻辑结构与物理结构按照视点的不同，把数据结构分为逻辑结构和物理结构 逻辑结构逻辑结构：指数据对象中数据元素之间的相互关系 逻辑结构分为以下四种： 集合结构：集合结构中的数据元素除了同属于一个集合外没有其他关系。各个数据元素是“平等”的，共同属性是“同属于一个集合”。集合关系就类似于数学中的集合 线性结构：线性结构中的数据元素之间是一对一的关系 树形结构：树形结构中的数据元素之间存在一种一对多的层次关系 图形结构：图形结构的数据元素是多对多的关系 用示意图表示数据的逻辑结构时，要注意两点： 将每一个数据元素看做一个结点，用圆圈表示。 元素之间的逻辑关系用结点之间的连线表示，如果这个关系是有方向的，那么用带箭头的连线表示。 逻辑结构是针对具体问题的，是为了解决某个问题，在对问题理解的基础上，选择一个合适的数据结构表示数据元素之间的逻辑关系 物理结构（存储结构）物理结构：指数据的逻辑结构在计算机中的存储形式 数据是数据元素的集合，根据物理结构的定义，实际上就是如何把数据元素存储到计算机的存储器中 数据的存储结构应正确反映数据元素之间的逻辑关系 数据元素的存储结构形式有两种： 顺序存储结构：是把数据元素存放在地址连续的存储单元里，其数据间的逻辑关系和物理关系是一致的 链式存储结构：是把数据元素存放在任意的存储单元里，这组存储单元可以是连续的，也可以是不连续的。数据元素的存储关系并不能反映其逻辑关系，需要用一个指针存放数据元素的地址，通过地址就可以找到相关联数据元素的位置 逻辑结构是面向问题的，而物理结构是面向计算机的，其基本的目标就是将数据及其逻辑关系存储到计算机的内存中 1.3 抽象数据类型数据类型数据类型：是指一组性质相同的值的集合及定义在此集合上的一些操作的总称。 数据类型是按照值的不同进行划分的。在高级语言中，每个变量、常量和表达式都有各自的取值范围。类型就用来说明变量或表达式的取值范围和所能进行的操作 在C语言中，按照取值的不同，数据类型可以分为两类： 原子类型：是不可以再分解的基本类型，包括整型、实型、字符型等。 结构类型：由若干个类型组合而成，是可以再分解的。例如，整型数组是由若干整型数据组成的。 比如，在C语言中变量声明int a,b，这就意味着，在给变量a和b赋值时不能超出int的取值范围，变量a和b之间的运算只能是int类型所允许的运算。 抽象：指抽取出事物具有的普遍性的本质。它是抽出问题的特征而忽略非本质的细节，是对具体事物的一个概括。抽象是一种思考问题的方式，它隐藏了繁杂的细节，只保留实现目标所必需的信息 抽象数据类型抽象数据类型（Abstract Data Type，ADT）：指一个数学模型及定义在该模型上的一组操作 抽象数据类型的定义仅取决于它的一组逻辑特性，而与其在计算机内部如何表示和实现无关 一个抽象数据类型定义了：一个数据对象、数据对象中各数据元素之间的关系及对数据元素的操作 抽象数据类型体现了程序设计中问题分解、抽象和信息隐藏的特性。抽象数据类型把实际生活中的问题分解为多个规模小且容易处理的问题，然后建立一个计算机能处理的数据模型，并把每个功能模块的实现细节作为一个独立的单元，从而使具体实现过程隐藏起来 抽象数据类型的标准格式： 12345678910111213141516171819202122232425ADT抽象数据类型名Data数据元素之间逻辑关系的定义Operation操作1初始条件操作结果描述操作2......操作n......endADT 严版： 123456789ADT 抽象数据类型名 &#123;数据对象：&lt;数据对象的定义&gt;数据关系：数据关系的定义&gt;基本操作：&lt;基本操作的定义&gt;&#125;ADT 抽象数据类型名 1.4 回顾总结]]></content>
      <categories>
        <category>数据结构</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构 第八章 查找]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%85%AB%E7%AB%A0%20%20%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[8.1 查找概论查找表（Search Table）是由同一类型的数据元素（或记录）构成的集合 关键字（Key）是数据元素中某个数据项的值，又称为键值，可以标识一个数据元素，也可以标识一个记录的某个数据项（字段），称为关键码，如①和② 若关键字可以唯一地标识一个记录，称此关键字为主关键字（Primary Key）。意味着对不同的记录其主关键字均不相同。主关键字所在的数据项称为主关键码，如③和④ 可以识别多个数据元素（或记录）的关键字，称为次关键字（SecondaryKey），如⑤。可以理解为是不以唯一标识一个数据元素（或记录）的关键字，对应的数据项就是次关键码 查找（Searching）是根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录） 若表中存在这样的一个记录，则称查找是成功的，此时查找的结果给出整个记录的信息，或指示该记录在查找表中的位置。比如查找主关键码“代码”的主关键字为“sh601398”的记录时，得到第2条唯一记录。查找次关键码“涨跌额”为“-0.11”的记录时，得到两条记录 若表中不存在关键字等于给定值的记录，称查找不成功，此时查找的结果可给出一个“空”记录或“空”指针 查找表按照操作方式来分有两大种： 静态查找表（Static Search Table）：只作查找操作的查找表 主要操作： 查询某个“特定的”数据元素是否在查找表中 检索某个“特定的”数据元素和各种属性 动态查找表（Dynamic Search Table）：在查找过程中同时插入查找表中不存在的数据元素，或者从查找表中删除已经存在的某个数据元素 主要操作： 查找时插入数据元素 查找时删除数据元素 面向查找操作的数据结构称为查找结构 8.2 顺序表查找顺序查找（Sequential Search）又叫线性查找，是最基本的查找技术 查找过程是： 从表中第一个（或最后一个）记录开始，逐个进行记录的关键字和给定值比较 若某个记录的关键字和给定值相等，则查找成功，找到所查的记录 如果直到最后一个（或第一个）记录，其关键字和给定值比较都不等时，则表中没有所查的记录，查找不成功 顺序表查找算法顺序查找的算法： 123456789/* 顺序查找，a为数组，n为要查找的数组长度， key为要查找的关键字 */int Sequential_Search(int *a, int n, int key)&#123;int i;for (i = 1; i &lt;= n; i++)if (a[i] == key)return i;return 0;&#125; 顺序表查找优化设置一个哨兵，可以解决不需要每次让i与n作比较 改进后的顺序查找算法： 12345678910111213/* 有哨兵顺序查找 */int Sequential_Search2(int *a, int n, int key)&#123;int i;/* 设置a[0]为关键字值，我们称之为“哨兵” */a[0] = key;/* 循环从数组尾部开始 */i = n;while (a[i] != key)i--;/* 返回0则说明查找失败 */return i;&#125; 代码是从尾部开始查找，由于a[0]=key，如果在a[i]中有key则返回i值，查找成功。否则一定在最终的a[0]处等于key，此时返回的是0，即说明a[1]～a[n]中没有关键字key，查找失败 时间复杂度为O(n) 顺序查找技术n很大时，查找效率极为低下，不过优点是算法非常简单，对静态查找表的记录没有任何要求，在一些小型数据的查找时，是可以适用的。 由于查找概率的不同，将容易查找到的记录放在前面，不常用的记录放置在后面，效率可以大幅提高 8.3 有序表查找折半查找（Binary Search）技术，又称为二分查找。前提是线性表中的记录必须是关键码有序（通常从小到大有序），线性表必须采用顺序存储。 折半查找的基本思想：在有序表中，取中间记录作为比较对象，若给定值与中间记录的关键字相等，则查找成功；若给定值小于中间记录的关键字，则在中间记录的左半区继续查找；若给定值大于中间记录的关键字，则在中间记录的右半区继续查找。不断重复上述过程，直到查找成功，或所有查找区域无记录，查找失败为止 有序表数组{0,1,16,24,35,47,59,62,73,88,99}，除0下标外共10个数字。对它进行查找是否存在62这个数 123456789101112131415161718192021222324/* 折半查找 */int Binary_Search(int *a, int n, int key)&#123;int low, high, mid;/* 定义最低下标为记录首位 */low = 1;/* 定义最高下标为记录末位 */high = n;while (low &lt;= high)&#123;/* 折半 */mid = (low + high) / 2;/* 若查找值比中值小 */if (key &lt; a[mid])/* 最高下标调整到中位下标小一位 */high = mid - 1;/* 若查找值比中值大 */else if (key &gt; a[mid])/* 最低下标调整到中位下标大一位 */low = mid + 1;else/* 若相等则说明mid即为查找到的位置 */return mid;&#125; 将这个数组的查找过程绘制成一棵二叉树，如果查找的关键字不是中间记录47的话，折半查找等于是把静态有序查找表分成了两棵子树，即查找结果只需要找其中的一半数据记录即可，等于工作量少了一半，然后继续折半查找 折半算法的时间复杂度为O(\log n) 插值查找折半查找代码改进为： mid =low +\frac{key-a[low]}{a[high]-a[low]}(high - low)假设a[11]={0,1,16,24,35,47,59,62,73,88,99}，low=1，high=10，则a[low]=1，a[high]=99，如果要找的是key=16时，按原来折半的做法，需要四次才可以得到结果，如果用新办法，\frac{key-a[low]}{a[high]-a[low]}=(16-1)/(99-1)≈0.153，即mid≈1+0.153×(10-1)=2.377取整得到mid=2，只需要二次就查找到结果 折半查找算法的代码中更改如下： 1mid=low+ (high-low)*(key-a[low])/(a[high]-a[low]); /* 插值 */ 得到了另一种有序表查找算法，插值查找法 插值查找（Interpolation Search）是根据要查找的关键字key与查找表中最大最小记录的关键字比较后的查找方法，其核心就在于插值的计算公式\frac{key-a[low]}{a[high]-a[low]}。时间复杂度是O(\log n)，对于表长较大，关键字分布又比较均匀的查找表，插值查找算法的平均性能比折半查找要好得多 斐波那契查找123456789101112131415161718192021222324252627282930313233int Fibonacci_Search(int *a,int n,int key) /* 斐波那契查找 */&#123;int low,high,mid,i,k;low=1; /* 定义最低下标为记录首位 */high=n; /* 定义最高下标为记录末位 */k=0;while(n&gt;F[k]-1) /* 计算n位于斐波那契数列的位置 */k++;for (i=n;i&lt;F[k]-1;i++) /* 将不满的数值补全 */a[i]=a[n];while(low&lt;=high)&#123;mid=low+F[k-1]-1; /* 计算当前分隔的下标 */if (key&lt;a[mid]) /* 若查找记录小于当前分隔记录 */&#123;high=mid-1; /* 最高下标调整到分隔下标mid-1处 */k=k-1; /* 斐波那契数列下标减一位 */&#125;else if (key&gt;a[mid]) /* 若查找记录大于当前分隔记录 */&#123;low=mid+1; /* 最低下标调整到分隔下标mid+1处 */k=k-2; /* 斐波那契数列下标减两位 */&#125;else&#123;if (mid&lt;=n)return mid; /* 若相等则说明mid即为查找到的位置 */elsereturn n; /* 若mid&gt;n说明是补全数值，返回n */&#125;&#125;return 0;&#125; 程序开始运行，参数a={0,1,16,24,35,47,59,62,73,88,99}，n=10，要查找的关键字key=59。斐波那契数列F={0,1,1,2,3,5,8,13,21,……} 第6～8行是计算当前的n处于斐波那契数列的位置。现在n=10，F[6]&lt;n&lt;F[7]，所以计算得出k=7 第9～10行，由于k=7，计算时是以F[7]=13为基础，而a中最大的仅是a[10]，后面的a[11]，a[12]均未赋值，这不能构成有序数列，因此将它们都赋值为最大的数组值，所以此时a[11]=a[12]=a[10]=99 第13行，mid=1＋F[7-1]-1=8，第一个要对比的数值从下标为8开始 由于此时key=59而a[8]=73，因此执行第16～17行，得到high=7，k=6 再次循环，mid=1＋F[6-1]-1=5。此时a[5]=47&lt;key，因此执行第21～22行，得到low=6，k=6-2=4。注意此时k下调2个单位 再次循环，mid=6＋F[4-1]-1=7。此时a[7]=62&gt;key，因此执行第16～17行，得到high=6，k=4-1=3 再次循环，mid=6＋F[3-1]-1=6。此时a[6]=59=key，因此执行第26～27行，得到返回值为6。程序运行结束 如果key=99，此时查找循环第一次时，mid=8与上例是相同的，第二次循环时，mid=11，如果a[11]没有值就会使得与key的比较失败，为了避免这样的情况出现，第9～10行的代码就起到这样的作用 斐波那契查找算法的核心在于： 当key=a[mid]时，查找就成功 当key&lt;a[mid]时，新范围是第low个到第mid-1个，此时范围个数为F[k-1]-1个 当key&gt;a[mid]时，新范围是第m+1个到第high个，此时范围个数为F[k-2]-1个 时间复杂度为O(\log n)，就平均性能来说，斐波那契查找要优于折半查找。如果是最坏情况，比如这里key=1，那么始终都处于左侧长半区在查找，则查找效率要低于折半查找 折半查找是进行加法与除法运算 mid=\frac{low+high}{2}插值查找进行复杂的四则运算 mid =low +\frac{key-a[low]}{a[high]-a[low]}(high - low)斐波那契查找是最简单加减法运算 mid=low+F[k-1]-1在海量数据的查找过程中，这种细微的差别可能会影响最终的查找效率。三种有序表的查找本质上是分隔点的选择不同，各有优劣 8.4 线性索引查找索引是把一个关键字与它对应的记录相关联的过程，一个索引由若干个索引项构成，每个索引项至少应包含关键字和其对应的记录在存储器中的位置等信息 线性索引是将索引项集合组织为线性结构，也称为索引表 稠密索引稠密索引是指在线性索引中，将数据集中的每个记录对应一个索引项 对于稠密索引这个索引表来说，索引项一定是按照关键码有序的排列 分块索引分块有序，是把数据集的记录分成了若干块，并且这些块需要满足两个条件： 块内无序，即每一块内的记录不要求有序 块间有序，例如，要求第二块所有记录的关键字均要大于第一块中所有记录的关键字，第三块的所有记录的关键字均要大于第二块的所有记录关键字……只有块间有序，才有可能在查找时带来效率 对于分块有序的数据集，将每块对应一个索引项，这种索引方法叫做分块索引 分块索引的索引项结构分三个数据项： 最大关键码，存储每一块中的最大关键字，使得在它之后的下一块中的最小关键字也能比这一块最大的关键字要大 存储了块中的记录个数，以便于循环时使用 用于指向块首数据元素的指针，便于开始对这一块中记录进行遍历 在分块索引表中查找，分两步进行： 在分块索引表中查找要查关键字所在的块。由于分块索引表是块间有序的，很容易利用折半、插值等算法得到结果。上图的数据集中查找62，由57&lt;62&lt;96得到62在第三个块中 根据块首指针找到相应的块，并在块中顺序查找关键码。因为块中可以是无序的，因此只能顺序查找 分块索引的平均查找长度： 设n个记录的数据集被平均分成m块，每个块中有t条记录，显然n=m×t（m=n/t）。L_b为查找索引表的平均查找长度，因最好与最差的等概率原则，所以L_b的平均长度为\frac{m+1}{2}。L_w为块中查找记录的平均查找长度，同理可知平均查找长度为\frac{t+1}{2}。这样分块索引查找的平均查找长度为： 最佳的情况就是分的块数m与块中的记录数t相同，此时意味着n=m\times t=t^2，即： # ASL_w=\frac{1}{2}\times(\frac{n}{t}+t)+1=t+1=\sqrt n+1倒排索引 索引项的通用结构是： 次关键码，如“英文单词” 记录号表，如“文章编号” 其中记录号表存储具有相同次关键字的所有记录的记录号（可以是指向记录的指针或者是该记录的主关键字）。这样的索引方法就是倒排索引（in-verted index）。倒排索引源于实际应用中需要根据属性（或字段、次关键码）的值来查找记录。这种索引表中的每一项都包括一个属性值和具有该属性值的各记录的地址。由于不是由记录来确定属性值，而是由属性值来确定记录的位置，因而称为倒排索引 8.5 二叉排序树 对集合{62,88,58,47,35,73,51,99,37,93}做查找，用二叉树结构排好序的二叉树来创建： 进行中序遍历时，得到一个有序的序列{35,37,47,51,58,62,73,88,93,99}，称它为二叉排序树 二叉排序树（Binary Sort Tree），又称为二叉查找树。它或者是一棵空树，或者是具有下列性质的二叉树： 若左子树不空，则左子树上所有结点的值均小于它的根结构的值 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值 左、右子树也分别为二叉排序树 二叉排序树查找操作二叉树的结构。 123456/* 二叉树的二叉链表结点结构定义 */typedef struct BiTNode /* 结点结构 */&#123;int data; /* 结点数据 */struct BiTNode *lchild, *rchild; /* 左右孩子指针 */&#125; BiTNode, *BiTree; 二叉排序树的查找实现： 123456789101112131415161718192021/* 递归查找二叉排序树T中是否存在key, *//* 指针f指向T的双亲，其初始调用值为NULL *//* 若查找成功，则指针p指向该数据元素结点，并返回TRUE *//* 否则指针p指向查找路径上访问的最后一个结点并返回FALSE */Status SearchBST(BiTree T, int key, BiTree f, BiTree *p)&#123;if (!T) /* 查找不成功 */&#123;*p = f;return FALSE;&#125;else if (key==T-&gt;data) /* 查找成功 */&#123;*p = T;return TRUE;&#125;else if (key&lt;T-&gt;data)return SearchBST(T-&gt;lchild, key, T, p); /* 在左子树中继续查找 */elsereturn SearchBST(T-&gt;rchild, key, T, p); /* 在右子树中继续查找 */&#125; 函数调用时的语句为SearchBST(T,93,NULL,p)，参数T是一个二叉链表，数据如图，key代表要查找的关键字，打算查找93，二叉树f指向T的双亲，当T指向根结点时，f的初值为NULL，它在递归时有用，最后的参数p是为了查找成功后可以得到查找到的结点位置 第7～11行，判断当前二叉树是否到叶子结点，下图当前T指向根结点62的位置，T不为空，第9～10行不执行 第12～16行是查找到相匹配的关键字时执行语句，显然93≠62，第14～15行不执行 第17～18行是当要查找关键字小于当前结点值时执行语句，由于93&gt;62，第18行不执行 第19～20行是当要查找关键字大于当前结点值时执行语句，由于93&gt;62，所以递归调用SearchBST(T-&gt;rchild,key,T,p)。此时T指向了62的右孩子88 此时第二层SearchBST，因93比88大，执行第20行，再次递归调用SearchBST(T-&gt;rchild,key,T,p)。此时T指向了88的右孩子99 第三层的SearchBST，因93比99小，所以执行第18行，递归调用SearchBST(T-&gt;lchild,key,T,p)。此时T指向了99的左孩子93 第四层SearchBST，因key等于T-&gt;data，所以执行第14～15行，此时指针p指向93所在的结点，并返回True到第三层、第二层、第一层，最终函数返回True 二叉排序树插入操作123456789101112131415161718192021/* 当二叉排序树T中不存在关键字等于key的数据元素时， *//* 插入key并返回TRUE，否则返回FALSE */Status InsertBST(BiTree *T, int key)&#123;BiTree p,s;if (!SearchBST(*T, key, NULL, &amp;p)) /* 查找不成功 */&#123;s = (BiTree)malloc(sizeof(BiTNode));s-&gt;data = key;s-&gt;lchild = s-&gt;rchild = NULL;if (!p)*T = s; /* 插入s为新的根结点 */else if (key&lt;p-&gt;data)p-&gt;lchild = s; /* 插入s为左孩子 */elsep-&gt;rchild = s; /* 插入s为右孩子 */return TRUE;&#125;elsereturn FALSE; /* 树中已有关键字相同的结点，不再插入 */&#125; 调用函数是“InsertBST(&amp;T,93);”，那么结果就是FALSE，如果是“InsertBST(&amp;T,95);”，那么一定就是在93的结点增加一个右孩子95，并且返回True 实现二叉排序树的构建： 123456int i;int a[10] = &#123; 62, 88, 58, 47, 35, 73, 51, 99, 37, 93 &#125;;BiTree T = NULL;for (i = 0; i &lt; 10; i++)InsertBST(&amp;T, a[i]); 二叉排序树删除操作如果需要查找并删除如37、51、73、93这些在二叉排序树中是叶子的结点 删除的结点只有左子树或只有右子树的情况，就是结点删除后，将它的左子树或右子树整个移动到删除结点的位置 先删除35和99结点，再删除58结点 要删除的结点既有左子树又有右子树的情况： 找到需要删除的结点p的直接前驱（或直接后继）s，用s来替换结点p，然后再删除此结点s，若要删除47，即对二叉排序树进行中序遍历，得到的序列{29,35,36,37,47,48,49,50,51,56,58,62,73,88,93,99}，37和48正好是47的前驱和后继 删除结点三种情况的分析： 叶子结点 仅有左或右子树的结点 左右子树都有的结点 递归方式对二叉排序树T查找key，查找到时删除： 1234567891011121314151617/* 若二叉排序树T中存在关键字等于key的数据元素时，则删除该数据元素结点, *//* 并返回TRUE；否则返回FALSE。 */Status DeleteBST(BiTree *T,int key)&#123;if(!*T) /* 不存在关键字等于key的数据元素 */return FALSE;else&#123;if (key==(*T)-&gt;data) /* 找到关键字等于key的数据元素 */return Delete(T);else if (key&lt;(*T)-&gt;data)return DeleteBST(&amp;(*T)-&gt;lchild,key);elsereturn DeleteBST(&amp;(*T)-&gt;rchild,key);&#125;&#125; 12345678910111213141516171819202122232425262728/* 从二叉排序树中删除结点p，并重接它的左或右子树。 */Status Delete(BiTree *p)&#123;BiTree q,s;if((*p)-&gt;rchild==NULL) /* 右子树空则只需重接它的左子树（待删结点是叶子也走此分支) */&#123;q=*p; *p=(*p)-&gt;lchild; free(q);&#125;else if((*p)-&gt;lchild==NULL) /* 只需重接它的右子树 */&#123;q=*p; *p=(*p)-&gt;rchild; free(q);&#125;else /* 左右子树均不空 */&#123;q=*p; s=(*p)-&gt;lchild;while(s-&gt;rchild) /* 转左，然后向右到尽头（找待删结点的前驱） */&#123;q=s; s=s-&gt;rchild;&#125;(*p)-&gt;data=s-&gt;data; /* s指向被删结点的直接前驱（将被删结点前驱的值取代被删结点的值） */if(q!=*p)q-&gt;rchild=s-&gt;lchild; /* 重接q的右子树 */elseq-&gt;lchild=s-&gt;lchild; /* 重接q的左子树 */free(s);&#125;return TRUE;&#125; 程序开始执行，代码第5～8行目的是为了删除没有右子树只有左子树的结点。此时只需将此结点的左孩子替换它自己，然后释放此结点内存，就等于删除了。 代码第9～12行处理只有右子树没有左子树的结点删除问题 第13～26行处理复杂的左右子树均存在的问题 第15行，将要删除的结点p赋值给临时的变量q，再将p的左孩子p-&gt;lchild赋值给临时的变量s。此时q指向47结点，s指向35结点 第16～19行，循环找到左子树的右结点，直到右侧尽头。让q指向35，而s指向了37这个再没有右子树的结点 第20行，此时让要删除的结点p的位置的数据被赋值为s-&gt;data，即让p-&gt;data=37 第21～24行，如果p和q指向不同，则将s-&gt;lchild赋值给q-&gt;rchild，否则就是将s-&gt;lchild赋值给q-&gt;lchild。显然这个例子p不等于q，将s-&gt;lchild指向的36赋值给q-&gt;rchild，也就是让q-&gt;rchild指向36结点 第25行，free(s)，将37结点删除 二叉排序树总结二叉排序树是以链接的方式存储，保持了链接存储结构在执行插入或删除操作时不用移动元素的优点，只要找到合适的插入和删除位置后，仅需修改链接指针即可 对于二叉排序树的查找，走的就是从根结点到要查找的结点的路径，其比较次数等于给定值的结点在二叉排序树的层数。最少为1次，即根结点就是要找的结点，最多不超过树的深度 8.6 平衡二叉树（AVL树）平衡二叉树（Self-Balancing Binary SearchTree或Height-Balanced Binary Search Tree），是一种二叉排序树，其中每一个节点的左子树和右子树的高度差至多等于1 高度平衡的二叉排序树： 要么它是一棵空树 要么它的左子树和右子树都是平衡二叉树，且左子树和右子树的深度之差的绝对值不超过1 二叉树上结点的左子树深度减去右子树深度的值称为平衡因子BF（Balance Factor），平衡二叉树上所有结点的平衡因子只可能是-1、0和1。只要二叉树上有一个结点的平衡因子的绝对值大于1，则该二叉树就是不平衡的 当新插入结点37时，距离它最近的平衡因子绝对值超过1的结点是58（即它的左子树高度3减去右子树高度1），所以从58开始以下的子树为最小不平衡子树 平衡二叉树实现原理平衡二叉树构建的基本思想就是在构建二叉排序树的过程中，每当插入一个结点时，先检查是否因插入而破坏了树的平衡性，若是，则找出最小不平衡子树。在保持二叉排序树特性的前提下，调整最小不平衡子树中各结点之间的链接关系，进行相应的旋转，使之成为新的平衡子树 对数组a[10]={3,2,1,4,5,6,7,10,9,8}构建平衡二叉树： 当最小不平衡子树根结点的平衡因子BF是大于1时，就右旋，小于-1时就左旋，如上例中结点1、5、6、7的插入等。插入结点后，最小不平衡子树的BF与它的子树的BF符号相反时，就需要对结点先进行一次旋转以使得符号相同后，再反向旋转一次才能够完成平衡操作，如上例中结点9、8的插入时 平衡二叉树实现算法1234567/* 二叉树的二叉链表结点结构定义 */typedef struct BiTNode /* 结点结构 */&#123;int data; /* 结点数据 */int bf; /* 结点的平衡因子 */struct BiTNode *lchild, *rchild; /* 左右孩子指针 */&#125; BiTNode, *BiTree; 对于右旋操作，代码如下： 12345678910/* 对以p为根的二叉排序树作右旋处理， *//* 处理之后p指向新的树根结点，即旋转处理之前的左子树的根结点 */void R_Rotate(BiTree *P)&#123;BiTree L;L=(*P)-&gt;lchild; /* L指向P的左子树根结点 */(*P)-&gt;lchild=L-&gt;rchild; /* L的右子树挂接为P的左子树 */L-&gt;rchild=(*P);*P=L; /* P指向新的根结点 */&#125; 此函数代码的意思是当传入一个二叉排序树P，将它的左孩子结点定义为L，将L的右子树变成P的左子树，再将P改成L的右子树，最后将L替换P成为根结点。这样就完成了一次右旋操作，如图。图中三角形代表子树，N代表新增结点 左旋操作代码： 12345678910/* 对以P为根的二叉排序树作左旋处理， *//* 处理之后P指向新的树根结点，即旋转处理之前的右子树的根结点0 */void L_Rotate(BiTree *P)&#123;BiTree R;R=(*P)-&gt;rchild; /* R指向P的右子树根结点 */(*P)-&gt;rchild=R-&gt;lchild; /* R的左子树挂接为P的右子树 */R-&gt;lchild=(*P);*P=R; /* P指向新的根结点 */&#125; 左平衡旋转处理的函数代码： 12345678910111213141516171819202122232425262728293031323334#define LH +1 /* 左高 */#define EH 0 /* 等高 */#define RH -1 /* 右高 *//* 对以指针T所指结点为根的二叉树作左平衡旋转处理 *//* 本算法结束时，指针T指向新的根结点 */void LeftBalance(BiTree *T)&#123;BiTree L,Lr;L=(*T)-&gt;lchild; /* L指向T的左子树根结点 */switch(L-&gt;bf)&#123; /* 检查T的左子树的平衡度，并作相应平衡处理 */case LH: /* 新结点插入在T的左孩子的左子树上，要作单右旋处理 */(*T)-&gt;bf=L-&gt;bf=EH;R_Rotate(T);break;case RH: /* 新结点插入在T的左孩子的右子树上，要作双旋处理 */Lr=L-&gt;rchild; /* Lr指向T的左孩子的右子树根 */switch(Lr-&gt;bf)&#123; /* 修改T及其左孩子的平衡因子 */case LH: (*T)-&gt;bf=RH;L-&gt;bf=EH;break;case EH: (*T)-&gt;bf=L-&gt;bf=EH;break;case RH: (*T)-&gt;bf=EH;L-&gt;bf=LH;break;&#125;Lr-&gt;bf=EH;L_Rotate(&amp;(*T)-&gt;lchild); /* 对T的左子树作左旋平衡处理 */R_Rotate(T); /* 对T作右旋平衡处理 */&#125;&#125; 函数被调用，传入一个需调整平衡性的子树T。由于LeftBalance函数被调用时，已经确认当前子树是不平衡状态，且左子树的高度大于右子树的高度。此时T的根结点平衡因子BF的值是大于1 当L的平衡因子为LH，即为1时，表明它与根结点的BF值符号相同，第14行，将它们的BF值都改为0，第15行，进行右旋操作 当L的平衡因子为RH，即为-1时，表明它与根结点的BF值符号相反，此时需要做双旋处理。第19～28行，针对L的右孩子Lr的BF作判断，修改根结点T和L的BF值 第31行，对根结点的左子树进行左旋，如第二图所示 第32行，对根结点进行右旋，如第三图所示，完成平衡操作 主函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/* 若在平衡的二叉排序树T中不存在和e有相同关键字的结点，则插入一个 *//* 数据元素为e的新结点，并返回1，否则返回0。若因插入而使二叉排序树 *//* 失去平衡，则作平衡旋转处理，布尔变量taller反映T长高与否。 */Status InsertAVL(BiTree *T,int e,Status *taller)&#123;if(!*T)&#123; /* 插入新结点，树“长高”，置taller为TRUE */*T=(BiTree)malloc(sizeof(BiTNode));(*T)-&gt;data=e;(*T)-&gt;lchild=(*T)-&gt;rchild=NULL;(*T)-&gt;bf=EH;*taller=TRUE;&#125;else&#123;if (e==(*T)-&gt;data)&#123; /* 树中已存在和e有相同关键字的结点则不再插入 */*taller=FALSE;return FALSE;&#125;if (e&lt;(*T)-&gt;data)&#123; /* 应继续在T的左子树中进行搜索 */if(!InsertAVL(&amp;(*T)-&gt;lchild,e,taller)) /* 未插入 */return FALSE;if(taller) /* 已插入到T的左子树中且左子树“长高” */&#123;switch((*T)-&gt;bf) /* 检查T的平衡度 */&#123;case LH: /* 原本左子树比右子树高，需要作左平衡处理 */LeftBalance(T);*taller=FALSE;break;case EH: /* 原本左、右子树等高，现因左子树增高而使树增高 */(*T)-&gt;bf=LH;*taller=TRUE;break;case RH: /* 原本右子树比左子树高，现左、右子树等高 */(*T)-&gt;bf=EH;*taller=FALSE;break;&#125;&#125;&#125;else&#123; /* 应继续在T的右子树中进行搜索 */if(!InsertAVL(&amp;(*T)-&gt;rchild,e,taller)) /* 未插入 */return FALSE;if(*taller) /* 已插入到T的右子树且右子树“长高” */&#123;switch((*T)-&gt;bf) /* 检查T的平衡度 */&#123;case LH: /* 原本左子树比右子树高，现左、右子树等高 */(*T)-&gt;bf=EH;*taller=FALSE;break;case EH: /* 原本左、右子树等高，现因右子树增高而使树增高 */(*T)-&gt;bf=RH;*taller=TRUE;break;case RH: /* 原本右子树比左子树高，需要作右平衡处理 */RightBalance(T);*taller=FALSE;break;&#125;&#125;&#125;&#125;return TRUE;&#125; 第6～13行是指当前T为空时，则申请内存新增一个结点 第16～20行表示当存在相同结点，则不需要插入 第21～43行，当新结点e小于T的根结点值时，则在T的左子树查找 第23～24行，递归调用本函数，直到找到则返回false，否则说明插入结点成功，执行下面语句 第25～42行，当taller为TRUE时，说明插入了结点，此时需要判断T的平衡因子，如果是1，说明左子树高于右子树，需要调用LeftBalance函数进行左平衡旋转处理。如果为0或-1，则说明新插入结点没有让整棵二叉排序树失去平衡性，只需要修改相关的BF值即可 第44～66行，说明新结点e大于T的根结点的值，在T的右子树查找 只需要在构建平衡二叉树的时候执行如下列代码即可在内存中生成一棵下图的平衡二叉树： 123456789101112int main(void)&#123;int i;int a[10]=&#123;3,2,1,4,5,6,7,10,9,8&#125;;BiTree T=NULL;Status taller;for(i=0;i&lt;10;i++)&#123;InsertAVL(&amp;T,a[i],&amp;taller);&#125;return 0;&#125; 查找时间复杂度为O(\log n)，插入和删除也为O(\log n) 8.7 多路查找树（B树）多路查找树（muitl-way search tree），其每一个结点的孩子数可以多于两个，且每一个结点处可以存储多个元素。由于是查找树，所有元素之间存在某种特定的排序关系 2-3树2-3树是每一个结点都具有两个孩子（2结点）或三个孩子（3结点）的一棵多路查找树 一个2结点包含一个元素和两个孩子（或没有孩子），左子树包含的元素小于该元素，右子树包含的元素大于该元素。这个2结点要么没有孩子，要有就有两个，不能只有一个孩子 一个3结点包含一小一大两个元素和三个孩子（或没有孩子），一个3结点要么没有孩子，要么具有3个孩子。如果某个3结点有孩子的话，左子树包含小于较小元素的元素，右子树包含大于较大元素的元素，中间子树包含介于两元素之间的元素。并且2-3树中所有的叶子都在同一层次上 2-3树的插入实现2-3树的插入操作一定是发生在叶子结点上。插入一个元素的过程有可能会对该树的其余结构产生连锁反应。 2-3树插入可分为三种情况： 对于空树，插入一个2结点即可 插入结点到一个2结点的叶子上。由于其本身就只有一个元素，只需要将其升级为3结点 往3结点中插入一个新元素。因为3结点本身已经是2-3树的结点最大容量（已经有两个元素），需要将其拆分，且将树中两元素或插入元素的三者中选择其一向上移动一层 第一种情况，元素5需要插入在拥有6、7元素的3结点位置。将6、7结点拆分，让6与4结成3结点，将5成为它的中间孩子，将7成为它的右孩子 向左图中插入元素11。需要插入在拥有9、10元素的3结点位置。9和10结点不能再增加结点。双亲结点12、14是一个3结点，也不能再插入元素。12、14结点的双亲，结点8是个2结点。将9、10拆分，12、14也拆分，让根结点8升级为3结点 插入元素2，1、3，4、6，8、12都是3结点，不能再插入元素，当前树结构是三层不能满足当前结点的增加。于是将1、3拆分，4、6拆分，连根结点8、12也拆分如果2-3树插入的传播效应导致了根结点的拆分，则树的高度就会增加 2-3树的删除实现1.删除元素位于一个3结点的叶子结点上，只需要在该结点处删除该元素即可，不会影响到整棵树的其他结点结构 2.删除的元素位于一个2结点上，分四种情形 此结点的双亲也是2结点，且拥有一个3结点的右孩子。删除结点1，只需要左旋 此结点的双亲是2结点，它的右孩子也是2结点。删除结点4，直接左旋会造成没有右孩子，办法是让结点7变成3结点，让比7稍大的元素8下来，让比元素8稍大的元素9补充结点8的位置（中间图），再用左旋的方式变成右图结果 此结点双亲是一个3结点，将双亲拆分，并将12与13合并成为左孩子 当前树是一个满二叉树的情况，删除任何一个叶子都会使得整棵树不能满足2-3树的定义。删除叶子结点8时（任何一个结点都一样），要将2-3的层数减少，办法是将8的双亲和其左子树6合并为一3个结点，再将14与9合并为3结点 3.删除的元素位于非叶子的分支结点。通常是将树按中序遍历后得到此元素的前驱或后继元素，让它们来补位 删除的分支结点是2结点。删除结点4，前驱是1后继是6，由于6、7是3结点，用6来补位 删除的分支结点是3结点的某一元素，删除结点12、14的12，将是3结点的左孩子的10上升到删除位置 2-3-4树2-3-4树是2-3树的概念扩展，包括了4结点的使用。一个4结点包含小中大三个元素和四个孩子（或没有孩子），一个4结点要么没有孩子，要么具有4个孩子。如果某个4结点有孩子的话，左子树包含小于最小元素的元素；第二子树包含大于最小元素，小于第二元素的元素；第三子树包含大于第二元素，小于最大元素的元素；右子树包含大于最大元素的元素 构建一个数组为{7,1,2,5,6,9,8,4,3}的2-3-4树过程，图1是在分别插入7、1、2时的结果图，因为3个元素满足2-3-4树的单个4结点定义，因此此时不需要拆分，接着插入元素5，因为已经超过了4结点的定义，因此拆分为图2的形状。之后的图是在元素不断插入时最后形成的 2-3-4树删除结点的演变过程，删除顺序是1、6、3、4、5、2、9 B树B树（B-tree）是一种平衡的多路查找树，2-3树和2-3-4树是B树的特例。结点最大的孩子数目称为B树的阶（order），2-3树是3阶B树，2-3-4树是4阶B树 m$$阶的B树具有如下属性： * 如果根结点不是叶结点，则其至少有两棵子树 * 每一个非根的分支结点都有$$k-1$$个元素和$$k$$个孩子，其中。每一个叶子结点$$n$$都有$$k-1$$个元素，其中$$[\frac{m}{2}]\le k \le m 所有叶子结点都位于同一层次 所有分支结点包含下列信息数据(n,A_0,K_1,A_1,K_2,A_2,\cdots,K_n,A_n)，其中：K_i(i=1,2,\cdots,n)为关键字，且K_i]]></content>
      <categories>
        <category>数据结构</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构 第二章 算法]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%20%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[2.1 算法的特性 输入输出：算法具有零个或多个输入，至少有一个或多个输出。输出的形式可以是打印输出，也可以是返回一个或多个值等 有穷性：指算法在执行有限的步骤之后，自动结束而不会出现无限循环，并且每一个步骤在可接受的时间内完成 确定性：算法的每一步骤都具有确定的含义，不会出现二义性。算法在一定条件下，只有一条执行路径，相同的输入只能有唯一的输出结果。算法的每个步骤被精确定义而无歧义 可行性：算法的每一步都必须是可行的，都能够通过执行有限次数完成。 2.2 算法设计的要求 正确性：算法的正确性是指算法至少应该具有输入、输出和加工处理无歧义性、能正确反映问题的需求、能够得到问题的正确答案 算法的“正确”大体分为四个层次： 算法程序没有语法错误。 算法程序对于合法的输入数据能够产生满足要求的输出结果。 算法程序对于非法的输入数据能够得出满足规格说明的结果。 算法程序对于精心选择的，甚至刁难的测试数据都有满足要求的输出结果。 层次1要求最低，层次4是最困难的，几乎不可能逐一验证所有的输入都得到正确的结果。因此算法的正确性在大部分情况下都不可能用程序来证明，而是用数学方法证明的。证明一个复杂算法在所有层次上都是正确的， 代价非常昂贵。一般情况下把层次3作为一个算法是否正确的标准 可读性：算法设计的另一目的是为了便于阅读、理解和交流。 健壮性：当输入数据不合法时，算法也能做出相关处理，而不是产生异常或莫名其妙的结果 时间效率高：时间效率指的是算法的执行时间，对于同一个问题，如果有多个算法能够解决，执行时间短的算法效率高，执行时间长的效率低 存储量低 ：存储量需求指的是算法在执行过程中需要的最大存储空间，主要指算法程序运行时所占用的内存或外部硬盘存储空间 好的算法，应该具有正确性、可读性、健壮性、高效率和低存储量的特征 2.3 算法效率的度量方法事后统计方法事后统计方法：这种方法主要是通过设计好的测试程序和数据，利用计算机计时器对不同算法编制的程序的运行时间进行比较，从而确定算法效率的高低 缺陷： 必须依据算法事先编制好程序，这通常需要花费大量的时间和精力。 时间的比较依赖计算机硬件和软件等环境因素，有时会掩盖算法本身的优劣 算法的测试数据设计困难，并且程序的运行时间往往还与测试数据的规模有很大关系，效率高的算法在小的测试数据面前往往得不到体现 故不予采纳事后统计方法 事前分析估算方法事前分析估算方法：在计算机程序编制前，依据统计方法对算法进行估算。 一个用高级程序语言编写的程序在计算机上运行时所消耗的时间取决于下列因素： 算法采用的策略、方法。 编译产生的代码质量。 问题的输入规模。 机器执行指令的速度。 第1条是算法好坏的根本，第2条要由软件来支持，第4条要看硬件性能。抛开与计算机硬件、软件有关的因素，一个程序的运行时间，依赖于算法的好坏和问题的输入规模。所谓问题输入规模是指输入量的多少 2.4 函数的渐近增长 函数的渐近增长：给定两个函数f(n)和g(n)，如果存在一个整数N，使得对于所有的n > N，f(n)总是比g(n)大，那么说f(n)的增长渐近快于g(n) 判断一个算法的效率时，函数中的常数和其他次要项常可以忽略，而更应该关注主项（最高阶项）的阶数 2.5 算法时间复杂度算法时间复杂度定义算法的时间复杂度，也就是算法的时间量度，记作：T(n)=O(f(n))。表示随问题规模n的增大，算法执行时间的增长率和f(n)的增长率相同，称作算法的渐近时间复杂度，简称为时间复杂度。其中f(n)是问题规模n的某个函数。用大写O( )来体现算法时间复杂度的记法，称之为大O记法。一般情况下，随着n的增大，T(n)增长最慢的算法为最优算法 推导大O阶方法推导大O阶： 用常数1取代运行时间中的所有加法常数。 在修改后的运行次数函数中，只保留最高阶项。 如果最高阶项存在且不是1，则去除与这个项相乘的常数。 得到的结果就是大O阶 常数阶执行时间恒定的算法，称之为具有O(1)的时间复杂度，又叫常数阶。 不管这个常数是多少，都记作O(1)，而不能是O(3)、O(12)等其他任何数字 对于分支结构而言，无论是真，还是假，执行的次数都是恒定的，不会随着n的变大而发生变化，所以单纯的分支结构（不包含在循环结构中），其时间复杂度也是O(1) 线性阶线性阶的循环结构会复杂很多。要确定某个算法的阶次，需要确定某个特定语句或某个语句集运行的次数。 要分析算法的复杂度，关键就是要分析循环结构的运行情况 12345int i;for (i = 0; i &lt; n; i++)&#123;/* 时间复杂度为O(1)的程序步骤序列 */&#125; 循环的时间复杂度为O(n)，因为循环体中的代码须要执行n次 对数阶12345int count = 1;while (count &lt; n)&#123;count = count * 2; /* 时间复杂度为O(1)的程序步骤序列 */&#125; 由2^x=n得到x=log_2n。所以这个循环的时间复杂度为O(logn) 平方阶12345678int i, j;for (i = 0; i &lt; n; i++)&#123;for (j = 0; j &lt; n; j++)&#123;/* 时间复杂度为O(1)的程序步骤序列 */&#125;&#125; 的时间复杂度为O(n^2) 循环的时间复杂度等于循环体的复杂度乘以该循环运行的次数 12345678int i, j;for (i = 0; i &lt; n; i++)&#123; /* 注意j = i 而不是0 */for (j = i; j &lt; n; j++)&#123;/* 时间复杂度为O(1)的程序步骤序列 */&#125;&#125; 总的执行次数为： n+(n-1)+(n-2)+\cdot\cdot\cdot+1=\frac{n(n+1)}{2}=\frac{n^2}{2}+\frac{n}{2}推导大O阶的方法: 没有加法常数不予考虑； 只保留最高阶项，因此保留\frac{n^2}{2}； 去除这个项相乘的常数，也就是去除\frac{1}{2}，最终这段代码的时间复杂度为O(n^2) 1234567891011121314n++; /* 执行次数为1 */function(n); /* 执行次数为n */int i, j;for (i = 0; i &lt; n; i++) /* 执行次数为n2 */&#123;function (i);&#125;for (i = 0; i &lt; n; i++) /* 执行次数为n(n + 1)/2 */&#123;for (j = i; j &lt; n; j++)&#123;/* 时间复杂度为O(1)的程序步骤序列 */&#125;&#125; 执行次数f(n)=1+n+n^2+\frac{n(n+1)}{2}=\frac{3}{2}\cdot n^2+\frac{3}{2}\cdot n+1,时间复杂度也是O(n^2) 2.6 常见的时间复杂度 执行次数 函数阶 非正式术语 12 O(1) 常数阶 2n+3 O(n) 线性阶 3n^2+2n+1 O(n^2) 平方阶 5log_2n+20 O(logn) 对数阶 2n+3nlog_2n+19 O(nlogn) nlogn阶 6n^3+2n^2+3n+4 O(n^3) 立方阶 2^n O(2^n) 指数阶 常用的时间复杂度所耗费的时间从小到大依次是： O(1)]]></content>
      <categories>
        <category>数据结构</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构 第三章 线性表]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%B8%89%E7%AB%A0%20%20%E7%BA%BF%E6%80%A7%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[3.1 线性表的定义线性表（List）：零个或多个数据元素的有限序列。 首先它是一个序列。元素之间是有顺序的，若元素存在多个，则第一个元素无前驱，最后一个元素无后继，其他每个元素都有且只有一个前驱和后继 若将线性表记为(a_1,\cdots,a_{i-1},a_i,a_{i+1},\cdots,a_n)，则表中a_{i-1}领先于a_i，a_{i+1}领先于a_i，称a_{i-1}是a_i的直接前驱元素，a_{i+1}是a_i的直接后继元素。当i=1,2,\cdots,n-1时，a_i有且仅有一个直接后继，当i=2,3,\cdots,n时，a_i有且仅有一个直接前驱 线性表元素的个数n(n\ge 0)定义为线性表的长度，当n=0时，称为空表。i为数据元素a_i在线性表中的位序 在较复杂的线性表中，一个数据元素可以由若干个数据项组成 3.2 线性表的抽象数据类型对于一个线性表来说，插入数据和删除数据都是必须的操作 线性表的抽象数据类型定义如下： 1234567891011121314151617181920212223242526ADT 线性表(List)Data线性表的数据对象集合为&#123;a1, a2, ......, an&#125;，每个元素的类型均为DataType。 其中，除第一个元素a1外，每一个元素有且只有一个直接前驱元素，除了最后一个元素an外，每一个元素有且只有一个直接后继元素。数据元素之间的关系是一对一的关系。OperationInitList(*L): 初始化操作，建立一个空的线性表L。ListEmpty(L): 若线性表为空，返回true，否则返回false。ClearList(*L): 将线性表清空。GetElem(L, i, *e): 将线性表L中的第i个位置元素值返回给e。LocateElem(L, e): 在线性表L中查找与给定值e相等的元素，如果查找成功，返回该元素在表中序号表示成功；否则，返回0表示失败ListInsert(*L,i,e): 在线性表L中的第i个位置插入新元素eListDelete(*L,i,*e): 删除线性表L中的第i个位置元素，并用e返回其值ListLength(L): 返回线性表L的元素个数EndADT 123456789101112131415161718192021/* 将所有的在线性表Lb中但不在La中的数据元素插入到La中 */void unionL(List *La, List Lb)&#123;int La_len, Lb_len, i; /* 声明与La和Lb相同的数据元素e */ElemType e;La_len = ListLength(*La); /* 求线性表的长度 */Lb_len = ListLength(Lb);for (i = 1; i &lt;= Lb_len; i++)&#123;GetElem(Lb, i, &amp;e); /* 取Lb中第i个数据元素赋给e */if (!LocateElem(*La, e)) /* La中不存在和e相同数据元素 */ListInsert(La, ++La_len, e); /* 插入 */&#125;&#125; 3.3 线性表的顺序存储结构顺序存储定义线性表的顺序存储结构，指的是用一段地址连续的存储单元依次存储线性表的数据元素 顺序存储方式线性表的顺序存储的结构代码: 12345678typedef int ElemType; /* ElemType类型根据实际情况而定，这里假设为int */typedef struct&#123;ElemType data[MAXSIZE]; /* 数组存储数据元素，最大值为MAXSIZE */int length; /* 线性表当前长度 */&#125; SqList; 顺序存储结构需要三个属性： 存储空间的起始位置：数组data，它的存储位置就是存储空间的存储位置 线性表的最大存储容量：数组长度MaxSize 线性表的当前长度：length 数组长度与线性表长度区别 数组的长度是存放线性表的存储空间的长度，存储分配后这个量一般是不变的 线性表的长度是线性表中数据元素的个数，随着线性表插入和删除操作的进行，这个量是变化的。 在任意时刻，线性表的长度应该小于等于数组的长度 地址计算方法 存储器中的每个存储单元都有自己的编号，这个编号称为地址 每个数据元素，不管它是整型、实型还是字符型，它都是需要占用一定的存储单元空间的。假设占用的是c个存储单元，那么线性表中第i+1个数据元素的存储位置和第i个数据元素的存储位置满足下列关系： LOC(a_{i+1})=LOC(a_i)+c LOC表示获得存储位置的函数 第i个数据元素a_i的存储位置可以由a_1推算得出： LOC(a_i)=LOC(a_1)+(i-1)*c对每个线性表位置的存入或者取出数据，对于计算机来说都是相等的时间，也就是一个常数，存取时间性能为O(1)。通常把具有这一特点的存储结构称为随机存取结构 3.4 顺序存储结构的插入与删除获得元素操作实现GetElem操作，即将线性表L中的第i个位置元素值返回： 123456789101112131415#define OK 1#define ERROR 0#define TRUE 1#define FALSE 0typedef int Status;/* Status是函数的类型，其值是函数结果状态代 码，如OK等 *//* 初始条件：顺序线性表L已存在，1≤i≤ ListLength(L) *//* 操作结果：用e返回L中第i个数据元素的值 */Status GetElem(SqList L, int i, ElemType *e)&#123;if (L.length == 0 || i &lt; 1 || i &gt; L.length)return ERROR;*e = L.data[i - 1];return OK;&#125; 返回值类型Status是一个整型，返回OK代表1，ERROR代表0 插入操作 插入算法的思路： 如果插入位置不合理，抛出异常； 如果线性表长度大于等于数组长度，则抛出异常或动态增加容量； 从最后一个元素开始向前遍历到第i个位置，分别将它们都向后移动一个位置； 将要插入元素填入位置i处； 表长加1 实现代码如下： 123456789101112131415161718192021/* 初始条件：顺序线性表L已存在，1≤i≤ListLength(L)， *//* 操作结果：在L中第i个位置之前插入新的数据元 素e，L的长度加1 */Status ListInsert(SqList *L, int i, ElemType e)&#123;int k;if (L-&gt;length == MAXSIZE) /* 顺序线性表已经满 */return ERROR;if (i &lt; 1 || i &gt;L-&gt;length + 1) /* 当i不在范围内时 */return ERROR;if (i &lt;= L-&gt;length) /* 若插入数据位置不在表尾 */&#123;for (k = L-&gt;length - 1; k &gt;= i - 1; k--) /*将要插入位置后数据元素向后移动一位 */L-&gt;data[k + 1] = L-&gt;data[k];&#125;L-&gt;data[i - 1] = e; /* 将新元素插入 */L-&gt;length++;return OK;&#125; 删除操作 删除算法的思路： 如果删除位置不合理，抛出异常； 取出删除元素； 从删除元素位置开始遍历到最后一个元素位置，分别将它们都向前移动一个位置； 表长减1 123456789101112131415161718192021/* 初始条件：顺序线性表L已存在，1≤i≤ ListLength(L) *//* 操作结果：删除L的第i个数据元素，并用e返回 其值，L的长度减1 */Status ListDelete(SqList *L, int i, ElemType *e)&#123; int k;if (L-&gt;length == 0) /* 线性表为空 */return ERROR;if (i &lt; 1 || i &gt; L-&gt;length) /* 删除位置不正确 */return ERROR;*e = L-&gt;data[i - 1];if (i &lt; L-&gt;length) /* 如果删除不是最后位置 */&#123;for (k = i; k &lt; L-&gt;length; k++) /* 将删除位置后继元素前移 */L-&gt;data[k - 1] = L-&gt;data[k];&#125;L-&gt;length--;return OK;&#125; 如果元素要插入到最后一个位置，或者删除最后一个元素，时间复杂度为O(1) 如果元素要插入到第一个位置或者删除第一个元素，意味着要移动所有的元素向后或者向前，时间复杂度为O(n) 平均的情况，由于元素插入到第i个位置，或删除第i个元素，需要移动n-i个元素。根据概率原理，每个位置插入或删除元素的可能性是相同的，也就说位置靠前，移动元素多，位置靠后，移动元素少。最终平均移动次数和最中间的那个元素的移动次数相等，为\frac{n-1}{2},平均时间复杂度还是O(n) 线性表的顺序存储结构，在存、读数据时，不管是哪个位置，时间复杂度都是O(1)；而插入或删除时，时间复杂度都是O(n) 线性表顺序存储结构的优缺点 3.5 线性表的链式存储结构顺序存储结构不足的解决办法线性表的顺序存储结构最大的缺点就是插入和删除时需要移动大量元素，显然需要耗费时间 线性表链式存储结构定义线性表的链式存储结构的特点是用一组任意的存储单元存储线性表的数据元素，这组存储单元可以是连续的，也可以是不连续的。这些数据元素可以存在内存未被占用的任意位置 链式结构中，除了要存数据元素信息外，还要存储它的后继元素的存储地址。 对数据元素a_i来说，除了存储其本身的信息之外，还需存储一个指示其直接后继的信息（即直接后继的存储位置）。存储数据元素信息的域称为数据域，存储直接后继位置的域称为指针域。指针域中存储的信息称做指针或链。这两部分信息组成数据元素a_i的存储映像，称为结点（Node）。n个结点（a_i的存储映像）链结成一个链表，即为线性表（a1,a2,\cdots,a_n）的链式存储结构，因为此链表的每个结点中只包含一个指针域，所以叫做单链表。单链表正是通过每个结点的指针域将线性表的数据元素按其逻辑次序链接在一起 链表中第一个结点的存储位置叫做头指针,线性链表的最后一个结点指针为“空”（通常用NULL或“^”符号表示) 为了更加方便地对链表进行操作，会在单链表的第一个结点前附设一个结点，称为头结点。头结点的数据域可以不存储任何信息，也可以存储如线性表的长度等附加信息，头结点的指针域存储指向第一个结点的指针 头指针与头结点的异同 线性表链式存储结构代码描述若线性表为空表，则头结点的指针域为“空” 改用更方便的存储示意图来表示单链表 带有头结点的单链表 空链表 单链表中，在C语言中可用结构指针来描述: 1234567/* 线性表的单链表存储结构 */typedef struct Node&#123;ElemType data;struct Node *next;&#125; Node;typedef struct Node *LinkList; /* 定义LinkList */ 结点由存放数据元素的数据域和存放后继结点地址的指针域组成。假设p是指向线性表第i个元素的指针，则该结点a_i的数据域可以用p-\gt data来表示，p-\gt data的值是一个数据元素，结点a_i的指针域可以用p-\gt next来表示，p-\gt next的值是一个指针，指向第i+1个元素，即指向a_{i+1}的指针。 如果p-\gt data=a_i，那么p-\gt next-\gt data=a_{i+1} 3.6 单链表的读取获得链表第i个数据的算法思路： 声明一个指针p指向链表第一个结点，初始化j从1开始； 当jnext=s； 返回成功 实现代码算法如下： 123456789101112131415161718192021222324252627/* 初始条件：顺序线性表L已存在，1≤i≤ListLength(L)， *//* 操作结果：在L中第i个结点位置之前插入新的数据元素e，L的长度加1 */Status ListInsert(LinkList *L, int i, ElemType e)&#123;int j;LinkList p, s;p = *L;j = 1;while (p &amp;&amp; j &lt; i) /* 寻找第i-1个结点 */&#123;p = p-&gt;next;++j;&#125;if (!p || j &gt; i) /* 第i个结点不存在 */return ERROR;s = (LinkList)malloc(sizeof(Node)); /* 生成新结点（C标准函数） */s-&gt;data = e;s-&gt;next = p-&gt;next; /* 将p的后继结点赋值给s的后继 */p-&gt;next = s; /* 将s赋值给p的后继 */return OK;&#125; C语言的mal-loc标准函数，作用是生成一个新的结点，类型与Node一样，实质就是在内存中找了一小块空地，准备用来存放数据e的s结点 单链表的删除 单链表第i个数据删除结点的算法思路： 声明一指针p指向链表头结点，初始化j从1开始； 当j]]></content>
      <categories>
        <category>数据结构</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构 第四章 栈与队列]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%20%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[4.1 栈的定义栈的定义栈（stack）是限定仅在表尾进行插入和删除操作的线性表 允许插入和删除的一端称为栈顶（top），另一端称为栈底（bottom），不含任何数据元素的栈称为空栈栈又称为后进先出（LastIn First Out）的线性表，简称LIFO结构 栈的插入操作，叫作进栈，也称压栈、入栈 栈的删除操作，叫作出栈，也有的叫作弹栈 4.2 栈的抽象数据类型12345678910111213ADT 栈（stack）Data同线性表。元素具有相同的类型，相邻元素具有前驱和后继关系。OperationInitstack(*S);初始化操作，建立一个空栈S。DestroyStack(*S);若栈存在，则销毁它。ClearStack(*S);将栈清空。StackEmpty(S);若栈为空，返回true;否则返回false。GetTop(S,*e);若栈存在且非空，用e返回S的栈顶元素。Push(*S,e);若栈S存在，插入新元素e到栈S中并成为栈顶元素。又称：进栈，压栈，入栈。Pop(*S,*e);删除栈S中栈顶元素，并用e返回其值。又称：出栈，弹栈。StackLength(S);返回栈S的元素个数。endADT 4.3 栈的顺序存储结构及实现栈的顺序存储结构下标为0的一端作为栈底，因为首元素都存在栈底，变化最小，若存储栈的长度为StackSize，则栈顶位置top必须小于StackSize。当栈存在一个元素时，top等于0，通常把空栈的判定条件定为top等于-1 123456typedef int SElemType; /* SElemType类型根据实际情况而定，这里假设为int */typedef struct&#123;SElemType data[MAXSIZE];int top; /* 用于栈顶指针 */&#125;SqStack; 栈普通情况、空栈和栈满的情况示意图(StackSize是5) 栈的顺序存储结构——进栈操作栈的插入，即进栈操作 1234567891011/* 插入元素e为新的栈顶元素 */Status Push(SqStack *S,SElemType e)&#123;if(S-&gt;top == MAXSIZE -1) /* 栈满 */&#123;return ERROR;&#125;S-&gt;top++; /* 栈顶指针增加一 */S-&gt;data[S-&gt;top]=e; /* 将新插入元素赋值给栈顶空间 */return OK;&#125; 栈的顺序存储结构——出栈操作出栈操作pop： 123456789/* 若栈不空，则删除S的栈顶元素，用e返回其值， 并返回OK；否则返回ERROR */Status Pop(SqStack *S, SElemType *e)&#123;if (S-&gt;top == -1)return ERROR;*e = S-&gt;data[S-&gt;top]; /* 将要删除的栈顶元素赋值给e */S-&gt;top--; /* 栈顶指针减一 */return OK;&#125; 两者时间复杂度均是O(1) 4.4 两栈共享空间用一个数组来存储两个栈，数组有两个端点，两个栈有两个栈底，让一个栈的栈底为数组的始端，即下标为0处，另一个栈为数组的末端，即下标为数组长度n-1处。两个栈如果增加元素，就是两端点向中间延伸 两栈共享空间的结构的代码如下： 1234567/* 两栈共享空间结构 */typedef struct&#123;SElemType data[MAXSIZE];int top1; /* 栈1栈顶指针 */int top2; /* 栈2栈顶指针 */&#125; SqDoubleStack; 对于两栈共享空间的push方法，除了要插入元素值参数外，还需要有一个判断是栈1还是栈2的栈号参数stackNumber 插入元素的代码如下： 12345678910111213141516/* 插入元素e为新的栈顶元素 */Status Push(SqDoubleStack *S, SElemType e, int stackNumber)&#123;/* 栈已满，不能再push新元素了 */if (S-&gt;top1 + 1 == S-&gt;top2)return ERROR;/* 栈1有元素进栈 */if (stackNumber == 1)/* 若栈1则先top1+1后给数组元素赋值 */S-&gt;data[++S-&gt;top1] = e;/* 栈2有元素进栈 */else if (stackNumber == 2)/* 若栈2则先top2-1后给数组元素赋值 */S-&gt;data[--S-&gt;top2] = e;return OK;&#125; 对于两栈共享空间的pop方法，参数就只是判断栈1栈2的参数stackNumber 1234567891011121314151617/* 若栈不空，则删除S的栈顶元素，用e返回其值，并返回OK；否则返回ERROR */Status Pop(SqDoubleStack *S,SElemType *e,int stackNumber)&#123;if (stackNumber==1)&#123;if (S-&gt;top1==-1)return ERROR; /* 说明栈1已经是空栈，溢出 */*e=S-&gt;data[S-&gt;top1--]; /* 将栈1的栈顶元素出栈 */&#125;else if (stackNumber==2)&#123;if (S-&gt;top2==MAXSIZE)return ERROR; /* 说明栈2已经是空栈，溢出 */*e=S-&gt;data[S-&gt;top2++]; /* 将栈2的栈顶元素出栈 */&#125;return OK;&#125; 4.5 栈的链式存储结构及实现栈的链式存储结构栈的链式存储结构，简称为链栈 由于单链表有头指针，而栈顶指针也是必须的，所以比较好的办法是把栈顶放在单链表的头部。另外，都已经有了栈顶在头部了，单链表中比较常用的头结点也就失去了意义，通常对于链栈来说，是不需要头结点的 对于链栈来说，基本不存在栈满的情况，除非内存已经没有可以使用的空间，如果真的发生，那此时的计算机操作系统已经面临死机崩溃的情况，而不是这个链栈是否溢出的问题。但对于空栈来说，链表原定义是头指针指向空，那么链栈的空其实就是top=NULL的时候 链栈的结构代码如下： 12345678910111213/* 链栈结构 */typedef struct StackNode&#123;SElemType data;struct StackNode *next;&#125;StackNode,*LinkStackPtr;typedef struct LinkStack&#123;LinkStackPtr top;int count;&#125;LinkStack; 栈的链式存储结构——进栈操作对于链栈的进栈push操作，假设元素值为e的新结点是s，top为栈顶指针 12345678910/* 插入元素e为新的栈顶元素 */Status Push(LinkStack *S,SElemType e)&#123;LinkStackPtr s=(LinkStackPtr)malloc(sizeof(StackNode));s-&gt;data=e;s-&gt;next=S-&gt;top; /* 把当前的栈顶元素赋值给新结点的直接后继，见图中1 */S-&gt;top=s; /* 将新的结点s赋值给栈顶指针，见图中2 */S-&gt;count++;return OK;&#125; 栈的链式存储结构——出栈操作假设变量p用来存储要删除的栈顶结点，将栈顶指针下移一位，最后释放p即可 12345678910111213/* 若栈不空，则删除S的栈顶元素，用e返回其值，并返回OK；否则返回ERROR */Status Pop(LinkStack *S,SElemType *e)&#123;LinkStackPtr p;if(StackEmpty(*S))return ERROR;*e=S-&gt;top-&gt;data;p=S-&gt;top; /* 将栈顶结点赋值给p，见图中③ */S-&gt;top=S-&gt;top-&gt;next; /* 使得栈顶指针下移一位，指向后一结点，见图中④ */free(p); /* 释放结点p */S-&gt;count--;return OK;&#125; 链栈的进栈push和出栈pop没有任何循环操作，时间复杂度均为O(1)。 顺序栈与链栈： 时间复杂度上均为O(1)。 对于空间性能，顺序栈需要事先确定一个固定的长度，可能会存在内存空间浪费的问题，但它的优势是存取时定位很方便，而链栈则要求每个元素都有指针域，这同时也增加了一些内存开销，但对于栈的长度无限制 如果栈的使用过程中元素变化不可预料，有时很小，有时非常大，那么最好是用链栈，如果它的变化在可控范围内，建议使用顺序栈 4.6 栈的作用栈的引入简化了程序设计的问题，划分了不同关注层次，使得思考范围缩小，更加聚焦于要解决的问题核心。反之，像数组等，因为要分散精力去考虑数组的下标增减等细节问题，反而掩盖了问题的本质 4.7 栈的应用——递归斐波那契数列实现 月数 1 2 3 4 5 6 7 8 9 10 11 12 兔子对数 1 1 2 3 5 8 13 21 34 55 89 144 编号①的一对兔子经过六个月就变成8对兔子 数学函数来定义就是： F(n)=\left\{\begin{matrix} 0,& n = 0 & \\ 1& n = 1 & \\ F(n-1)+F(n-2)& n \gt 1 & \end{matrix}\right.打印出前40位的斐波那契数列数。代码如下： 123456789101112131415161718192021222324252627282930#include "stdio.h"int Fbi(int i) /* 斐波那契的递归函数 */&#123;if( i &lt; 2 )return i == 0 ? 0 : 1;return Fbi(i - 1) + Fbi(i - 2); /* 这里Fbi就是函数自己，等于在调用自己 */&#125;int main()&#123;int i;int a[40];printf("迭代显示斐波那契数列：\n");a[0]=0;a[1]=1;printf("%d ",a[0]);printf("%d ",a[1]);for(i = 2;i &lt; 40;i++)&#123;a[i] = a[i-1] + a[i-2];printf("%d ",a[i]);&#125;printf("\n");printf("递归显示斐波那契数列：\n");for(i = 0;i &lt; 40;i++)printf("%d ", Fbi(i));return 0;&#125; Fbi(i)函数当i=5的执行过程 递归定义把一个直接调用自己或通过一系列的调用语句间接地调用自己的函数，称做递归函数 迭代 递归 循环结构 选择结构 不需要反复调用函数和占用额外的内存 使程序的结构更清晰、更简洁、更容易让人理解，从而减少读懂代码的时间。但是大量的递归调用会建立函数的副本，会耗费大量的时间和内存 在前行阶段，对于每一层递归，函数的局部变量、参数值以及返回地址都被压入栈中。在退回阶段，位于栈顶的局部变量、参数值和返回地址被弹出，用于返回调用层次中执行代码的其余部分，也就是恢复了调用的状态 4.8 栈的应用——四则运算表达式求后缀（逆波兰）表示法定义“9+(3-1)×3+10÷2”用后缀表示法表示：“9 3 1-3*+102/+” 叫后缀的原因在于所有的符号都是在要运算数字的后面出现 后缀表达式计算结果后缀表达式：9 3 1-3*+10 2/+ 规则：从左到右遍历表达式的每个数字和符号，遇到是数字就进栈，遇到是符号，就将处于栈顶两个数字出栈，进行运算，运算结果进栈，一直到最终获得结果 初始化一个空栈。此栈用来对要运算的数字进出使用，如左图 后缀表达式中前三个都是数字，所以9、3、1进栈，如右图 接下来是“-”，所以将栈中的1出栈作为减数，3出栈作为被减数，并运算3-1得到2，再将2进栈，如左图 接着是数字3进栈，如右图 后面是“*”，也就意味着栈中3和2出栈，2与3相乘，得到6，并将6进栈，如左图 下面是“+”，所以栈中6和9出栈，9与6相加，得到15，将15进栈，如图右图 接着是10与2两数字进栈，如左图所示 接下来是符号“/”，因此，栈顶的2与10出栈，10与2相除，得到5，将5进栈，如右图 最后一个是符号“+”，所以15与5出栈并相加，得到20，将20进栈，如图左图 结果是20出栈，栈变为空，如右图 中缀表达式转后缀表达式平时所用的标准四则运算表达式，即“9+(3-1)×3+10÷2”叫做中缀表达式 规则：从左到右遍历中缀表达式的每个数字和符号，若是数字就输出，即成为后缀表达式的一部分；若是符号，则判断其与栈顶符号的优先级，是右括号或优先级不高于栈顶符号（乘除优先加减）则栈顶元素依次出栈并输出，并将当前符号进栈，一直到最终输出后缀表达式为止 初始化一空栈，用来对符号进出栈使用，如左图 第一个字符是数字9，输出9，后面是符号“+”，进栈。如右图 第三个字符是“(”，依然是符号，因其只是左括号，还未配对，故进栈。如左图 第四个字符是数字3，输出，总表达式为93，接着是“-”，进栈。如右图 接下来是数字1，输出，总表达式为 9 3 1，后面是符号“)”，此时，需要去匹配此前的“(”，所以栈顶依次出栈，并输出，直到“(”出栈为止。此时左括号上方只有“-”，因此输出“-”。总的输出表达式为 9 3 1 -。如左图 紧接着是符号“×”，因为此时的栈顶符号为“+”号，优先级低于“×”，因此不输出，“*”进栈。接着是数字3，输出，总的表达式为 9 3 1 - 3。如右图 之后是符号“+”，此时当前栈顶元素“”比这个“+”的优先级高，因此栈中元素出栈并输出（没有比“+”号更低的优先级，所以全部出栈），总输出表达式为9 3 1 - 3 * +。然后将当前这个符号“+”进栈。前6张图的栈底的“+”是指中缀表达式中开头的9后面那个“+”，而下图中的栈底（也是栈顶）的“+”是指“9+(3-1)×3+”中的最后一个“+”。 紧接着数字10，输出，总表达式变为9 3 1 - 3 * + 10。后是符号“÷”，所以“/”进栈。如右图 最后一个数字2，输出，总的表达式为9 31- 3 * + 10 2。如左图 因已经到最后，所以将栈中符号全部出栈并输出。最终输出的后缀表达式结果为9 3 1 - 3 * +10 2 / +。如右图 4.9 队列的定义队列（queue）是只允许在一端进行插入操作，而在另一端进行删除操作的线性表 队列是一种先进先出（First In First Out）的线性表，简称FIFO。允许插入的一端称为队尾，允许删除的一端称为队头。假设队列是q=(a_1,a_2,\cdots,a_n)，那么a_1就是队头元素，而a_n是队尾元素。删除时，总是从a_1开始，而插入时，列在最后 4.10 队列的抽象数据类型12345678910111213ADT 队列(Queue)Data同线性表。元素具有相同的类型，相邻元素具有前驱和后继关系。OperationInitQueue(*Q): 初始化操作，建立一个空队列Q。DestroyQueue(*Q): 若队列Q存在，则销毁它。ClearQueue(*Q): 将队列Q清空。QueueEmpty(Q): 若队列Q为空，返回true，否则返回false。GetHead(Q, *e): 若队列Q存在且非空，用e返回队列Q的队头元素。EnQueue(*Q, e): 若队列Q存在，插入新元素e到队列Q中并成为队尾元素。 DeQueue(*Q, *e): 删除队列Q中队头元素，并用e返回其值。QueueLength(Q): 返回队列Q的元素个数endADT 4.11 循环队列队列顺序存储的不足假设一个队列有n个元素，则顺序存储的队列需建立一个大于n的数组，并把队列的所有元素存储在数组的前n个单元，数组下标为0的一端即是队头。入队列操作就是在队尾追加一个元素，不需要移动任何元素，时间复杂度为O(1) 队列元素的出列是在队头，即下标为0的位置，队列中的所有元素都得向前移动，以保证队列的队头，也就是下标为0的位置不为空，此时时间复杂度为O(n) 为了避免当只有一个元素时，队头和队尾重合使处理变得麻烦，引入两个指针，front指针指向队头元素，rear指针指向队尾元素的下一个位置，当front等于rear时，此队列不是还剩一个元素，而是空队列。 假设是长度为5的数组，初始状态，空队列如左图，front与rear指针均指向下标为0的位置。然后入队a_1a_2,a_3,a_4，front指针依然指向下标为0位置，而rear指针指向下标为4的位置 出队a_1,a_2，front指针指向下标为2的位置，rear不变，如左图，再入队a_5，front指针不变，rear指针移动到数组之外 如果接着入队的话，因数组末尾元素已经占用，再向后加，就会产生数组越界的错误，可实际上，下标为0和1的地方还是空闲的,这种现象叫做假溢出 循环队列定义队列的这种头尾相接的顺序存储结构称为循环队列 rear可以改为指向下标为0的位置 接着入队a_6，将它放置于下标为0处，rear指针指向下标为1处，如左图。再入队a_7，则rear指针就与front指针重合，同时指向下标为2的位置，如右图 如何判断此时的队列究竟是空还是满呢？ 办法一：设置一个标志变量flag，当front==rear，且flag=0时为队列空，当front==rear，且flag=1时为队列满。 办法二：当队列空时，条件就是front=rear，当队列满时，保留一个元素空间，如下图。也就是说，队列满时，数组中还有一个空闲单元，就认为此队列已经满了，即不允许上图右图情况出现 由于rear可能比front大，也可能比front小，若队列的最大尺寸为QueueSize，队列满的条件是 (rear+1)\%QueueSize==front（取模“%”的目的就是为了整合rear与front大小为一个问题）。 比如QueueSize = 5,front=0,rear=4,(4+) \% 5=0，所以此时队列满。比如front=2而rear=1。(1+1)\%5=2，此时队列也是满的。 front=2而rear=0,(0+1)\%5=1,1\neq2，此时队列并没有满。 当rear>front时，队列的长度为rear-front。当rear]]></content>
      <categories>
        <category>数据结构</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构 第七章 图]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E4%B8%83%E7%AB%A0%20%20%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[7.1 图的定义图（Graph）是由顶点的有穷非空集合和顶点之间边的集合组成，通常表示为：G(V,E)，其中G表示一个图，V是图G中顶点的集合，E是图G中边的集合 线性表中数据元素叫元素，树中将数据元素叫结点，图中数据元素称之为顶点（Vertex） 线性表中可以没有数据元素，称为空表。树中可以没有结点，叫做空树。在图结构中，不允许没有顶点。在定义中，若V是顶点的集合，则强调了顶点集合V有穷非空 线性表中，相邻的数据元素之间具有线性关系，树结构中，相邻两层的结点具有层次关系，图中任意两个顶点之间都可能有关系，顶点之间的逻辑关系用边来表示，边集可以是空的 各种图定义若顶点v_i到v_j之间的边没有方向，则称这条边为无向边（Edge），用无序偶对(v_i,v_j)来表示。如果图中任意两个顶点之间的边都是无向边，则称该图为无向图（Undirected graphs） 由于是无方向的，连接顶点A与D的边，可以表示成无序对(A,D)，也可以写成(D,A) 对于无向图G_1： G_1=(V_1,\{E_1\}) 顶点集合V_1=\{A,B,C,D\} 边集合E_1=\{(A,B),(B,C),(C,D),(D,A),(A,C)\} 若从顶点v_i到v_j的边有方向，则称这条边为有向边，也称为弧（Arc）。用有序偶来表示，v_i称为弧尾（Tail），v_j称为弧头（Head）。如果图中任意两个顶点之间的边都是有向边，则称该图为有向图（Directed graphs） 连接顶点A到D的有向边就是弧，A是弧尾，D是弧头，&lt;A,D&gt;表示弧，不能写成&lt;D,A&gt; 对于有向图G_2： G_2=(V_2,\{E_2\}) 顶点集合V_2=\{A,B,C,D\} 弧集合E_2=\{,,,\} 无向边用小括号“()”表示，而有向边则是用尖括号“&lt;&gt;”表示 在图中，若不存在顶点到其自身的边，且同一条边不重复出现，则称这样的图为简单图 两个图就不属于要讨论的范围 在无向图中，如果任意两个顶点之间都存在边，则称该图为无向完全图。含有n个顶点的无向完全图有\frac{n(n-1)}{2}条边 在有向图中，如果任意两个顶点之间都存在方向互为相反的两条弧，则称该图为有向完全图。含有n个顶点的有向完全图有n\times (n-1)条边 对于具有n个顶点和e条边数的图，无向图0≤e≤\frac{n(n-1)}{2}，有向图0≤e≤n(n-1) 有很少条边或弧的图称为稀疏图，反之称为稠密图。稀疏和稠密是模糊的概念，是相对而言的 有些图的边或弧具有与它相关的数字，这种与图的边或弧相关的数叫做权（Weight）。这些权可以表示从一个顶点到另一个顶点的距离或耗费。这种带权的图通常称为网（Network） 假设有两个图G=(V,\{E\})和G{'}=(V{'},\{E{'}\})，如果V{'}\subseteq V且E{'}\subseteq E，则称G{'}为G的子图（Sub-graph） 带底纹的图均为左侧无向图与有向图的子图 图的顶点与边间关系对于无向图G=(V,{E})，如果边(v,v’)∈E，则称顶点v和v’互为邻接点（Adjacent），即v和v’相邻接。边(v,v’)依附（incident）于顶点v和v’，或者说(v,v’)与顶点v和v’相关联。顶点v的度（Degree）是和v相关联的边的数目，记为TD(v)。边数是各顶点度数和的一半，多出的一半是因为重复两次记数,e=\frac{1}{2}\sum_{i=1}^{n}TD\{V_i\} 对于有向图G=(V,{E})，如果弧&lt;v,v’&gt;∈E，则称顶点v邻接到顶点v’，顶点v’邻接自顶点v。弧&lt;v,v’&gt;和顶点v，v’相关联。以顶点v为头的弧的数目称为v的入度（InDegree），记为ID(v)；以v为尾的弧的数目称为v的出度（OutDegree），记为OD(v) 顶点v的度为TD(v)=ID(v)+OD(v),e=\sum_{i=1}^{ n} ID(v_i)=\sum_{i=1}^{n}OD(v_i) 路径的长度是路径上的边或弧的数目 无向图G=(V,{E})中从顶点v到顶点v’的路径（Path）是一个顶点序列(v=v_{i,0},v_{i,1},...,v_{i,m}=v{'})，其中(v_{i,j-1},v_{i,j})∈E，1≤j≤m 顶点B到顶点D四种不同的路径,上方两条路径长度为2，下方两条路径长度为3 如果G是有向图，则路径也是有向的，顶点序列应满足∈E，1≤j≤m。 顶点B到D有两种路径。而顶点A到B，就不存在路径,左侧路径长为2，右侧路径长度为3 第一个顶点和最后一个顶点相同的路径称为回路或环（Cycle）。序列中顶点不重复出现的路径称为简单路径。除了第一个顶点和最后一个顶点之外，其余顶点不重复出现的回路，称为简单回路或简单环 两个图的粗线都构成环，左侧的环因第一个顶点和最后一个顶点都是B，且C、D、A没有重复出现，因此是一个简单环。右侧的环由于顶点C的重复，不是简单环 连通图相关术语在无向图G中，如果从顶点v到顶点v’有路径，则称v和v’是连通的。如果对于图中任意两个顶点v_i,v_j∈V，v_i和v_j都是连通的，则称G是连通图（Connected Graph） 图1的顶点A到顶点B、C、D都是连通的，但显然顶点A与顶点E或F就无路径，因此不能算是连通图。图2顶点A、B、C、D相互都是连通的，所以它本身是连通图 无向图中的极大连通子图称为连通分量。强调： 要是子图； 子图要是连通的； 连通子图含有极大顶点数； 具有极大顶点数的连通子图包含依附于这些顶点的所有边。 图1是一个无向非连通图。但有两个连通分量，即图2和图3。而图4，尽管是图1的子图，但是它却不满足连通子图的极大顶点数（图2满足）。因此它不是图1的无向图的连通分量 在有向图G中，如果对于每一对v_i,v_j∈V、v_i≠v_j，从v_i到v_j和从v_j到v_i都存在路径，则称G是强连通图。有向图中的极大强连通子图称做有向图的强连通分量 图1不是强连通图，因为顶点A到顶点D存在路径，而D到A就不存在。图2是强连通图，而且图2是图1的极大强连通子图，即是它的强连通分量 连通图的生成树是一个极小的连通子图，它含有图中全部的n个顶点，但只有足以构成一棵树的n-1条边。 图1是一普通图，但显然不是生成树，当去掉两条构成环的边后，比如图2或图3，就满足n个顶点n-1条边且连通的定义。它们都是一棵生成树。 如果一个图有n个顶点和小于n-1条边，则是非连通图，如果它多于n-1边条，必定构成一个环，因为这条边使得它依附的那两个顶点之间有了第二条路径。比如图2和图3，随便加哪两顶点的边都将构成环。 不过有n-1条边并不一定是生成树，比如图4 如果一个有向图恰有一个顶点的入度为0，其余顶点的入度均为1，则是一个有向树。入度为0相当于树中的根结点，其余顶点入度为1是说树的非根结点的双亲只有一个。一个有向图的生成森林由若干棵有向树组成，含有图中全部顶点，但只有足以构成若干棵不相交的有向树的弧。 图1是一个有向图。去掉一些弧后，可以分解为两棵有向树，如图2和图3，这两棵就是图1有向图的生成森林 图的定义与术语总结图按照有无方向分为无向图和有向图。无向图由顶点和边构成，有向图由顶点和弧构成。弧有弧尾和弧头之分 图按照边或弧的多少分稀疏图和稠密图。如果任意两个顶点之间都存在边叫完全图，有向的叫有向完全图。若无重复的边或顶点到自身的边则叫简单图 图中顶点之间有邻接点、依附的概念。无向图顶点的边数叫做度，有向图顶点分为入度和出度 图上的边或弧上带权则称为网 图中顶点间存在路径，两顶点存在路径则说明是连通的，如果路径最终回到起始点则称为环，当中不重复叫简单路径。若任意两顶点都是连通的，则图就是连通图，有向则称强连通图。图中有子图，若子图极大，连通则就是连通分量，有向的则称强连通分量 无向图中连通且n个顶点n-1条边叫生成树。有向图中一顶点入度为0其余顶点入度为1的叫有向树。一个有向图由若干棵有向树构成生成森林 7.2 图的抽象数据类型1234567891011121314151617181920212223242526272829303132333435ADT 图(Graph)Data顶点的有穷非空集合和边的集合OperationCreateGraph(*G,V,VR):按照顶点集合V和边弧集VR的定义构造图G。DestroyGraph(*G):图G存在则销毁。LocateVex(G,u):若图G中存在顶点u，则返回图中的位置。GetVex(G,v):返回图G中顶点v的值。PutVex(G,v,value):将图G中顶点v赋值value。FirstAdjvex(G,*v):返回顶点v的一个邻接顶点，若顶点在G中无邻接顶点返回空。NextAdjVex(G,v,*w):返回顶点v相对于顶点w的下一个邻接顶点，若w是v的最后一个邻接点则返回“空”。 InsertVex(*G,v):在图G中增添新顶点v.DeleteVex(*G,v):删除图G中顶点v及其相关的弧。InsertArc(*G,v,w):在图G中增添弧&lt;v,w&gt;,若G是无向图，还需要增添对称弧&lt;w,v&gt;。DeleteArc(*G,v,w):在图G中删除弧&lt;v,w&gt;，若G是无向图，则还删除对称弧&lt;w,v&gt;。DESTraverse(G):对图G中进行深度优先遍历，在遍历过程中对每个顶点调用。HFSTraverse(G):对图G中进行广度优先遍历，在遍历过程中对每个顶点调用。endADT 7.3 图的存储结构从图的逻辑结构定义来看，图上任何一个顶点都可被看成是第一个顶点，任一顶点的邻接点之间也不存在次序关系 同一个图，顶点位置不同 邻接矩阵图的邻接矩阵（Adjacency Matrix）存储方式是用两个数组来表示图。一个一维数组存储图中顶点信息，一个二维数组（称为邻接矩阵）存储图中的边或弧的信息 设图G有n个顶点，则邻接矩阵是一个n×n的方阵，定义为： 顶点数组为vertex[4]=\{v_0,v_1,v_2,v_3\}，边数组arc[4][4]为右图这样的一个矩阵。矩阵的主对角线的值，即arc[0][0]、arc[1][1]、arc[2][2]、arc[3][3]全为0是因为不存在顶点到自身的边，无向图的边数组是一个对称矩阵。对称矩阵就是n阶矩阵的元满足a_{ij}=a_{ji}，（0≤i,j≤n）。即从矩阵的左上角到右下角的主对角线为轴，右上角的元与左下角相对应的元全都是相等的 有了这个矩阵，很容易知道图中的信息： 判定任意两顶点是否有边无边 要知道某个顶点的度，就是这个顶点v_i在邻接矩阵中第i行（或第i列）的元素之和。 求顶点v_i的所有邻接点就是将矩阵中第i行元素扫描一遍，arc[i][j]为1就是邻接点 顶点数组为vertex[4]=\{v_0,v_1,v_2,v_3\}，弧数组arc[4][4]为右图的一个矩阵。主对角线上数值依然为0。但因为是有向图，所以此矩阵并不对称 有向图讲究入度与出度，顶点v_1的入度为1，正好是第v_1列各数之和。顶点v_1的出度为2，即第v_1行的各数之和 判断顶点v_i到v_j是否存在弧，只需要查找矩阵中arc[i][j]是否为1。求v_i的所有邻接点就是将矩阵第i行元素扫描一遍，查找arc[i][j]为1的顶点 设图G是网图，有n个顶点，则邻接矩阵是一个n×n的方阵，定义为： arc[i][j]]=\left\{\begin{matrix} W_{ij}&if\quad (v_i,v_j) \in E \quad or\quad \in E \\ 0 &if\quad i=j \\ \infty &other \end{matrix}\right. $w_{ij}$表示(v_i,v_j)或上的权值。∞表示一个计算机允许的、大于所有边上权值的值，也就是一个不可能的极限值 图的邻接矩阵存储结构的代码： 12345678910typedef char VertexType; //顶点类型，由用户定义typedef int EdgeType; //边上的权值类型，由用户定义#define MAXVEX 100 //最大顶点数，应由用户定义#define INFINITY 65535 //用65535来代表∞typedef struct&#123;VertexType vexs[MAXVEX]; //顶点表EdgeType arc[MAXVEX][MAXVEX]; //邻接矩阵，可看作边表int numVertexes,numEdges; //图中当前的顶点数和边数&#125;MGraph; 有了以上的结构定义，构造一个图就是给顶点表和边表输入数据的过程。 无向网图的创建代码： 123456789101112131415161718void CreateMGraph(MGraph *G)&#123;int i,j,k,w;printf("输入顶点数和边数：\n");scanf("%d,%d",&amp;G-&gt;numVertexes,&amp;G-&gt;numEdges);//输入顶点数和边数for(i=0;i&lt;G-&gt;numVertexes;i++)scanf(&amp;G-&gt;vexs[i]);for(i=0;i&lt;G-&gt;numVertexes;i++)for(j=0;j&lt;G-&gt;numVertexes;j++)G-&gt;arc[i][j] = INFINITY;//邻接矩阵初始化for(k=0;k&lt;G-&gt;numEdges;k++) //读入numEdges条边，建立邻接矩阵&#123;printf("输入边(vi,vj)上的下标i,下标j和权w:\n");scanf("%d,%d,%d",&amp;i,&amp;j,&amp;w); //输入边(vi,vj)上的权wG-&gt;arc[i][j] = w;G-&gt;arc[j][i] = G-&gt;arc[i][j];//因为是无向图，矩阵对称。&#125;&#125; 从代码中也可以得到，n个顶点和e条边的无向网图的创建，时间复杂度为O(n+n^2+e),其中对邻接矩阵G-&gt;arc的初始化耗费了O(n^2)的时间 邻接矩阵对于边数相对顶点较少的图来说对存储空间是极大的浪费 邻接表数组与链表相结合的存储方法称为邻接表(Adjacency List) 邻接表的处理办法： 图中顶点用一个一维数组存储，对于顶点数组中，每个数据元素还需要存储指向第一个邻接点的指针，以便于查找该顶点的边信息 图中每个顶点v_i的所有邻接点构成一个线性表，由于邻接点的个数不定，所以用单链表存储，无向图称为顶点v_i的边表，有向图则称为顶点v_i作为弧尾的出边表 顶点表的各个结点由data 和 firstedge两个域表示，data是数据域，存储顶点的信息，firstedge是指针域，指向边表的第一个结点，即此结点的第一个邻接点 边表结点由adjvex和next 两个域组成，adjvex 是邻接点域，存储某顶点的邻接点在顶点表中的下标，next则存储指向边表中下一个结点的指针 要想知道某个顶点的度，就去查找这个顶点的边表中结点的个数。若要判断顶点v_i到v_j是否存在边，只需要测试顶点v_i的边表中adjvex是否存在结点v_j的下标j就行了。若求顶点的所有邻接点，其实就是对此顶点的边表进行遍历，得到的adjvex域对应的顶点就是邻接点 若是有向图，第一幅图的邻接表就是第二幅图。有向图由于有方向，是以顶点为弧尾来存储边表的，很容易得到每个顶点的出度。有时为了便于确定顶点的入度或以顶点为弧头的弧，可以建立一个有向图的逆邻接表，即对每个顶点v_i都建立一个链接为v_i为弧头的表 对于带权值的网图，可以在边表结点定义中再增加一个weight的数据域存储权值信息 结点定义的代码: 123456789101112131415161718192021typedef char VertexType; //顶点类型，由用户定义typedef int EdgeType;typedef struct EdgeNode //边表结点&#123;int adjvex; //邻接点域，存储该顶点对应的下标EdgeType weight; //用于存储权值，对于非网图可以不需要struct EdgeNode *next;//链域，指向下一个邻接点&#125;EdgeNode;typedef struct VertexNode //顶点表结点&#123;VertexType data; //顶点域，存储顶点信息EdgeNode *firstedge; //边表头指针&#125;VertexNode,AdjList[MAXVEX];typedef struct&#123;AdjList adjList;int numVertexes,numEdges; //图中当前顶点数和边数&#125;GraphAdjList; 无向图的邻接表创建代码： 12345678910111213141516171819202122232425262728/* 建立图的邻接表结构 */void CreateALGraph(GraphAdjList *G)&#123;int i,j,k;EdgeNode *e;printf("输入顶点数和边数：\n");scanf("%d,%d",&amp;G-&gt;numVertexes,&amp;G-&gt;numEdges);//输入顶点数和边数for(i=0;i&lt;G-&gt;numVertexes,i++)&#123;scanf(&amp;G-&gt;adjList[i].data);//输入顶点信息G-&gt;adjList[i].firstedge=NULL;//将边表置位空表&#125;for(k=0;k&lt;G-&gt;numEdges;k++)&#123;printf("输入边(vi,vj)上的顶点序号：\n");scanf("%d,%d",&amp;i,&amp;j);//输入边(vi,vj)上的顶点序号e=(EdgeNode *)malloc(sizeof(EdgeNode));/*向内存申请空间，生成边表结点*/e-&gt;adjvex = j;//邻接序号为je-&gt;next = G-&gt;adjList[i].firstedge;//将e指针指向当前顶点指向的结点G-&gt;adjList[i].firstedge = e;//将当前顶点的指针指向ee=(EdgeNode *) malloc(sizeof(EdgeNode));/*向内存申请空间，生成边表结点*/e-&gt;adjvex = i;//邻接序号为ie-&gt;next=G-adjList[j].firstedge;//将e指针指向当前顶点指向的结点G-&gt;adjList[j].firstedge = e;//将当前顶点的指针指向e&#125;&#125; 对于无向图，一条边对应都是两个顶点，所以在循环中，一次就针对i和j分别进行了插入。本算法的时间复杂度，对于n个顶点e条边来说是O(n+e) 邻接表的缺陷： 关心了出度问题，想了解入度就必须要遍历整个图才能知道 逆邻接表解决了入度却不了解出度的情况 十字链表十字链表（Orthogonal List）：把邻接表和逆邻接表结合起来的存储方式 重新定义顶点表结点结构如下: data firstin firstout firstin表示入边表头指针，指向该顶点的入边表中第一个结点，firstout表示出边表头指针，指向该顶点的出边表中的第一个结点 重新定义的边表结点结构如下： tailvex headvex headlink taillink 顶点依然是存入一个一维数组{v_0,v_1,v_2,v_3}，实线箭头指针的图示邻接表相同。以顶点v_0来说，firstout指向的是出边表中的第一个结点v_3。所以v_0边表结点的headvex=3，而tailvex其实就是当前顶点v_0的下标0，由于v_0只有一个出边顶点，所以headlink和taillink都是空 虚线箭头的含义此图的逆邻接表的表示。v_0有两个顶点v_1和v_2的入边。因此v_0的firstin指向顶点v_1的边表结点中headvex为0的结点，如图中的①。接着由入边结点的headlink指向下一个入边顶点v_2，如图中的②。对于顶点v_1，它有一个入边顶点v_2，所以它的firstin指向顶点v_2的边表结点中headvex为1的结点，如图中的③。顶点v_2和v_3也是同样有一个入边顶点，如图中④和⑤ 十字链表的好处： 把邻接表和逆邻接表整合在了一起，容易找到以v_i为尾的弧和以v_i为头的弧，容易求得顶点的出度和入度 除了结构复杂一点外，创建图算法的时间复杂度和邻接表相同的 邻接多重表重新定义的边表结点结构如下： ivex ilink jvex jlink ivex和jvex是与某条边依附的两个顶点在顶点表中的下标。ilink指向依附顶点ivex的下一条边，jlink指向依附顶点jvex的下一条边。这就是邻接多重表结构 首先连线的①②③④就是将顶点的firstedge指向一条边，顶点下标要与ivex的值相同，接着，由于顶点v_0的(v_0,v_1)边的邻边有(v_0,v_3)和(v_0,v_2)。因此⑤⑥的连线就是满足指向下一条依附于顶点v_0的边的目标，ilink指向的结点的jvex一定要和它本身的ivex的值相同。连线⑦就是指(v_1,v_0)这条边，相当于顶点v_1指向(v_1,v_2)边后的下一条。v_2有三条边依附，所以在③之后就有了⑧⑨。连线⑩的就是顶点v_3在连线④之后的下一条边。左图一共有5条边，所以右图有10条连线 邻接多重表和邻接表的区别：同一条边在邻接表中用两个结点表示，在邻接多重表中只有一个结点。若要删除左图的(v_0,v_2)这条边，只需要将右图的⑥⑨的链接指向改为∧即可 边集数组边集数组是由两个一维数组构成。一个是存储顶点的信息；另一个是存储边的信息，这个边数组每个数据元素由一条边的起点下标(begin)、终点下标(end)和权(weight)组成 定义的边数组结构如下所示： begin end weight begin是存储起点下标，end是存储终点下标，weight是存储权值 边集数组关注的是边的集合，在边集数组中要查找一个顶点的度需要扫描整个边数组，效率并不高。因此它更适合对边依次进行处理的操作，而不适合对顶点相关的操作 7.4 图的遍历从图中的某一顶点出发，访遍图中其余顶点，且使每一顶点仅被访问一次，这一过程就叫做图的遍历(Traversing Graph) 深度优先遍历深度优先遍历(Depth First Search)，也有称为深度优先搜索，简称为DFS。类似于树的前序遍历 从图中某个顶点v出发，访问此顶点，然后从v的未被访问的邻接点出发深度优先遍历图，直至图中所有和v有路径相通的顶点都被访问到，以上说的只是连通图，对于非连通图，只需要对它的连通分量分别进行深度优先遍历，即在先前一个顶点进行一次深度优先遍历后，若图中尚有顶点未被访问，则另选图中一个未曾被访问的顶点作为起始点，重复上述过程，直至图中所有顶点都被访问到位置 访问数组visited[n]，n是图中顶点的个数，初值为0，访问过后设置为1 用邻接矩阵的方式来遍历： 1234567891011121314151617181920212223typedef int Boolean;//Boolean 是布尔类型，其值是TRUE或FALSEBoolean visited[MAX];//访问标志的数组/* 邻接矩阵的深度优先递归算法*/void DFS(MGraph G,int i)&#123;int j;visited[i]=TRUE;printf("%c",G.vexs[i]);//打印顶点，也可以其他操作for(j=0;j&lt;G.numVertexes;j++)if(G.arc[i][j]==1&amp;&amp;!visited[j])DFS(G,j); //对为访问的邻接顶点递归调用&#125;/*邻接矩阵的深度优先遍历*/void DFSTraverse(MGraph G)&#123;int i;for(i=0;i&lt;G.numVertexes;i++)visited[i] = FALSE; /* 初始所有顶点状态都是未访问过状态 */for(i=0;i&lt;numVertexes;i++)if(!visited[i]) /* 对未访问过的顶点调用DFS，若是连通图，只会执行一次 */DFS(G,i);&#125; 如果图结构为邻接表结构，其DFSTraverse函数的代码是几乎相同的，只是在递归函数中因为将数组换成了链表而有不同，代码如下： 12345678910111213141516171819202122232425/*邻接表的深度优先递归算法*/void DFS(GraphAdjList GL,int i)&#123;EdgeNode *p;visited[i] = TRUE;printf("%c",GL-&gt;adjList[i].data);//打印顶点，也可以其他操作p = GL-&gt;adjList[i].firstedge;while(p)&#123;if(!visited[p-&gt;adjvex]) /* 对未访问的邻接顶点递归调用 */DFS(GL,p-&gt;adjvex);p = p-&gt;next;&#125;&#125;/*邻接表的深度遍历操作*/void DFSTraverse(GraphAdjList GL)&#123;int i;for (i = 0;i&lt;GL-&gt;numVertexes;i++)visited[i] = FALSE;//初始所有顶点状态都是未访问过状态for (i = 0;i&lt;GL-&gt;numVertexes;i++)if(!visited[i])//对未访问过的顶点调用DFS，若是连通图，只会执行一次DFS(GL,i);&#125; 对于n个顶点e条边的图来说，邻接矩阵由于是二维数组，要查找每个顶点的邻接点需要访问矩阵中的所有元素，因此需要O(n^2)的时间。而邻接表做存储结构时，找邻接点所需的时间取决于顶点和边的数量，所以是O(n+e)。对于点多边少的稀疏图来说，邻接表结构使得算法在时间效率上大大提高 广度优先遍历广度优先遍历(Breadth_First_Search)，又称为广度优先搜索，简称为BFS。类似于树的层序遍历。 将第一幅图稍微变形，变形原则是顶点A放置在最上第一层，让与它有边的顶点B、F为第二层，再让与B和F有边的顶点C、I、G、E为第三层，再将这四个顶点有边的D、H放在第四层 邻接矩阵的广度优先遍历算法： 12345678910111213141516171819202122232425262728293031void BFSTraverse(MGraph G)&#123;int i,j;Queue Q;for(i = 0;i&lt;G.numVertexes;i++)visited[i] = FALSE;InitQueue(&amp;Q);//初始化一辅助用的队列for(i=0;i&lt;G.numVertexes;i++) //对每一个顶点做循环&#123;if(!visited[i])//若是未访问过&#123;visited[i] = TRUE;//设置当前顶点访问过printf("%c",G.vexs[i]);//打印顶点，也可以其他操作EnQueue(&amp;Q,i);//将此顶点如队列while(!QueueEmpty(Q))//若当前队列不为空&#123;DeQueue(&amp;Q,&amp;i);//将队列中元素出队列，赋值给ifor(j=0;j&lt;G.numVertexes;j++)&#123;//判断其他顶点若与当前顶点存在边且未访问过if(G.arc[i][j]==1&amp;&amp;!visited[j])&#123;visited[j] = TRUE;//将找到的此顶点标记为已访问过printf("%c",G.vexs[j]);//打印顶点EnQueue(&amp;Q,j);//将找到的此顶点入队列&#125;&#125;&#125;&#125;&#125;&#125; 邻接表的广度优先遍历： 12345678910111213141516171819202122232425262728293031323334/*邻接表的广度遍历算法*/void BFSTraverse(GraphAdjList GL)&#123;int i;EdgeNode *p;Queue Q;for(i=0;i&lt;GL-&gt;numVertexes;i++)visited = FALSE;InitQueue(&amp;Q);for(i = 0;i&lt;GL-&gt;numVertexes;i++)&#123;if(!visited[i])&#123;visited[i]=TRUE;printf("%c",GL-&gt;adjList[i].data);//打印顶点，也可以其他操作EnQueue(&amp;Q,i);while(!QueueEmpty(Q))&#123;DeQueue(&amp;Q,&amp;i);p = GL-&gt;adjList[i].firstedge;//找到当前顶点边表链表头指针while(p)&#123;if(!visited[p-&gt;adjvex)//若此顶点未被访问&#123;visited[p-&gt;adjvex] = TRUE;printf("%c",GL-&gt;adjList[p-&gt;adjvex].data);EnQueue(&amp;Q,p-&gt;adjvex);//将此顶点如队列&#125;p = p-&gt;next;//指针指向下一个邻接点&#125;&#125;&#125;&#125;&#125; 图的深度优先遍历和广度优先遍历算法在时间复杂度上一样，不同之处仅在于对顶点的访问次序不同深度优先算法更适合目标比较明确的。以找到目标为主要目的的情况，而广度优先更适合在不断扩大遍历范围时找到相对最优解的情况 7.5 最小生成树最小成本，就是n个顶点，用n-1条边把一个连通图连接起来，并且使得权值的和最小 最小生成树：构造连通网的最小代价生成树 普里姆（Prim）算法 存储结构为MGragh的G有9个顶点，用65535来代表∞ 12345678910111213141516171819202122232425262728293031323334353637void MiniSpanTree_Prim(MGraph MG) /* Prim算法生成最小生成树 */&#123;int min, i, j, k;int adjvex[MAXVEX];/* 保存相关顶点下标 */int lowcost[MAXVEX];/* 保存相关顶点间边的权值 */lowcost[0] = 0;/* 初始化第一个权值为0，即v0加入生成树,lowcost的值为0，在这里就是此下标的顶点已经加入生成树 */adjvex[0] = 0;/* 初始化第一个顶点下标为0 */for (i = 1; i &lt; MG.numVertexes; i++) /* 循环除下标为0外的全部顶点 */&#123;lowcost[i] = MG.arc[0][i];/* 将v0顶点与之有边的权值存入数组 */adjvex[i] = 0;/* 初始化都为v0的下标 */&#125;for (i = 1; i &lt; MG.numVertexes; i++)&#123;min = INFINITY; /* 初始化最小权值为∞ */j = 1; k = 0;while (j &lt; MG.numVertexes)/* 循环全部顶点 */&#123;if (lowcost[j] != 0 &amp;&amp; lowcost[j] &lt; min)&#123; /* 如果权值不为0且权值小于min */min = lowcost[j];/* 则让当前权值成为最小值 */k = j;/* 将当前最小值的下标存入k */&#125;j++;&#125;printf("(%d,%d)", adjvex[k], k); /* 打印当前顶点边中权值最小的边 */lowcost[k] = 0;/* 将当前顶点的权值设置为0,表示此顶点已经完成任务 */for (j = 1; j &lt; MG.numVertexes; j++)/* 循环所有顶点 */&#123;if (lowcost[j] != 0 &amp;&amp; MG.arc[k][j] &lt; lowcost[j])&#123; /* 如果下标为k顶点各边权值小于此前这些顶点未被加入生成树权值 */lowcost[j] = MG.arc[k][j];/* 将较小的权值存入lowcost相应位置 */adjvex[j] = k;/* 将下标为k的顶点存入adjvex */&#125;&#125;&#125;&#125; 程序开始运行，第4～5行，创建了两个一维数组lowcost和adjvex，长度都为顶点个数9 第6～7行分别给这两个数组的第一个下标位赋值为0，adjvex[0]=0意思是从顶点v_0开始，lowcost[0]=0就表示v_0已经被纳入到最小生成树中，之后凡是lowcost数组中的值被设置为0就是表示此下标的顶点被纳入最小生成树。 第8～12行表示读取右图邻接矩阵的第一行数据。将数值赋值给lowcost数组，此时lowcost数组值为{0,10,65535,65535,65535,11,65535,65535,65535}，而adjvex则全部为0。此时已经完成了整个初始化的工作，准备开始生成 第13～36行，整个循环过程就是构造最小生成树的过程 第15～16行，将min设置为了一个极大值65535，目的是为了之后找到一定范围内的最小权值。j是用来做顶点下标循环的变量，k是用来存储最小权值的顶点下标 第17～25行，循环中不断修改min为当前lowcost数组中最小值，并用k保留此最小值的顶点下标。经过循环后，min=10，k=1。19行if判断的lowcost[j]!=0表示已经是生成树的顶点不参与最小权值的查找 第26行，因k=1，adjvex[1]=0，所以打印结果为(0,1)，表示v_0至v_1边为最小生成树的第一条边 第27行，此时因k=1将lowcost[k]=0就是说顶点v_1纳入到最小生成树中。此时lowcost数组值为{0,0,65535,65535,65535,11,65535,65535,65535}。 第28～35行，j循环由1至8，因k=1，查找邻接矩阵的第v_1行的各个权值，与low-cost的对应值比较，若更小则修改low-cost值，并将k值存入adjvex数组中。因第v_1行有18、16、12均比65535小，所以最终lowcost数组的值为：{0,0,18,65535,65535,11,16,65535,12}。adjvex数组的值为：{0,0,1,0,0,0,1,0,1}。第30行if判断的lowcost[j]!=0也说明v_0和v_1已经是生成树的顶点不参与最小权值的比对 再次循环，由第15行到第26行，此时min=11，k=5，adjvex[5]=0。打印结构为(0,5)。表示v_0至v_5边为最小生成树的第二条边 接下来执行到36行，lowcost数组的值为：{0,0,18,65535,26,0,16,65535,12}。ad-jvex数组的值为：{0,0,1,0,5,0,1,0,1}。 之后通过不断的转换，构造的过程如图1～图6 普里姆（Prim）算法的实现定义： 假设N=(V,{E})是连通网，TE是N上最小生成树中边的集合。算法从U={u_0}(u_0∈V)，TE={}开始。重复执行下述操作：在所有u∈U,v∈V-U的边(u,v)∈E中找一条代价最小的边(u_0,v_0)并入集合TE，同时v_0并入U，直至U=V为止。此时TE中必有n-1条边，则T=(V,{TE})为N的最小生成树 由算法代码中的循环嵌套可得知此算法的时间复杂度为O(n^2) 克鲁斯卡尔（Kruskal）算法edge边集数组结构的定义代码： 1234567/* 对边集数组Edge结构的定义 */typedef struct&#123;int begin;int end;int weight;&#125; Edge; 将邻接矩阵通过程序转化为右图的边集数组，并且对它们按权值从小到大排序 123456789101112131415161718192021222324void MiniSpanTree_Kruskal(MGraph G) // Kruskal算法生成最小生成树&#123;int i, n, m;Edge edges[MAGEDGE]; // 定义边集数组int parent[MAXVEX]; // 定义parent数组用来判断边与边是否形成环路for( i=0; i &lt; G.numVertexes; i++ )parent[i] = 0;for( i=0; i &lt; G.numEdges; i++ )&#123;n = Find(parent, edges[i].begin); // 4 2 0 1 5 3 8 6 6 6 7m = Find(parent, edges[i].end); // 7 8 1 5 8 7 6 6 6 7 7if( n != m ) // 如果n==m，则形成环路，不满足！&#123;parent[n] = m; // 将此边的结尾顶点放入下标为起点的parent数组中，表示此顶点已经在生成树集合中printf("(%d, %d) %d ", edges[i].begin, edges[i].end, edges[i].weight);&#125;&#125;&#125;int Find(int *parent, int f) //查找连线顶点的尾部小标&#123;while( parent[f] &gt; 0 )f = parent[f];return f;&#125; 程序开始运行，第5行之后，省略掉颇占篇幅但却很容易实现的将邻接矩阵转换为边集数组，并按权值从小到大排序的代码，也就是说，在第5行开始，已经有了结构为edge，数据内容是上图右图的一维数组edges。 第5～7行，声明一个数组parent，并将它的值都初始化为0 第8～17行，开始对边集数组做循环遍历，开始时，i=0。 第10行，调用了第19～25行的函数Find，传入的参数是数组parent和当前权值最小边(v_4,v_7)的begin:4。因为parent中全都是0所以传出值使得n=4。 第11行，传入(v_4,v_7)的end:7。传出值使得m=7。 第12～16行，n与m不相等，因此parent[4]=7。此时parent数组值为{0,0,0,0,7,0,0,0,0}，并且打印得到“(4,7)7”。此时已经将边(v_4,v_7)纳入到最小生成树中 循环返回，执行10～16行，此时i=1，edge[1]得到边(v_2,v_8)，n=2，m=8，parent[2]=8，打印结果为“(2,8)8”，此时parent数组值为{0,0,8,0,7,0,0,0,0}，这也就表示边(v_4,v_7)和边(v_2,v_8)已经纳入到最小生成树 再次执行10～16行，此时i=2，edge[2]得到边(v_0,v_1)，n=0，m=1，parent[0]=1，打印结果为“(0,1)10”，此时parent数组值为{1,0,8,0,7,0,0,0,0}，此时边(v_4,v_7)、(v_2,v_8)和(v_0,v_1)已经纳入到最小生成树 当i=3、4、5、6时，分别将边(v_0,v_5),(v_1,v_8),(v_3,v_7),(v_1,v_6)纳入到最小生成树中。此时parent数组值为{1,5,8,7,7,8,0,0,6} 从图i=6的粗线连线可以得到，有两个连通的边集合A与B中纳入到最小生成树中的。当parent[0]=1，表示v_0和v_1已经在生成树的边集合A中。此时将parent[0]=1的1改为下标，由parent[1]=5，表示v_1和v_5在边集合A中，parent[5]=8表示v_5与v_8在边集合A中，parent[8]=6表示v_8与v_6在边集合A中，parent[6]=0表示集合A暂时到头，此时边集合A有v_0,v_1,v_5,v_8,v_6。查看parent中没有查看的值，parent[2]=8表示v_2与v_8在一个集合中，因此v_2也在边集合A中。再由parent[3]=7、parent[4]=7和parent[7]=0可知v_3,v_4,v_7在另一个边集合B中。 当i=7时，第10行，调用Find函数，会传入参数edges[7].begin=5。此时第21行，parent[5]=8&gt;0，所以f=8，再循环得parent[8]=6。因parent[6]=0所以Find返回后第10行得到n=6。而此时第11行，传入参数edges[7].end=6得到m=6。此时n=m，不再打印，继续下一循环。因为边(v_5,v_6)使得边集合A形成了环路。因此不能将它纳入到最小生成树中，如上图所示。 当i=8时，与上面相同，由于边(v_1,v_2)使得边集合A形成了环路。因此不能将它纳入到最小生成树中，如上图所示。 当i=9时，边(v_6,v_7)，第10行得到n=6，第11行得到m=7，因此parent[6]=7，打印“(6,7)19”。此时parent数组值为{1,5,8,7,7,8,7,0,6}。此后边的循环均造成环路，最终最小生成树即下图所示、 克鲁斯卡尔（Kruskal）算法的实现定义： 假设N=(V,{E})是连通网，则令最小生成树的初始状态为只有n个顶点而无边的非连通图T={V,{}}，图中每个顶点自成一个连通分量。在E中选择代价最小的边，若该边依附的顶点落在T中不同的连通分量上，则将此边加入到T中，否则舍去此边而选择下一条代价最小的边。依次类推，直至T中所有顶点都在同一连通分量上为止 此算法的Find函数由边数e决定，时间复杂度为O(loge)，而外面有一个for循环e次。所以克鲁斯卡尔算法的时间复杂度为O(eloge) 克鲁斯卡尔算法主要是针对边来展开，边数少时效率会非常高，对于稀疏图有很大的优势；普里姆算法对于稠密图，即边数非常多的情况会更好一些 7.6 最短路径非网图的最短路径，是指两顶点之间经过的边数最少的路径；网图的最短路径是指两顶点之间经过的边上权值之和最少的路径，并且称路径上的第一个顶点是源点，最后一个顶点是终点。就地图来说，距离就是两顶点间的权值之和。非网图可以理解为所有的边的权值都为1的网 迪杰斯特拉（Dijkstra）算法12345#define MAXVEX 9#define INFINITY 65535 // 是 2^16 ，而不是 2^32，因为这样可以防止溢出typedef int Patharc[MAXVEX]; // 用于存储最短路径下标的数组typedef int ShortPathTable[MAXVEX]; // 用于存储到各点最短路径的权值和 12345678910111213141516171819202122232425262728293031323334void ShortestPath_Dijkstar(MGraph G, int V0, Patharc *P, ShortPathTable *D)&#123;int v, w, k, min;int final[MAXVEX]; // final[w] = 1 表示已经求得顶点V0到Vw的最短路径for( v=0; v &lt; G.numVertexes; v++ ) // 初始化数据&#123;final[v] = 0; // 全部顶点初始化为未找到最短路径(*D)[V] = G.arc[V0][v]; // 将与V0点有连线的顶点加上权值(*P)[V] = 0; // 初始化路径数组P为0&#125;(*D)[V0] = 0; // V0至V0的路径为0final[V0] = 1; // V0至V0不需要求路径for( v=1; v &lt; G.numVertexes; v++ ) // 开始主循环，每次求得V0到某个V顶点的最短路径&#123;min = INFINITY;for( w=0; w &lt; G.numVertexes; w++ )&#123;if( !final[w] &amp;&amp; (*D)[w]&lt;min )&#123;k = w;min = (*D)[w];&#125;&#125;final[k] = 1; // 将目前找到的最近的顶点置1for( w=0; w &lt; G.numVextexes; w++ ) // 修正当前最短路径及距离&#123;if( !final[w] &amp;&amp; (min+G.arc[k][w] &lt; (*D)[w]) ) // 如经过v顶点的路径比现在这条路径的长度短的话，更新&#123;(*D)[w] = min + G.arc[k][w]; // 修改当前路径长度(*p)[w] = k; // 存放前驱顶点&#125;&#125;&#125;&#125; 程序开始运行，第4行final数组是为了v_0到某顶点是否已经求得最短路径的标记，如果v_0到v_w已经有结果，则final[w]=1。 第5～10行，是在对数据进行初始化的工作。此时final数组值均为0，表示所有的点都未求得最短路径。D数组为{65535,1,5,65535,65535,65535,65535,65535,65535}。因为v_0与v_1和v_2的边权值为1和5。P数组全为0，表示目前没有路径。 第11行，表示v_0到v_0自身，权值和结果为0。D数组为{0,1,5,65535,65535,65535,65535,65535,65535}。第12行，表示v_0点算是已经求得最短路径，因此final[0]=1。此时final数组为{1,0,0,0,0,0,0,0,0}。此时整个初始化工作完成。 第13～33行，为主循环，每次循环求得v_0与一个顶点的最短路径。因此v从1而不是0开始。 第15～23行，先令min为65535的极大值，通过w循环，与D[w]比较找到最小值min=1，k=1。 第24行，由k=1，表示与v_0最近的顶点是v_1，并且由D[1]=1，知道此时v_0到v_1的最短距离是1。因此将v_1对应的final[1]设置为1。此时final数组为{1,1,0,0,0,0,0,0,0}。 第25～32行是一循环，目的是在刚才已经找到v_0与v_1的最短路径的基础上，对v_1与其他顶点的边进行计算，得到v_0与它们的当前最短距离，如下图。因为min=1，所以本来D[2]=5，现在v_0\rightarrow v_1\rightarrow v_2=D[2]=min+3=4,v_0\rightarrow v_1\rightarrow v_3=D[3]=min+7=8,v_0\rightarrow v_1\rightarrow v_4=D[4]=min+5=6，因此，D数组当前值为{0,1,4,8,6,65535,65535,65535,65535}。而P[2]=1，P[3]=1，P[4]=1，它表示的意思是v_0到v_2,v_3,v_4点的最短路径它们的前驱均是v_1。此时P数组值为：{0,0,1,1,1,0,0,0,0}。 重新开始循环，此时v=2。第15～23行，对w循环，注意因为final[0]=1和fi-nal[1]=1，由第18行的!final[w]可知，v_0与v_1并不参与最小值的获取。通过循环比较，找到最小值min=4，k=2。 第24行，由k=2，表示已经求出v_{0}到v_{2}的最短路径，并且由D[2]=4，知道最短距离是4。因此将v_2对应的final[2]设置为1，此时final数组为：{1,1,1,0,0,0,0,0,0}。 第25～32行。在刚才已经找到v_0与v_2的最短路径的基础上，对v_2与其他顶点的边，进行计算，得到v_0与它们的当前最短距离，如下图。因为min=4，所以本来D[4]=6，现在v_0\rightarrow v_2\rightarrow v_4=D[4]=min+1=5,v_0\rightarrow v_2\rightarrow v_5=D[5]=min+7=11，因此，D数组当前值为：{0,1,4,8,5,11,65535,65535,65535}。而原本P[4]=1，此时P[4]=2，P[5]=2，它表示v_0到v_4,v_5点的最短路径它们的前驱均是v_2。此时P数组值为：{0,0,1,1,2,2,0,0,0} 重新开始循环，此时v=3。第15～23行，通过对w循环比较找到最小值min=5，k=4。 第24行，由k=4，表示已经求出v_0到v_4的最短路径，并且由D[4]=5，知道最短距离是5。因此将v_4对应的final[4]设置为1。此时final数组为：{1,1,1,0,1,0,0,0,0}。 第25～32行。对v_4与其他顶点的边进行计算，得到v_0与它们的当前最短距离，如图。因为min=5，所以本来D[3]=8，现在v_0\rightarrow v_4\rightarrow v_3=D[3]=min+2=7，本来D[5]=11，现在v_0\rightarrow v_4\rightarrow v_5=D[5]=min+3=8，另外v_0\rightarrow v_4\rightarrow v_6=D[6]=min+6=11，v_0\rightarrow v_4\rightarrow v_7=D[7]=min+9=14，因此，D数组当前值为：{0,1,4,7,5,8,11,14,65535}。而原本P[3]=1，此时P[3]=4，原本P[5]=2，此时P[5]=4，另外P[6]=4，P[7]=4，它表示v_0到v_3,v_5,v_6,v_7点的最短路径它们的前驱均是v_4。此时P数组值为：{0,0,1,4,2,4,4,4,0}。 得到最终的结果如图。此时final数组为：{1,1,1,1,1,1,1,1,1}，它表示所有的顶点均完成了最短路径的查找工作。此时D数组为：{0,1,4,7,5,8,10,12,16}，它表示v_0到各个顶点的最短路径数，比如D[8]=1+3+1+2+3+2+4=16。此时的P数组为：{0,0,1,4,2,4,3,6,7}，P[8]=7意思是v_0到v_8的最短路径，顶点v_8的前驱顶点是v_7，再由P[7]=6表示v_7的前驱是v_6，P[6]=3，表示v_6的前驱是v_3。这样就可以得到，v_0到v_8的最短路径即v_0\rightarrow v_1\rightarrow v_2\rightarrow v_4\rightarrow v_3\rightarrow v_6\rightarrow v_7\rightarrow v_8。 最终返回的数组D和数组P可以得到v_0到任意一个顶点的最短路径和路径长度,迪杰斯特拉（Dijkstra）算法解决了从某个源点到其余各顶点的最短路径问题。从循环嵌套得到此算法的时间复杂度为O(n^2) 弗洛伊德（Floyd）算法 D^{-1}是网图的邻接矩阵，P^{-1}初设为P[i][j]=j这样的矩阵，用来存储路径 时间复杂度为O(n^3),代码如下： 123typedef int Pathmatirx[MAXVEX][MAXVEX];typedef int ShortPathTable[MAXVEX][MAXVEX];/* Floyd算法，求网图G中各顶点v到其余顶点w最短 路径P[v][w]及带权长度D[v][w] */ 1234567891011121314151617181920212223242526void ShortestPath_Floyd(MGraph G, Pathmatirx *P, ShortPathTable *D)&#123;int v, w, k;for( v=0; v &lt; G.numVertexes; v++ ) // 初始化D和P&#123;for( w=0; w &lt; G.numVertexes; w++ )&#123;(*D)[v][w] = G.matirx[v][w]; /* D[v][w]值即为对应点间的权值 */(*P)[v][w] = w; /* 初始化P */&#125;&#125;for( k=0; k &lt; G.numVertexes; k++ )&#123;for( v=0; v &lt; G.numVertexes; v++ )&#123;for( w=0; w &lt; G.numVertexes; w++ )&#123;if( (*D)[v][w] &gt; (*D)[v][k] + (*D)[k][w] )&#123;/* 如果经过下标为k顶点路径比原两点间路径更短 将当前两点间权值设为更小的一个 */(*D)[v][w] = (*D)[v][k] + (*D)[k][w];(*P)[v][w] = (*P)[v][k]; /* 路径设置经过下标为k的顶点 */&#125;&#125;&#125;&#125;&#125; 程序开始运行，第4～11行初始化D和P 第12～25行，是算法的主循环，一共三层嵌套，k代表中转顶点的下标。v代表起始顶点，w代表结束顶点 当K=0时，所有的顶点都经过V_0中转，计算是否有最短路径的变化。不过没有任何变化 当K=1时，所有的顶点都经过v_1中转。此时，当v=0时，原本D[0][2]=5，现在由于D[0][1]+D[1][2]=4。因此由代码的第20行，二者取其最小值，得到D[0][2]=4，同理可得D[0][3]=8、D[0][4]=6，当v=2、3、4时，也修改了一些数据，即左图中虚线框数据。由于最小权值的修正，路径矩阵P改为当前的P[v][k]值，见代码第21行 k=2一直到8结束，表示针对每个顶点做中转得到的计算结果，D^0是以D^{-1}为基础，D^1是以D^0为基础，……，D^8是以D^7为基础，路径矩阵P也是如此。当k=8时，两矩阵数据如图 最短路径的显示代码： 12345678910111213141516for(v=0; v&lt;G.numVertexes; ++v)&#123;for(w=v+1; w&lt;G.numVertexes; w++)&#123;printf("v%d-v%d weight: %d ",v,w,D[v][w]);k=P[v][w]; /* 获得第一个路径顶点下标 */printf(" path: %d",v); /* 打印源点 */while(k!=w) /* 如果路径顶点下标不是终点 */&#123;printf(" -&gt; %d",k); /* 打印路径顶点 */k=P[k][w]; /* 获得下一个路径顶点下标 */&#125;printf(" -&gt; %d\n",w); /* 打印终点 */&#125;printf("\n");&#125; 7.7 拓扑排序无环，即图中没有回路 拓扑排序介绍在一个表示工程的有向图中，用顶点表示活动，用弧表示活动之间的优先关系，这样的有向图为顶点表示活动的网，称为AOV网（ActivityOn Vertex Network）。AOV网中的弧表示活动之间存在的某种制约关系。AOV网中不能存在回路 设G=(V,E)是一个具有n个顶点的有向图，V中的顶点序列v_1,v_2,\cdots,v_n，满足若从顶点v_i到v_j有一条路径，则在顶点序列中顶点v_i必在顶点v_j之前。称这样的顶点序列为一个拓扑序列 拓扑排序是对一个有向图构造拓扑序列的过程。 构造时会有两个结果： 如果此网的全部顶点都被输出，说明它是不存在环（回路）的AOV网 如果输出顶点数少了，哪怕是少了一个，也说明这个网存在环（回路），不是AOV网 拓扑排序算法对AOV网进行拓扑排序的基本思路是：从AOV网中选择一个入度为0的顶点输出，然后删去此顶点，并删除以此顶点为尾的弧，继续重复此步骤，直到输出全部顶点或者AOV网中不存在入度为0的顶点为止 为AOV网建立一个邻接表。考虑到算法过程中始终要查找入度为0的顶点，因此在原来顶点表结点结构中，增加一个入度域in，其中in就是入度的数字 in data firstedge 在拓扑排序算法中，涉及的结构代码如下： 1234567891011121314151617181920typedef struct EdgeNode /* 边表结点 */&#123;int adjvex; /* 邻接点域，存储该顶点对应的下标 */int weight; /* 用于存储权值，对于非网图可以不需要 */struct EdgeNode *next; /* 链域，指向下一个邻接点 */&#125; EdgeNode;typedef struct VertexNode /* 顶点表结点 */&#123;int in; /* 顶点入度*/int data; /* 顶点域，存储顶点信息 */EdgeNode *firstedge; /* 边表头指针 */&#125; VertexNode, AdjList[MAXVEX];typedef struct&#123;AdjList adjList;int numVertexes,numEdges; /* 图中当前顶点数和边数 */&#125; graphAdjList, *GraphAdjList; 在算法中，还需要辅助的数据结构—栈，用来存储处理过程中入度为0的顶点，目的是为了避免每个查找时都要去遍历顶点表找有没有入度为0的顶点 12345678910111213141516171819202122232425262728Status TopoLogicalSort(GraphAdjList GL) /*拓扑排序，若GL无回路，则输出拓扑排序序列并返回OK，若有回路返回ERROR*/&#123;EdgeNode *e;int i,k,gettop;int top = 0; /*用于栈指针下表*/int count = 0; /*用于统计输出顶点的个数*/int *stack; /*建栈用于存储入度为0的顶点*/stack = (int*)malloc(GL-&gt;numVertexes *sizeof(int));for(i = 0; i&lt;GL-&gt;numVertexes; i++)if(GL-&gt;adjList[i].in == 0) /*将入度为0的顶点入栈*/stack[++top] = i;while(top != 0)&#123;gettop = stack[top--]; /*出栈*/printf("%d -&gt; ",GL-&gt;adjList[gettop].data); /*打印此顶点*/count++; /*统计输出顶点数*/for(e=GL-&gt;adjList[gettop].firstedge; e！; e=e-&gt;next)&#123; /*对此顶点弧表遍历*/k = e-&gt;adjvex;if(--GL-&gt;adjList[k].in == 0) //将k号顶点邻接点的入度减1,且减1后，入度为0的顶点需要存到栈中stack[++top] = k;&#125;&#125;if(count &lt; GL-&gt;numVertexes) /*如果count小于顶点数，说明存在环*/return -1;elsereturn OK;&#125; 程序开始运行，第3～7行都是变量的定义，其中stack是一个栈，用来存储整型的数字。 第9～11行，作了一个循环判断，把入度为0的顶点下标都入栈，此时stack应该为：{0,1,3}，即v_0,v_1,v_3的顶点入度为0 第12～23行，while循环，当栈中有数据元素时，始终循环 第14～16行，v_3出栈得到gettop=3。并打印此顶点，然后count加1 第17～22行，循环是对v3顶点对应的弧链表进行遍历，即下图中的灰色部分，找到v_3连接的两个顶点v_2和v_{13}，并将它们的入度减少一位，此时v_2和v_{13}的in值都为1。目的是为了将v_3顶点上的弧删除 再次循环，第12～23行。此时处理的是顶点v_1。经过出栈、打印、count=2后，对v_1到v_2,v_4,v_8的弧进行了遍历。并同样减少了它们的入度数，此时v_2入度为0，于是由第20～21行知，v_2入栈，如图 接下来是同样的处理方式。v_2 ,v_6 ,v_0, v_4,v_5 ,v_8的打印删除过程如下，后面还剩几个顶点都类似 最终拓扑排序打印结果为3-&gt;1-&gt;2-&gt;6-&gt;0-&gt;4-&gt;5-&gt;8-&gt;7-&gt;12-&gt;9-&gt;10-&gt;13-&gt;11，这结果并不是唯一的一种拓扑排序方案 对一个具有n个顶点e条弧的AOV网来说，第9～11行扫描顶点表，将入度为0的顶点入栈的时间复杂为O(n)，之后的while循环中，每个顶点进一次栈，出一次栈，入度减1的操作共执行了e次，所以整个算法的时间复杂度为O(n+e) 7.8 关键路径在一个表示工程的带权有向图中，用顶点表示事件，用有向边表示活动，用边上的权值表示活动的持续时间，这种有向图的边表示活动的网，称之为AOE网（Activity On Edge Net-work） AOE网中没有入边的顶点称为始点或源点，没有出边的顶点称为终点或汇点 $v_0$是源点，表示一个工程的开始，v_9是汇点，表示整个工程的结束，顶点v_0,v_1,\cdots,v_9分别表示事件，弧, , \cdots,都表示一个活动，用a_0,a_1,\cdots,a_{12}表示，它们的值代表着活动持续的时间，弧就是从源点开始的第一个活动a_0，它的时间是3个单位 AOV网是顶点表示活动的网，它只描述活动之间的制约关系，而AOE网是用边表示活动的网，边上的权值表示活动持续的时间，AOE网是要建立在活动之间制约关系没有矛盾的基础之上，再来分析完成整个工程至少需要多少时间，或者为缩短完成工程所需时间，应当加快哪些活动等问题 路径上各个活动所持续的时间之和称为路径长度，从源点到汇点具有最大长度的路径叫关键路径，在关键路径上的活动叫关键活动 开始→发动机完成→部件集中到位→组装完成就是关键路径，路径长度为5.5 关键路径算法原理所有活动的最早开始时间和最晚开始时间如果相等就意味着此活动是关键活动，活动间的路径为关键路径 定义如下几个参数： 事件的最早发生时间etv（earliest time ofvertex）：即顶点v_k的最早发生时间 事件的最晚发生时间ltv（latest time ofvertex）：即顶点v_k的最晚发生时间，也就是每个顶点对应的事件最晚需要开始的时间，超出此时间将会延误整个工期 活动的最早开工时间ete（earliest time ofedge）：即弧a_k的最早发生时间 活动的最晚开工时间lte（latest time ofedge）：即弧a_k的最晚发生时间，也就是不推迟工期的最晚开工时间 由1和2可以求得3和4，然后再根据ete[k]是否与lte[k]相等来判断a_k是否是关键活动 关键路径算法将AOE网转化为邻接表结构，与拓扑排序时邻接表结构不同的是弧链表增加了weight域，用来存储弧的权值 求事件的最早发生时间etv的过程，就是从头至尾找拓扑序列的过程，在求关键路径之前，需要先调用一次拓扑序列算法的代码来计算etv和拓扑序列列表。为此，首先在程序开始处声明几个全局变量： 123int *etv, *ltv; /* 事件最早发生时间和最迟发生时间数组 */int *stack2; /* 用于存储拓扑序列的栈 */int top2; /* 用于stack2的指针 */ stack2用来存储拓扑序列，以便后面求关键路径时使用 改进过的求拓扑序列算法： 1234567891011121314151617181920212223242526272829303132333435status TopoLogicalSort(GraphAdjList GL) /* 拓扑排序，用于关键路径计算 */&#123;EdgeNode *e;int i,k,gettop;int top = 0; /*用于栈指针下表*/int count = 0; /*用于统计输出顶点的个数*/int *stack; /*建栈用于存储入度为0的顶点*/stack = (int*)malloc(GL-&gt;numVertexes *sizeof(int)); //动态分配内存，大小为n个顶点for(i = 0; i&lt;GL-&gt;numVertexes; i++)if(GL-&gt;adjList[i].in == 0) /*将入度为0的顶点入栈*/stack[++top] = i;top2 = 0; //初始化为0etv = (int*) malloc(GL-&gt;numVertexes*sizeof(int)); /*事件的最早发生时间*/for(i=0;i&lt;GL-&gt;numVertexes; i++)etv[i] = 0; //初始化为0stack2 = (int*) malloc(GL-&gt;numVertexes*sizeof(int)); /*初始化*/while(top != 0)&#123;gettop = stack[top--]; /*出栈*/count++; /*统计输出顶点数*/stack2[++top2] = gettop; /*将弹出的顶点序列号压入拓扑序列的栈中*/for(e=GL-&gt;adjList[gettop].firstedge; e; e=e-&gt;next)&#123; /*对此顶点弧表遍历*/k = e-&gt;adjvex;if(!(--GL-&gt;adjList[k].in)) //将k号顶点邻接点的入度减1,且减1后，入度为0的顶点需要存到栈中stack[++top] = k;if((etv[gettop]+e-&gt;weight)&gt;etv[k]) /*求各顶点时间最早发生时间*/etv[k] = etv[gettop] + e-&gt;weight; /*某个顶点的最早发生时间= 和它相关的活动必须全部完成*/&#125;&#125;if(count &lt; GL-&gt;numVertexes) /*如果count小于顶点数，说明存在环*/return -1;elsereturn 0;&#125; 第27～28行是求etv数组的每一个元素的值。比如说已经求得顶点v_0对应的etv[0]=0，顶点v_1对应的etv[1]=3，顶点v_2对应的etv[2]=4，求顶点v_3对应的etv[3]就是求etv[1]+len&lt;v_1,v_3&gt;与etv[2]+len&lt;v_2,v_3&gt;的较大值。显然3+5&lt;4+8，得到etv[3]=12，代码中e-&gt;weight就是当前弧的长度 计算顶点v_k即求etv[k]的最早发生时间的公式是： P[K]表示所有到达顶点v_k的弧的集合。比如P[3]就是&lt;v_1,v_3&gt;和&lt;v_2,v_3&gt;两条弧。len&lt;v_i,v_k&gt;是弧&lt;v_i,v_k&gt;上的权值 求关键路径的算法代码： 12345678910111213141516171819202122232425262728293031void CriticalPath(GraphAdjList GL) /*求关键路径，GL为有向网，输出GL的各项关键活动*/&#123;EdgeNode *e;int i,gettop,k,j;int ete,lte; /*声明活动最早发生时间和最迟发生时间*/TopoLogicalSort(GL); /*求拓扑序列，计算数组etv和stack2的值*/ltv = (int*) malloc(GL-&gt;numVertexes*sizeof(int)); /*时间的最晚发生时间*/for(i= 0; i&lt;GL-&gt;numVertexes;i++)ltv[i]=etv[GL-&gt;numVertexes-1]; /*初始化ltv[i] 为工程完成的最早时间，etv[i]初始化为0*/while(top2!=0) /*计算ltv*/&#123;gettop = stack2[top2--];for(e=GL-&gt;adjList[gettop].firstedge;e!=NUll;e=e-&gt;next)&#123;/*求各定点事件的最迟发生时间ltv值*/k=e-&gt;adjvex;if(ltv[k]-e-&gt;weight&lt;ltv[gettop])ltv[gettop]= ltv[k]-e-&gt;weight; /*求最晚发生时间，是从拓扑序列的最后一个顶点逆着推导*/&#125;&#125;for(j=0;j&lt;GL-&gt;numVertexes;j++) /*求ete，lte关键活动*/&#123;for(e=GL-&gt;adjList[j].firstedge;e!=NULL;e=e-&gt;next)&#123;k=e-&gt;adjvex;ete = etv[j]; /*活动最早开始时间*/lte = ltv[k] - e-&gt;weight;/*活动最晚发生时间*/if(ete ==lte)printf("&lt;v%d,v%d&gt; length: %d, ",GL-&gt;adjList[j].data,GL-&gt;adjList[k].data,e-&gt;weight);&#125;&#125;&#125; 程序开始执行。第5行，声明了ete和lte两个活动最早最晚发生时间变量 第6行，调用求拓扑序列的函数。执行完毕后，全局变量数组etv和栈stack的值如下图，top2=10。即对于每个事件的最早发生时间，已经计算出来 第7～9行为初始化全局变量ltv数组，因为etv[9]=27，所以数组ltv当前的值为：{27,27,27,27,27,27,27,27,27,27} 第10～19行为计算ltv的循环。第12行，先将stack2的栈头出栈，由后进先出得到gettop=9。根据邻接表中，v_9没有弧表，所以第13～18行循环体未执行 再次来到第12行，gettop=8，在第13～18行的循环中，v_8的弧表只有一条&lt;v_8,v_9&gt;，第15行得到k=9，因为ltv[9]-3&lt;ltv[8]，所以ltv[8]=ltv[9]-3=24 再次循环，当gettop=7、5、6时，同理可算出ltv相对应的值为19、13、25，此时ltv值为：{27，27，27，27，27，13，25，19，24，27} 当gettop=4时，由邻接表可得到v_4有两条弧&lt;v_4,v_6&gt;、&lt;v_4,v_7&gt;，通过第13～18行的循环，可以得到ltv[4]=min(ltv[7]-4,ltv[6]-9)=min(19-4,25-9)=1 可以得出计算顶点v_k即求ltv[k]的最晚发生时间的公式如下，S[K]表示所有从顶点v_k出发的弧的集合。比如S[4]就是&lt;v_4,v_6&gt;和&lt;v_4,v_7&gt;两条弧，len&lt;v_k,v_j&gt;是弧&lt;v_k,v_j&gt;上的权值。 当程序执行到第20行时，相关变量的值如图，比如etv[1]=3而ltv[1]=7，表示的意思就是如果时间单位是天的话，哪怕v_1这个事件在第7天才开始，也可以保证整个工程的按期完成，可以提前v_1事件开始时间，但最早也只能在第3天开始 第20～30行是来求另两个变量活动最早开始时间ete和活动最晚开始时间lte，并对相同下标的它们做比较。两重循环嵌套是对邻接表的顶点和每个顶点的弧表遍历 当j=0时，从v_0点开始，有&lt;v_0,v_2&gt;和&lt;v_0,v_1&gt;两条弧。当k=2时，ete=etv[j]=etv[0]=0。lte=ltv[k]-e-&gt;weight=ltv[2]-len&lt;v_0,v_2&gt;=4-4=0，此时ete=lte，表示弧&lt;v_0,v_2&gt;是关键活动，因此打印。当k=1时，ete=etv[j]=etv[0]=0。lte=ltv[k]-e-&gt;weight=ltv[1]-len&lt;v_0,v_1&gt;=7-3=4，此时ete≠lte，因此&lt;v_0,v_1&gt;并不是关键活动 ete本来是表示活动&lt;v_k,v_j&gt;的最早开工时间，是针对弧来说的。但只有此弧的弧尾顶点v_k的事件发生了，它才可以开始，因此ete=etv[k]。而lte表示的是活动&lt;v_k,v_j&gt;的最晚开工时间，但此活动再晚也不能等v_j事件发生才开始，而必须要在v_j事件之前发生，所以lte=ltv[j]-len&lt;v_k,v_j&gt;。判断ete与lte是否相等，相等意味着活动没有任何空闲，是关键活动，否则就不是 j=1一直到j=9为止，做法是完全相同的，关键路径打印结果为“&lt;v_0,v_2&gt;4,&lt;v_2,v_3&gt;8,&lt;v_3,v_4&gt;3,&lt;v_4,v_7&gt;4,&lt;v_7,v_8&gt;5,&lt;v_8,v_9&gt;3” 分析整个求关键路径的算法，第6行是拓扑排序，时间复杂度为O(n+e)，第8～9行时间复杂度为O(n)，第10～19行时间复杂度为O(n+e)，第20～31行时间复杂也为O(n+e)，根据对时间复杂度的定义，所有的常数系数可以忽略，所以最终求关键路径算法的时间复杂度依然是O(n+e)]]></content>
      <categories>
        <category>数据结构</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大话数据结构 第六章 树]]></title>
    <url>%2F2019%2F03%2F08%2F%E5%A4%A7%E8%AF%9D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AC%AC%E5%85%AD%E7%AB%A0%20%20%E6%A0%91%2F</url>
    <content type="text"><![CDATA[6.1 树的定义树（Tree）是n（n≥0）个结点的有限集。n=0时称为空树。在任意一棵非空树中：（1）有且仅有一个特定的称为根（Root）的结点；（2）当n＞1时，其余结点可分为m（m＞0）个互不相交的有限集T_1、T_2、……、T_m，其中每一个集合本身又是一棵树，并且称为根的子树（SubTree） 子树T_1和子树T_2就是根结点A的子树。D、G、H、I组成的树又是B为根结点的子树，E、J组成的树是以C为根结点的子树 树的定义还需要强调两点： n&gt;0时根结点是唯一的，不可能存在多个根结点， m&gt;0时，子树的个数没有限制，但它们一定是互不相交的。下图中的两个结构就不符合树的定义，因为它们都有相交的子树 结点分类树的结点包含一个数据元素及若干指向其子树的分支。结点拥有的子树数称为结点的度（De-gree）。度为0的结点称为叶结点（Leaf）或终端结点；度不为0的结点称为非终端结点或分支结点。除根结点之外，分支结点也称为内部结点。树的度是树内各结点的度的最大值。 结点的度的最大值是结点D的度，为3，所以树的度也为3 结点间关系结点的子树的根称为该结点的孩子（Child），相应地，该结点称为孩子的双亲（Parent），同一个双亲的孩子之间互称兄弟（Sibling）。结点的祖先是从根到该结点所经分支上的所有结点。以某结点为根的子树中的任一结点都称为该结点的子孙 树的其他相关概念结点的层次（Level）从根开始定义起，根为第一层，根的孩子为第二层。若某结点在第l层，则其子树就在第l+1层。其双亲在同一层的结点互为堂兄弟。树中结点的最大层次称为树的深度（Depth）或高度 如果将树中结点的各子树看成从左至右是有次序的，不能互换的，则称该树为有序树，否则称为无序树。森林（Forest）是m（m≥0）棵互不相交的树的集合。对树中每个结点而言，其子树的集合即为森林 线性表与树的结构： 6.2 树的抽象数据类型1234567891011121314151617181920ADT 树 (tree)Data:树是由一个根结点和若干棵子数构成.树中结点具有相同数据类型及层次关系Operation:InitTree (*T):构成空树TDestroyTree(*T):销毁树TCreateTree(*T,definition):按definition中给出的树的定义来构造树ClearTree(*T):若树T存在,则将树T请为空树TreeEmpty(T):若T为空树,返回true,否则返回falseTreeDepth(T):返回T的深度Root(T):返回T的根结点Value(T,cur_e):cur_e是树T中一个结点,返回此结点的值Assign(T,cur_e,value):给树T的结点cur_e赋值为valueParent(T,cur_e):若cur_e是树T的非根结点,返回它的双亲,否则返回空LeftChild(T,cur_e):若cur_e是树T的非叶节点,则返回它的最左孩子,否则返回空RightSibling(T,cur_e):若cur_e有右兄弟,则返回他的右兄弟,否则返回空InsertChild(*T,*p,i,c):其中p指向树T的某个结点,T为所指结点p的度加上1,非空树c与T不相交,操作结果为插入c为树T中p所指结点的第i棵子树DeleteChile(*T,*p,i):其中p指向树T的某个结点,i为所指结点p的度,操作结果为删除T中p所指结点的第i棵子树endADT 6.3 树的存储结构双亲表示法假设以一组连续空间存储树的结点，同时在每个结点中，附设一个指示器指示其双亲结点在数组中的位置。每个结点除了知道自己是谁以外，还知道它的双亲在哪里 data parent 数据域，存储结点的数据信息 指针域，存储该结点的双亲在数组中的下标 结点结构定义代码 12345678910111213//树的双亲表示法结点结构定义#define MAX_TREE_SIZE 100typedef int TElemType; //树结点的数据类型,目前暂定义为整型typedef struct PTNode //结点结构&#123;TElemType data; //结点数据int parent; //双亲位置&#125;PTNode;typedef struct //树结构&#123;PTNode nodes[MAX_TREE_SIZE]; //结点数组int r,n; //根的位置和结点数&#125;PTree; 由于根结点是没有双亲的,约定根结点的位置域设置为-1,这就意味着所有的结点都存在他的双亲结点 根据结点的parent指针很容易找到它的双亲结点,所用时间复杂度为O(1),知道parent为-1,表示找到了树结点的根.如果要知道结点的孩子是什么,就必须遍历整个结构才行 改进： 增加一个结点最左边孩子的域,叫长子域，如果没有孩子的结点,这个长子域就设置为-1 每一个结点如果它存在右兄弟,则记录下右兄弟的下标,如果右兄弟不存在,则赋值为-1 孩子表示法由于树中每个结点可能有多棵子树,可以考虑多重链表,即每个指针指向一棵子数的根结点,这种方法叫多重链表表示法 不过,树的每个结点的度,也就是它的孩子个数是不同的 方案一:指针域的个数等于树的度 这种方法对于树中各结点的度相差很大时,显然很浪费空间,因为有很多的结点指针域都是空的.如果树的各结点度相差很小时,开辟空间被充分利用了,这时缺点变优点 方案二:按需分配空间 每个结点指针域的个数等于该结点的度,取一个位置来储存结点指针域的个数 克服了浪费空间的缺点,对空间利用率提高,但由于各个结点的链表是不相同的结构,加上要维护结点的度的数值,在运算上会带来时间上的损耗 孩子表示法:把每个结点的孩子结点排列起来,以单链表作存储结构,则n个结点有n个孩子链表,如果是叶子结点则此链表为空.然后n个头指针又组成一个线性表,采用顺序存储结构,存放进一个一维数组中 为此设置了两种结点结构:一个是孩子链表的孩子结点 另一个是表头数组的表头结点 123456789101112131415161718/*数的孩子表示法结构定义*/#define MAX_TREE_SIZE 100typedef int TElemType;typedef struct CTNode //孩子结点&#123;int child;struct CTNode *next;&#125; *ChildPtr;typedef struct //表头结构&#123;TElemType data;ChildPtr firstchild;&#125;CTBox;typedef struct //树结构&#123;CTBox nodes[MAX_TREE_SIZE]; //结点数组int r, n; //根的位置和结点数&#125;CTree; 这样的结构对于要查找某个结点的某个孩子,或者找某结点的兄弟,只需要查找这个结点的孩子单链表即可.但要知道某结点的双亲是谁,比较麻烦,需要遍历整棵树才行 双亲孩子表示法 孩子兄弟表示法任意一棵树,它的结点的第一个孩子如果存在就是唯一的,它的右兄弟如果存在也是唯一的,因此,设置两个指针,分别指向该结点的第一个孩子和此结点的兄弟 结构定义代码如下 123456/* 树的孩子兄弟表示法结构定义 */typedef struct CSNode&#123;TElemType data;struct CSNode *firstchild, *rightsib;&#125; CSNode, *CSTree; 这种表示法，查找某个结点的某个孩子只需要通过fistchild找到此结点的长子，再通过长子结点的rightsib找到它的二弟，接着一直下去，直到找到具体的孩子 这个表示法的最大好处是把一棵复杂的树变成了一棵二叉树 6.4 二叉树的定义二叉树是n(n&gt;=0)个结点的有限集合,该集合或者为空集(称为空二叉树),或者由一个根结点和两棵互不相交的,分别称为根结点的左子树和右子树的二叉树组成 二叉树特点 每个结点最多有_两颗_子树,所以二叉树不存在度大于2的结点 左子树和右子树是有顺序的,次序不能任意颠倒 即使树中某结点只有一颗子树,也要区分左子树和右子树的 二叉树的五种基本形态： 空二叉树 只有一个根结点 根结点只有左子树 根结点只有右子树 树既有左子树也有右子树 只从形态上考虑，三个结点的树只有两种情况，图中有两层的树1和有三层的后四种的任意一种，对于二叉树来说，由于要区分左右，就演变成五种形态，树2、树3、树4和树5分别代表不同的二叉树 特殊二叉树1.斜树斜树一定是斜的,所有的结点都只有左子树的二叉树叫左斜树.所有的二叉树只有右子数的树叉树叫右斜树.两者统称为斜树。斜树每一层都只有一个结点,结点的个数跟二叉树的深度相同. 线性表就是特殊的二叉树 2.满二叉树 在一棵二叉树中,如果所有分支结点都存在左子树和右子树,并且所有叶子都在同一层上,这样的二叉树称为满二叉树 满二叉树的特点: 叶子只能出现在最下一层,出现在其他层就不可能达到平衡 非叶子结点的度一定是2 在同样深度的二叉树中,满二叉树的结点个数最多,叶子树最多 3.完全二叉树 对一棵具有n个结点的二叉树按层序编号,如果编号为i(1&lt;= i&lt;=n)的结点与同样深度的满二叉树中编号为i的结点在二叉树中位置完全相同,则这棵二叉树树完全二叉树 尽管不是满二叉树，但是编号是连续的，所以它是完全二叉树 满二叉树一定是完全二叉树,而完全二叉树不一定是满二叉树 完全二叉树的所有结点与同样深度的满二叉树，它们按层序编号相同的结点，是一一对应的。关键词是按层序编号，树1因为5结点没有左子树，却有右子树，使得按层序编号的第10个编号空档。树2由于3结点没有子树，使得6、7编号的位置空档。树3因为5编号下没有子树造成第10和第11位置空档。 完全二叉树的特点: 叶子结点只能出现在最下两层 最下层的叶子一定集中在左部连续位置 倒数两层,若有叶子结点,一定都在右部连续位置 若结点度为1,则该结点只有左孩子,即不存在只有右子树的情况 同样结点树的二叉树,完全二叉树的深度最小 6.5 二叉树的性质性质1在二叉树的第i层上至多有2^{i-1}个结点(i&gt;=1) 性质2深度为k的二叉树至多有2^{k-1}个结点(k&gt;=1) 性质3对任何一棵二叉树T,如果其终端结点数(叶子结点数)为n_0,度为2的结点数为n_2,则n_0 = n_2+1 一棵二叉树中,除了叶子结点外,剩下的就是度为1或2的结点树了,设n_1为度为1的结点数.则树的总结点数n = n_0+n_1+n_2 性质4具有n个结点的完全二叉树的深度为[log_2N]+1（|x|表示不大于x的最大整数） 深度为k的满二叉树的结点数n一定是2^k-1。这是最多的结点个数。对于n=2^k-1倒推得到满二叉树的深度为k=\log_2(n+1)，比如结点数为15的满二叉树，深度为4。完全二叉树的叶子结点只会出现在最下面的两层。结点数一定少于等于同样深度的满二叉树的结点数2^k-1，但一定多于2^{k-1}-1。即满足2^{k-1}-1]]></content>
      <categories>
        <category>数据结构</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[莫烦Python爬虫笔记]]></title>
    <url>%2F2019%2F03%2F07%2F%E8%8E%AB%E7%83%A6Python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[爬虫简介1.1 了解网页结构网页基本组成部分在 HTML 中, 基本上所有的实体内容, 都会有个 tag 来框住它. 而这个被tag 住的内容, 就可以被展示成不同的形式, 或有不同的功能 主体的 tag 分成两部分,header和body 在header中, 存放这一些网页的网页的元信息, 比如说title 这些信息是不会被显示到你看到的网页中的. 这些信息大多数时候是给浏览器看, 或者是给搜索引擎的爬虫看 12345&lt;head&gt;&lt;meta charset="UTF-8"&gt;&lt;title&gt;Scraping tutorial 1 | 莫烦Python&lt;/title&gt;&lt;link rel="icon" href="https://morvanzhou.github.io/static/img/description/tab_icon.png"&gt;&lt;/head&gt; HTML 的第二大块是body, 这个部分才是你看到的网页信息. 网页中的heading, 视频, 图片和文字等都存放在这里. ﹤h1﹥﹤/h1﹥tag 就是主标题, 呈现出来的效果就是大一号的文字. ﹤p﹥﹤/p﹥ 里面的文字就是一个段落. ﹤a﹥﹤/a﹥里面都是一些链接. 1234567&lt;body&gt;&lt;h1&gt;爬虫测试1&lt;/h1&gt;&lt;p&gt;这是一个在 &lt;a href="https://morvanzhou.github.io/"&gt;莫烦Python&lt;/a&gt;&lt;a href="https://morvanzhou.github.io/tutorials/scraping"&gt;爬虫教程&lt;/a&gt; 中的简单测试.&lt;/p&gt;&lt;/body&gt; 用 Python 登录网页因为网页中存在中文, 为了正常显示中文, read() 完以后, 要对读出来的文字进行转换, decode() 成可以正常显示中文的形式 123456789101112131415161718192021222324&gt;&gt;&gt;from urllib.request import urlopen&gt;&gt;&gt;# if has Chinese, apply decode()&gt;&gt;&gt;html = urlopen("https://morvanzhou.github.io/static/scraping/basic-structure.html").read().decode('utf-8')&gt;&gt;&gt;print(html)&gt;&gt;&gt;&lt;!DOCTYPE html&gt;&lt;html lang="cn"&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;&lt;title&gt;Scraping tutorial 1 | 莫烦Python&lt;/title&gt;&lt;link rel="icon" href="https://morvanzhou.github.io/static/img/description/tab_icon.png"&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;爬虫测试1&lt;/h1&gt;&lt;p&gt;这是一个在 &lt;a href="https://morvanzhou.github.io/"&gt;莫烦Python&lt;/a&gt;&lt;a href="https://morvanzhou.github.io/tutorials/scraping"&gt;爬虫教程&lt;/a&gt; 中的简单测试.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 匹配网页内容选好要使用的 tag 名称 &lt; title&gt;,找到这个网页的 title 12345import reres = re.findall(r"&lt;title&gt;(.+?)&lt;/title&gt;", html)print("\nPage title is: ", res[0])# Page title is: Scraping tutorial 1 | 莫烦Python 想要找到中间的段落 &lt; p&gt;, 因为这个段落在 HTML 中还夹杂着 tab,new line, 所以给一个 flags=re.DOTALL 来对这些 tab, new line不敏感 123456res = re.findall(r"&lt;p&gt;(.*?)&lt;/p&gt;", html, flags=re.DOTALL) # re.DOTALL if multi lineprint("\nPage paragraph is: ", res[0])# Page paragraph is:# 这是一个在 &lt;a href="https://morvanzhou.github.io/"&gt;莫烦Python&lt;/a&gt;# &lt;a href="https://morvanzhou.github.io/tutorials/scraping"&gt;爬虫教程&lt;/a&gt; 中的简单测试. 找所有的链接 123456res = re.findall(r'href="(.*?)"', html)print("\nAll links: ", res)# All links:['https://morvanzhou.github.io/static/img/description/tab_icon.png','https://morvanzhou.github.io/','https://morvanzhou.github.io/tutorials/scraping'] BeautifulSoup 解析网页2.1 BeautifulSoup 解析网页: 基础要输出&lt; h1&gt;标题, 可以就直接soup.h1: 123456789101112131415soup = BeautifulSoup(html, features='lxml')print(soup.h1)"""&lt;h1&gt;爬虫测试1&lt;/h1&gt;"""print('\n', soup.p)"""&lt;p&gt;这是一个在 &lt;a href="https://morvanzhou.github.io/"&gt;莫烦Python&lt;/a&gt;&lt;a href="https://morvanzhou.github.io/tutorials/scraping"&gt;爬虫教程&lt;/a&gt; 中的简单测试.&lt;/p&gt;""" 如果网页中有多个同样的 tag, 比如链接&lt; a&gt;, 可以使用find_all来找到所有的选项. 因为真正的 link 不是在&lt; a&gt;中间&lt; /a&gt;,而是在&lt; a href = ‘link’&gt;里面, 也可以看做是&lt; a&gt;的一个属性.用像 Python 字典的形式, 用 key 来读取l[‘href’] 123456789"""&lt;a href="https://morvanzhou.github.io/tutorials/scraping"&gt;爬虫教程&lt;/a&gt;"""all_href = soup.find_all('a')all_href = [l['href'] for l in all_href]print('\n', all_href)# ['https://morvanzhou.github.io/', 'https://morvanzhou.github.io/tutorials/scraping'] 2.2 BeautifulSoup 解析网页: CSSCSS 的 ClassCSS 在装饰每一个网页部件的时候, 都会给它一个名字. 而且一个类型的部件, 名字都可以一样 里面的字体/背景颜色, 字体大小, 都是由 CSS 来掌控的, CSS 的代码, 可能就会放在这个网页的 &lt; head&gt; 中 123456from bs4 import BeautifulSoupfrom urllib.request import urlopen# if has Chinese, apply decode()html = urlopen("https://morvanzhou.github.io/static/scraping/list.html").read().decode('utf-8')print(html) 12345678910111213141516171819202122232425&lt;head&gt;...&lt;style&gt;.jan &#123;background-color: yellow;&#125;....month &#123;color: red;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;...&lt;ul&gt;&lt;li class="month"&gt;一月&lt;/li&gt;&lt;ul class="jan"&gt;&lt;li&gt;一月一号&lt;/li&gt;&lt;li&gt;一月二号&lt;/li&gt;&lt;li&gt;一月三号&lt;/li&gt;&lt;/ul&gt;...&lt;/ul&gt;&lt;/body&gt; 在 &lt; head&gt; 中,被放在 &lt; style&gt; 里面的都是某些 class 的 CSS 代码.，比如 jan 就是一个 class，jan 这个类掌控了这个类型的背景颜色，所以在 &lt; ul class=”jan”&gt; 这里, 这个 ul 的背景颜色就是黄色的，而如果是 month 这个类, 它们的字体颜色就是红色 按 Class 匹配找所有 class=month 的信息. 并打印出它们的 tag 内文字. 1234567891011121314soup = BeautifulSoup(html, features='lxml')# use class to narrow searchmonth = soup.find_all('li', &#123;"class": "month"&#125;)for m in month:print(m.get_text())"""一月二月三月四月五月""" 找到 class=jan 的信息. 然后在 &lt; ul&gt; 下面继续找 &lt; ul&gt; 内部的&lt; li&gt; 信息 12345678910jan = soup.find('ul', &#123;"class": 'jan'&#125;)d_jan = jan.find_all('li') # use jan as a parentfor d in d_jan:print(d.get_text())"""一月一号一月二号一月三号""" 2.3 BeautifulSoup 解析网页: 正则表达正则匹配123456from bs4 import BeautifulSoupfrom urllib.request import urlopenimport re# if has Chinese, apply decode()html = urlopen("https://morvanzhou.github.io/static/scraping/table.html").read().decode('utf-8') 如果是图片, 它们都藏在这样一个 tag 中: 123&lt;td&gt;&lt;img src="https://morvanzhou.github.io/static/img/course_cover/tf.jpg"&gt;&lt;/td&gt; 把正则的 compile 形式放到BeautifulSoup 的功能中 1234567891011soup = BeautifulSoup(html, features='lxml')img_links = soup.find_all("img", &#123;"src": re.compile('.*?\.jpg')&#125;)for link in img_links:print(link['src'])"""https://morvanzhou.github.io/static/img/course_cover/tf.jpghttps://morvanzhou.github.io/static/img/course_cover/rl.jpghttps://morvanzhou.github.io/static/img/course_cover/scraping.jpg""" 1234567891011course_links = soup.find_all('a', &#123;'href': re.compile('.*?')&#125;)for link in course_links:print(link['href'])"""https://morvanzhou.github.io/https://morvanzhou.github.io/tutorials/scrapinghttps://morvanzhou.github.io/tutorials/machine-learning/tensorflow/https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/https://morvanzhou.github.io/tutorials/data-manipulation/scraping/""" 2.4 小练习: 爬百度百科制作爬虫123456789101112131415161718192021222324252627from bs4 import BeautifulSoupfrom urllib.request import urlopenimport reimport randombase_url = "https://baike.baidu.com"his = ["/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711"]url = base_url + his[-1]html = urlopen(url).read().decode('utf-8')soup = BeautifulSoup(html, features='lxml')print(soup.find('h1').get_text(), ' url: ', his[-1])# 网络爬虫 url: /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711# find valid urlssub_urls = soup.find_all("a", &#123;"target": "_blank", "href": re.compile("/item/(%.&#123;2&#125;)+$")&#125;)if len(sub_urls) != 0:his.append(random.sample(sub_urls, 1)[0]['href'])else:# no valid sub link foundhis.pop()print(his)# ['/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711', '/item/%E4%B8%8B%E8%BD%BD%E8%80%85'] 1234567891011121314151617his = ["/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711"]for i in range(20):url = base_url + his[-1]html = urlopen(url).read().decode('utf-8')soup = BeautifulSoup(html, features='lxml')print(i, soup.find('h1').get_text(), ' url: ', his[-1])# find valid urlssub_urls = soup.find_all("a", &#123;"target": "_blank", "href": re.compile("/item/(%.&#123;2&#125;)+$")&#125;)if len(sub_urls) != 0:his.append(random.sample(sub_urls, 1)[0]['href'])else:# no valid sub link foundhis.pop() 12345670 网络爬虫 url: /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/51627111 路由器 url: /item/%E8%B7%AF%E7%94%B1%E5%99%A82 服务等级 url: /item/%E6%9C%8D%E5%8A%A1%E7%AD%89%E7%BA%A7...17 呼损率 url: /item/%E5%91%BC%E6%8D%9F%E7%8E%8718 服务等级 url: /item/%E6%9C%8D%E5%8A%A1%E7%AD%89%E7%BA%A719 呼损率 url: /item/%E5%91%BC%E6%8D%9F%E7%8E%87 更多请求/下载方式3.1 多功能的 Requests获取网页的方式 post 1.账号登录 2.搜索内容 3.上传图片 4.上传文件 5.往服务器传数据 等 get 1.正常打开网页 2.不往服务器传数据 requests get 请求将parameters用字典代替, 然后传入requests.get() 功能，用webbrowser模块打开一个默认浏览器, 观看在百度的搜索页面. 12345678import requestsimport webbrowserparam = &#123;"wd": "莫烦Python"&#125; # 搜索的信息r = requests.get('http://www.baidu.com/s', params=param)print(r.url)webbrowser.open(r.url)# http://www.baidu.com/s?wd=%E8%8E%AB%E7%83%A6Python requests post 请求填入姓名的地方是在一个 &lt; form&gt; 里面 &lt; form&gt; 里面有一些 &lt; input&gt; 的tag, &lt; input&gt; 里面的 name=”firstname”和 name=”lastname”就是要post提交上去的关键信息 12345data = &#123;'firstname': '暴走', 'lastname': '林'&#125;r = requests.post('http://pythonscraping.com/files/processing.php', data=data)print(r.text)# Hello there, 暴走 林! 上传图片传照片是 post 的一种 传送完照片以后的 url 有变动，“choose file” 按键链接的 &lt; input&gt; 是一个叫 uploadFile 的名字，将这个名字放入 python 的字典当一个 “key” 在字典中, 使用 open 打开一个图片文件, 当做要上传的文件，把这个字典放入 post 里面的 files 参数. 就能上传图片，网页会返回一个页面, 将图片名显示在上面. 12345file = &#123;'uploadFile': open('./image.png', 'rb')&#125;r = requests.post('http://pythonscraping.com/files/processing2.php', files=file)print(r.text)# The file image.png has been uploaded. 登录 登录账号, 浏览器做了: 1.使用 post 方法登录了第一个红框的 url 2.post 的时候, 使用了 Form data 中的用户名和密码 3.生成了一些 cookies 因为打开网页时, 每一个页面都是不连续的, 没有关联的, cookies 就是用 来衔接一个页面和另一个页面的关系 用 requests.post + payload 的用户信息发给网页, 返回的 r 里面会有生成的cookies 信息. 请求去登录后的页面时, 使用 request.get, 并将之前的 cookies 传入到 get 请求. 就能以登录的名义访问 get 的页面. 1234567891011payload = &#123;'username': 'Morvan', 'password': 'password'&#125;r = requests.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)print(r.cookies.get_dict())# &#123;'username': 'Morvan', 'loggedin': '1'&#125;r = requests.get('http://pythonscraping.com/pages/cookies/profile.php', cookies=r.cookies)print(r.text)# Hey Morvan! Looks like you're still logged into the site! 使用 Session 登录创建完一个 session 过后, 直接只用session 来 post 和 get，而且这次 get 的时候, 并没有传入 cookies. 但是实际上 session 内部就已经有了之前的 cookies 123456789101112session = requests.Session()payload = &#123;'username': 'Morvan', 'password': 'password'&#125;r = session.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)print(r.cookies.get_dict())# &#123;'username': 'Morvan', 'loggedin': '1'&#125;r = session.get("http://pythonscraping.com/pages/cookies/profile.php")print(r.text)# Hey Morvan! Looks like you're still logged into the site! 3.2 下载文件1234import osos.makedirs('./img/', exist_ok=True)IMAGE_URL = "https://morvanzhou.github.io/static/img/description/learning_step_flowchart.png" 使用 urlretrieve在 urllib 模块中, 提供了一个下载功能 urlretrieve. 输入下载地址 IMAGE_URL 和要存放的位置. 图片就会被自动下载过去 12from urllib.request import urlretrieveurlretrieve(IMAGE_URL, './img/image1.png') 使用 request1234import requestsr = requests.get(IMAGE_URL)with open('./img/image2.png', 'wb') as f:f.write(r.content) 如果要下载的是大文件, 比如视频等. requests 能下一点, 保存一点，而不是要全部下载完才能保存去另外的地方，这就是一个 chunk 一个 chunk 的下载. 使用 r.iter_content(chunk_size) 来控制每个 chunk 的大小, 然后在文件中写入这个 chunk 大小的数据. 12345r = requests.get(IMAGE_URL, stream=True) # stream loadingwith open('./img/image3.png', 'wb') as f:for chunk in r.iter_content(chunk_size=32):f.write(chunk) 3.3 小练习: 下载美图 图片都存在于 img_list 的这种 &lt; ul&gt; 中 图片地址都是在 &lt; img&gt; 中 1&lt;img src="http://image.nationalgeographic.com.cn/2017/1228/20171228030617696.jpg"&gt; 下载图片12345678910111213141516171819202122232425from bs4 import BeautifulSoupimport requestsURL = "http://www.nationalgeographic.com.cn/animals/"html = requests.get(URL).textsoup = BeautifulSoup(html, 'lxml')img_ul = soup.find_all('ul', &#123;"class": "img_list"&#125;)for ul in img_ul:imgs = ul.find_all('img')for img in imgs:url = img['src']r = requests.get(url, stream = True)image_name = url.split('/')[-1]with open('./img/%s' % image_name, 'wb') as f:for chunk in r.iter_content(chunk_size = 128):f.write(chunk)print('Saved %s' % image_name)"""Saved 20171227102206573.jpg...Saved 20171214020322682.jpg""" 加速你的爬虫4.1 加速爬虫: 多进程分布式 首页中有很多url,使用多进程同时开始下载这些url,得到这些 url 的HTML以后, 同时开始解析网页内容. 在网页中寻找这个网站还没有爬过的链接. 最终爬完整个莫烦Python网站所有页面 12345678import multiprocessing as mpimport timefrom urllib.request import urlopen, urljoinfrom bs4 import BeautifulSoupimport rebase_url = 'https://morvanzhou.github.io/' 12345678910111213def crawl(url):response = urlopen(url)time.sleep(0.1) # slightly delay for downloadingreturn response.read().decode()def parse(html):soup = BeautifulSoup(html, 'lxml')urls = soup.find_all('a', &#123;"href": re.compile('^/.+?/$')&#125;)title = soup.find('h1').get_text().strip()page_urls = set([urljoin(base_url, url['href']) for url in urls])url = soup.find('meta', &#123;'property': "og:url"&#125;)['content']return title, page_urls, url 123456789101112131415161718192021222324unseen = set([base_url,])seen = set()count, t1 = 1, time.time()while len(unseen) != 0: # still get some url to visitif len(seen) &gt; 20:breakprint('\nDistributed Crawling...')htmls = [crawl(url) for url in unseen]print('\nDistributed Parsing...')results = [parse(html) for html in htmls]print('\nAnalysing...')seen.update(unseen) # seen the crawledunseen.clear() # nothing unseenfor title, page_urls, url in results:print(count, title, url)count += 1unseen.update(page_urls - seen) # get new url to crawlprint('Total time: %.1f s' % (time.time()-t1, )) # 53 s 1234567891011121314151617181920212223242526if __name__ == '__main__':unseen = set([base_url,])seen = set()pool = mp.Pool(4)count, t1 = 1, time.time()while len(unseen) != 0: # still get some url to visitif len(seen) &gt; 20:breakprint('\nDistributed Crawling...')crawl_jobs = [pool.apply_async(crawl, args=(url,)) for url in unseen]htmls = [j.get() for j in crawl_jobs] # request connectionprint('\nDistributed Parsing...')parse_jobs = [pool.apply_async(parse, args=(html,)) for html in htmls]results = [j.get() for j in parse_jobs] # parse htmlprint('\nAnalysing...')seen.update(unseen) # seen the crawledunseen.clear() # nothing unseenfor title, page_urls, url in results:print(count, title, url)count += 1unseen.update(page_urls - seen) # get new url to crawlprint('Total time: %.1f s' % (time.time()-t1, )) # 16 s !!! 4.2 加速爬虫: 异步加载 Asyncio异步加载 Asyncio原理: 在单线程里使用异步计算, 下载网页的时候和处理网页的时候是不连续的, 更有效利用了等待下载的这段时间 传统的单线程下载处理网页就像下图(来源)左边蓝色那样, 计算机执行一些代码, 然后等待下载网页, 下好以后, 再执行一些代码… 或者在等待的时候,用另外一个线程执行其他的代码, 这是多线程的手段. asyncio就像右边,只使用一个线程,但是将这些等待时间统统掐掉,下载都调到后台,这个时间里,执行其他异步的功能,下载好了之后,再调回来接着往下执行 asyncio是一个线程,是在 Python 的功能间切换着执行. 切换的点用await来标记, 能够异步的功能用async标记, 比如async def function(): 12345678910111213141516171819202122232425# 不是异步的import timedef job(t):print('Start job ', t)time.sleep(t) # wait for "t" secondsprint('Job ', t, ' takes ', t, ' s')def main():[job(t) for t in range(1, 3)]t1 = time.time()main()print("NO async total time : ", time.time() - t1)"""Start job 1Job 1 takes 1 sStart job 2Job 2 takes 2 sNO async total time : 3.008603096008301""" 使用 asyncio 的形式, job 1 在等待 time.sleep(t) 结束的时候, 比如是等待一个网页的下载成功, 在这个地方是可以切换给 job 2, 让它开始执行. 12345678910111213141516171819202122232425262728import asyncioasync def job(t): # async 形式的功能print('Start job ', t)await asyncio.sleep(t) # 等待 "t" 秒, 期间切换其他任务print('Job ', t, ' takes ', t, ' s')async def main(loop): # async 形式的功能tasks = [loop.create_task(job(t)) for t in range(1, 3)] # 创建任务, 但是不执行await asyncio.wait(tasks) # 执行并等待所有任务完成t1 = time.time()loop = asyncio.get_event_loop() # 建立 looploop.run_until_complete(main(loop)) # 执行 looploop.close() # 关闭 loopprint("Async total time : ", time.time() - t1)"""Start job 1Start job 2Job 1 takes 1 sJob 2 takes 2 sAsync total time : 2.001495838165283""" job 1 触发了 await 的时候就切换到了 job 2 了. 这时, job 1 和 job 2 同时在等待 await asyncio.sleep(t), 所以最终的程序完成时间, 取决于等待最长的 t, 也就是 2秒 aiohttp1234567891011121314151617181920import requestsURL = 'https://morvanzhou.github.io/'def normal():for i in range(2):r = requests.get(URL)url = r.urlprint(url)t1 = time.time()normal()print("Normal total time:", time.time()-t1)"""https://morvanzhou.github.io/https://morvanzhou.github.io/Normal total time: 0.3869960308074951""" 12345678910111213141516171819202122232425import aiohttpasync def job(session):response = await session.get(URL) # 等待并切换return str(response.url)async def main(loop):async with aiohttp.ClientSession() as session: # 官网推荐建立 Session 的形式tasks = [loop.create_task(job(session)) for _ in range(2)]finished, unfinished = await asyncio.wait(tasks)all_results = [r.result() for r in finished] # 获取所有结果print(all_results)t1 = time.time()loop = asyncio.get_event_loop()loop.run_until_complete(main(loop))loop.close()print("Async total time:", time.time() - t1)"""['https://morvanzhou.github.io/', 'https://morvanzhou.github.io/']Async total time: 0.11447715759277344""" 和多进程分布式爬虫对比 asyncio :解析网页还是用的和 multiprocessing 一样的并行处理, 因为asyncio 不支持解析网页的异步, 毕竟是计算密集型工序. 在下载网页时, 不用 multiprocessing, 改用 asyncio, 用一个单线程的东西挑战多进程. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import aiohttpimport asyncioimport timefrom bs4 import BeautifulSoupfrom urllib.request import urljoinimport reimport multiprocessing as mpbase_url = "https://morvanzhou.github.io/"seen = set()unseen = set([base_url])def parse(html):soup = BeautifulSoup(html, 'lxml')urls = soup.find_all('a', &#123;"href": re.compile('^/.+?/$')&#125;)title = soup.find('h1').get_text().strip()page_urls = set([urljoin(base_url, url['href']) for url in urls])url = soup.find('meta', &#123;'property': "og:url"&#125;)['content']return title, page_urls, urlasync def crawl(url, session):r = await session.get(url)html = await r.text()await asyncio.sleep(0.1) # slightly delay for downloadingreturn htmlasync def main(loop):pool = mp.Pool(8) # slightly affectedasync with aiohttp.ClientSession() as session:count = 1while len(unseen) != 0:print('\nAsync Crawling...')tasks = [loop.create_task(crawl(url, session)) for url in unseen]finished, unfinished = await asyncio.wait(tasks)htmls = [f.result() for f in finished]print('\nDistributed Parsing...')parse_jobs = [pool.apply_async(parse, args=(html,)) for html in htmls]results = [j.get() for j in parse_jobs]print('\nAnalysing...')seen.update(unseen)unseen.clear()for title, page_urls, url in results:# print(count, title, url)unseen.update(page_urls - seen)count += 1if __name__ == "__main__":t1 = time.time()loop = asyncio.get_event_loop()loop.run_until_complete(main(loop))# loop.close()print("Async total time: ", time.time() - t1)"""Async Crawling...Distributed Parsing...Analysing...Async Crawling...Distributed Parsing...Analysing...Async Crawling...Distributed Parsing...Analysing...Async total time: 7.21798300743103""" 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081from urllib.request import urlopen, urljoinfrom bs4 import BeautifulSoupimport multiprocessing as mpimport reimport timedef crawl(url):response = urlopen(url)time.sleep(0.1) # slightly delay for downloadingreturn response.read().decode()def parse(html):soup = BeautifulSoup(html, 'lxml')urls = soup.find_all('a', &#123;"href": re.compile('^/.+?/$')&#125;)title = soup.find('h1').get_text().strip()page_urls = set([urljoin(base_url, url['href']) for url in urls])url = soup.find('meta', &#123;'property': "og:url"&#125;)['content']return title, page_urls, urlif __name__ == '__main__':# base_url = 'https://morvanzhou.github.io/'base_url = "http://127.0.0.1:4000/"# DON'T OVER CRAWL THE WEBSITE OR YOU MAY NEVER VISIT AGAINif base_url != "http://127.0.0.1:4000/":restricted_crawl = Trueelse:restricted_crawl = Falseunseen = set([base_url,])seen = set()pool = mp.Pool(8) # number strongly affectedcount, t1 = 1, time.time()while len(unseen) != 0: # still get some url to visitif restricted_crawl and len(seen) &gt; 20:breakprint('\nDistributed Crawling...')crawl_jobs = [pool.apply_async(crawl, args=(url,)) for url in unseen]htmls = [j.get() for j in crawl_jobs] # request connectionhtmls = [h for h in htmls if h is not None] # remove Noneprint('\nDistributed Parsing...')parse_jobs = [pool.apply_async(parse, args=(html,)) for html in htmls]results = [j.get() for j in parse_jobs] # parse htmlprint('\nAnalysing...')seen.update(unseen)unseen.clear()for title, page_urls, url in results:# print(count, title, url)count += 1unseen.update(page_urls - seen)print('Total time: %.1f s' % (time.time()-t1, ))"""Distributed Crawling...Distributed Parsing...Analysing...Distributed Crawling...Distributed Parsing...Analysing...Distributed Crawling...Distributed Parsing...Analysing...Total time: 11.5 s""" Number of Process Multiprocessing Asyncio 2 25.5s 7.5s 4 15.4s 7.0s 8 11.5s 7.2s 高级爬虫5.1 高级爬虫: 让 Selenium 控制你的浏览器帮你爬Python 控制浏览器将 selenium 绑定到 Chrome 上 webdriver.Chrome()： 1234567891011121314151617from selenium import webdriverdriver = webdriver.Chrome() # 打开 Chrome 浏览器# 将刚刚复制的帖在这driver.get("https://morvanzhou.github.io/")driver.find_element_by_xpath(u"//img[@alt='强化学习 (Reinforcement Learning)']").click()driver.find_element_by_link_text("About").click()driver.find_element_by_link_text(u"赞助").click()driver.find_element_by_link_text(u"教程 ▾").click()driver.find_element_by_link_text(u"数据处理 ▾").click()driver.find_element_by_link_text(u"网页爬虫").click()# 得到网页 html, 还能截图html = driver.page_source # get htmldriver.get_screenshot_as_file("./img/sreenshot1.png")driver.close() 让 selenium 不弹出浏览器窗口,创建 driver 之前定义几个参数就能摆脱浏览器: 1234567from selenium.webdriver.chrome.options import Optionschrome_options = Options()chrome_options.add_argument("--headless") # define headlessdriver = webdriver.Chrome(chrome_options=chrome_options)... Selenium 能做的事:填 Form 表单, 超控键盘等等 5.2 高级爬虫: 高效无忧的 Scrapy 爬虫库ScrapyScrapy 是一个整合了的爬虫框架, 有着非常健全的管理系统. 也是分布 式爬虫 一定还要给这个 spider 一个名字,给定一些初始爬取的网页, 写在start_urls 里,在 scrapy 中它自动帮你去重 123456789101112131415161718import scrapyclass MofanSpider(scrapy.Spider):name = "mofan"start_urls = ['https://morvanzhou.github.io/',]# unseen = set()# seen = set() # 我们不在需要 set 了, 它自动去重def parse(self, response):yield &#123; # return some results'title': response.css('h1::text').extract_first(default='Missing').strip().replace('"', ""),'url': response.url,&#125;urls = response.css('a::attr(href)').re(r'^/.+?/$') # find all sub urlsfor url in urls:yield response.follow(url, callback=self.parse) # it will filter duplication automatically 不需要使用 urljoin(),在 follow() 这一步会自动检测 url 的格式 1$ scrapy runspider 5-2-scrapy.py -o res.json -s FEED_EXPORT_ENCODING=utf-8 -o res.json 这个 -o 就是输出的指令, 可以在那个文件夹中找到一个名字叫 res.json 的文件, 里面存有所有找到的 {title:, url:}.]]></content>
      <categories>
        <category>爬虫</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福机器学习笔记(Week 7)]]></title>
    <url>%2F2019%2F03%2F05%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Week-7%2F</url>
    <content type="text"><![CDATA[十、支持向量机( Support Vector Machines)10.1 优化目标代价函数 在逻辑回归中，预测函数为： h_{\theta}(x) = \frac{1}{1+e^{-\theta^T x}}代价函数为： cost = -(ylog(h_{\theta}(x))+(1-y)log(1-h_{\theta}(x)))当y=1时，代价函数就为： cost = -log(h_{\theta}(x) ) = -log\frac{1}{1+e^{-z}}, \quad z=\theta^T x此时，代价函数随z的变化曲线如下图： 当y=1时，随着z取值变大，预测代价变小，因此，逻辑回归想要在面对正样本y=1时，获得足够高的预精度，就希望z=\theta^T x \gg 0。而 SVM 则将上图的曲线拉直为下图中的折线，构成了y=1时的代价函数曲线cost_1(z): 当y=1时，为了预测精度足够高，SVM 希望\theta^T x \geq 1 在y = 0时，SVM 定义了代价函数cost_0(z)，为了预测精度足够高，SVM 希望\theta^T x \leq -1： 最小化预测代价SVM定义其最小化预测代价的过程为： \min\limits_{\theta}C[\sum\limits_{i=1}^{m}y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum\limits_{j=1}^{n}\theta_j^2而在逻辑回归中，最小化预测代价的过程为： \min\limits_{\theta}\frac{1}{m}[\sum\limits_{i=1}^{m}y^{(i)}(-logh_\theta(x^{(i)}))+(1-y^{(i)})(-log(1-h_\theta(x^{(i)})))] + \frac{\lambda}{2m}\sum\limits_{j=1}^{n}\theta_j^2可以将逻辑回归的代价函数简要描述为： cost = A+\lambda BSVM 的代价函数描述为： cost = CA+B在逻辑回归中，通过正规化参数\lambda调节A、B所占的权重，且A的权重与\lambda取值成反比。而在 SVM 中，则通过参数C调节A、B所占的权重，且A权重与C的取值成反比。即参数C可以被认为是扮演了\frac{1}{\lambda}的角色 预测函数当训练得到\theta之后，可以代入下面的 SVM 预测函数进行预测： h_\theta(x) = \begin{cases} 1,\quad \text{if } \theta^T x {\geq 0} \\ 0,\quad \text{otherwise} \end{cases}10.2 大间距分类器SVM 最小化代价函数过程为： \min\limits_{\theta}C[\sum\limits_{i=1}^{m}y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum\limits_{j=1}^{n}\theta_j^2当y^{(i)}=1时，SVM 希望\theta^Tx^{(i)} \geq 1；而当y^{(i)}=0时，SVM 希望\theta^Tx^{(i)} \leq -1 最小化代价函数的过程就可以描述为： \begin{aligned} & \min\frac{1}{2}\sum\limits_{j=1}^{n}\theta_j^2 \\ \text{s.t}. \quad &\theta^T x^{(i)} \geq 1 &\text{if } y^{(i)}=1 \\ & \theta^T x^{(i)} \leq -1 &\text{if } y^{(i)}=0 \end{aligned}SVM 最终找出的决策边界会是下图中黑色直线所示的决策边界，而不是绿色或者紫色的决策边界。该决策边界保持了与正、负样本都足够大的距离，这个距离叫做支持向量机的间距。因此，SVM 是典型的大间距分类器（Large margin classifier） 回顾 C=1/\lambda： $C$ 较大时，相当于 \lambda 较小，可能会导致过拟合，高方差。 $C$ 较小时，相当于 \lambda 较大，可能会导致欠拟合，高偏差。 推导假定有两个 2 维向量： u=\left(\begin{matrix}u_1 \\ u_2\end{matrix}\right),v=\left(\begin{matrix}v_1 \\ v_2\end{matrix}\right) 内积为： \begin{aligned} u^Tv &= p \cdot ||u|| \\ &= u_1v_1+u_2v_2 \end{aligned}$||u||$为u的范数，也是u的长度 假定\theta=\left(\begin{matrix}\theta_1 \\ \theta_2\end{matrix}\right)，且令\theta_0 = 0，以使得向量\theta过原点，则： \begin{aligned} \min\limits_\theta\frac{1}{2}\sum\limits_{j=1}^{2}\theta_j^2 &=\min\limits_\theta\frac{1}{2}(\theta_1^2+\theta_2^2) \\ &=\min\limits_\theta\frac{1}{2}(\sqrt{\theta_1^2+\theta_2^2})^2 \\ &=\min\limits_\theta\frac{1}{2}||\theta||^2 \end{aligned}由向量内积公式可得： \theta^T x^{(i)} = p^{(i)}\cdot||\theta||$p^{(i)}$为特征向量x^{(i)}在\theta上的投影： 当y^{(i)}=1时，希望\theta^T x^{(i)} \geq 1，亦即希望p^{(i)}\cdot||\theta|| \geq 1，此时考虑两种情况： 1.p^{(i)}很小，则需要||\theta||很大，这与\min\limits_\theta\frac{1}{2}||\theta||^2矛盾 2.p^{(i)}很大，如下图所示，即样本与决策边界的距离足够大，才能在既要||\theta||足够小的情况下，又能有 $\theta^Tx^{(i)} \geq 1$，保证预测精度够高。这解释了为什么 SVM 的模型会具有大间距分类器的性质 10.3 核函数定义 在逻辑回归中，会通过多项式扩展来处理非线性分类问题： h_\theta(x) = \theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+\cdots假设令： f_1=x_1,f_2=x_2,f_3=x_1x_2,f_4=x_1^2,f_5=x_2^2则预测函数为： h_\theta(x) = \theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3+\cdots但多项式回归所带来的高阶项不一定作用明显，针对这一问题，SVM不会引入高阶项来作为新的特征，而是会选择一些标记点（landmark），并将样本x与标记点l^{(i)}的相似程度作为新的训练特征f_i： f_i = similarity(x, l^{(i)}) 距离度量的方式就称之为核函数（Kernel），最常见的核函数是高斯核函数（Gaussian Kernel）： f_i = exp(-\frac{||x-l^{(i)}||^2}{2\sigma ^2})其中： ||x-l^{(i)}||^2 = \sum_{j=1}^{n}(x_j-l_j^{(i)})^2为实例 x中所有特征与地标l^{(i)}之间的距离的和 注：这个函数与正态分布没什么实际上的关系，只是看上去像而已 在高斯核中，注意到： 如果样本与标记点足够接近，即x \approx l^{(i)}，则： f \approx exp(-\frac{0^2}{2\sigma ^2}) \approx 1 如果样本远离标记点，则： f \approx exp(-\frac{(\text{large number})^2}{2\sigma ^2}) \approx 0假设训练实例含有两个特征[x_1\quad x_2 ],给定地标l^{(1)}与不同的\sigma值，见下图： 在使用高斯核函数前，需要做特征缩放（feature scaling），以使 SVM 同等程度地关注到不同的特征 水平面的坐标为 x_1 ，x_2 ,而垂直坐标轴代表 f。只有当 x 与 l^{ (1)} 重合时f才具有最大值。随着x的改变f值改变的速率受到\sigma ^2的控制 图中红色的封闭曲线所表示的范围，是依据一个单一的训练实例和选取的地标所得出的判定边界，在预测时，采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征f _1 ,f_ 2 ,f _3 标记点选取假定有如下的数据集： (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),(x^{(3)},y^{(3)}) \cdots (x^{(m)},y^{(m)})将每个样本作为一个标记点： l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},l^{(3)}=x^{(3)} \cdots l^{(m)}=x^{(m)}对于样本(x^{(i)}, y^{(i)})，计算其与各个标记点的距离： f_1^{(i)}=sim(x^{(i)},l^{(1)}) f_2^{(i)}=sim(x^{(i)},l^{(2)}) \vdots f_i^{(i)}=sim(x^{(i)},l^{(i)})=exp(-\frac{0^2}{2\sigma ^2})=1 \vdots f_m^{(i)}=sim(x^{(i)},l^{(m)})得到新的特征向量：f \in R^{m+1} f=\left(\begin{matrix}f_0 \\ f_1 \\ f_2 \\ \vdots \\ f_m \end{matrix}\right) \quad \text{其中} f_0=1将核函数运用到支持向量机中，修改支持向量机假设为： 给定x，计算新特征f，当\theta ^T f>=0时，预测y=1，否则反之。 相应地修改代价函数为： \min\limits_{\theta}C\big[\sum\limits_{i=1}^{m}(y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)}))\big]+\frac{1}{2}\sum\limits_{j=1}^{n=m}\theta_j^2在计算\sum\limits_{j=1}^{n=m}\theta_j^2=\theta^T\theta时，用\theta^T M\theta代替\theta ^T \theta，其中M是根据选择的核函数而不同的一个矩阵。这样做的原因是为了简化计算。理论上讲，也可以在逻辑回归中使用核函数，但是上面使用M来简化计算的方法不适用于逻辑回归，因为计算将非常耗费时间 支持向量机也可以不使用核函数，不使用核函数又称为线性核函数（linear kernel），当不采用非常复杂的函数，或者训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机 支持向量机的两个参数C和\sigma的影响： $C=1/\lambda$ C 较大时，相当于\lambda较小，可能会导致过拟合，高方差 C 较小时，相当于\lambda较大，可能会导致欠拟合，高偏差 \sigma较大时，可能会导致低方差，高偏差 \sigma较小时，可能会导致低偏差，高方差 10.4 使用支持向量机选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值 使用流行库作为当今最为流行的分类算法之一，SVM 已经拥有了不少优秀的实现库，如libsvm等，因此，不需要自己手动实现 SVM（一个能用于生产环境的 SVM 模型并非课程中介绍的那么简单） 在使用这些库时，通常需要声明 SVM 的两个关键部分： 参数C的选择 选择内核参数或想要使用的相似函数，其中一个选择是：选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。 核函数的选择： 当特征维度n较高，而样本规模m较小时，不宜使用核函数，否则容易引起过拟合 当特征维度n较低，而样本规模m足够大时，考虑使用高斯核函数。不过在使用高斯核函数前，需要进行特征缩放（feature scaling）。另外，当核函数的参数\sigma较大时，特征f_i较为平缓，即各个样本的特征差异变小，此时会造成欠拟合（高偏差，低方差）： 当\sigma较小时，特征f_i曲线变化剧烈，即各个样本的特征差异变大，此时会造成过拟合（低偏差，高方差）： 不是所有的相似度评估手段都能被用作SVM核函数，他们需要满足Mercer 理论 多分类问题流行的SVM库已经内置了多分类相关的 api，如果其不支持多分类，则与逻辑回归一样，使用 One-vs-All策略来进行多分类： 轮流选中某一类型i，将其视为正样本，即 “1” 分类，剩下样本都看做是负样本，即 “0” 分类 训练 SVM 得到参数\theta^{(1)}, \theta^{(2)}, ..., \theta^{(K)}，即总共获得了K-1个决策边界 分类模型的选择分类模型有：（1）逻辑回归；（2）神经网络；（3）SVM 考虑特征维度n及样本规模m普遍使用的准则： (1)如果相较于 m 而言，n 要大许多，即训练集数据量不够支持训练一个复杂的非线性模型，就选用逻辑回归模型或者不带核函数的支持向量机。 (2)如果 n 较小，m 大小中等，例如 n 在 1-1000 之间，而 m 在 10-10000 之间，使用高斯核函数的支持向量机 (3)如果 n 较小，而 m 较大，例如 n 在 1-1000 之间，而 m 大于 50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征（比如通过多项式扩展），然后使用逻辑回归或不带核函数的支持向量机 神经网络对于上述情形都有不错的适应性，但是计算性能上较慢]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福机器学习笔记(Week 9)]]></title>
    <url>%2F2019%2F03%2F05%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Week-9%2F</url>
    <content type="text"><![CDATA[十三、异常检测(Anomaly Detection)13.1 问题的动机异常检测是机器学习算法的一个常见应用。这种算法的一个有趣之处在于：它虽然主要用于非监督学习问题，但从某些角度看，它又类似于一些监督学习问题 飞机引擎 QA(质量控制测试)飞机引擎的一些特征变量作为这个测试的一部分，比如引擎运转时产生的热量，或者引擎的振动等等 数据集绘制成图表： 每个点、每个叉，都是无标签数据 异常检测问题可以定义如下：有一个新的飞机引擎从生产线上流出，新飞机引擎有特征变量x_ {test} 。 异常检测问题就是：知道这个新的飞机引擎是否有某种异常，判断这个引擎是否需要进一步测试 假使数据集是正常的，希望知道新的数据x _{test}是不是异常的，即这个测试数据不属于该组数据的几率如何。 构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性p(x) 蓝圈内的数据属于该组数据的可能性较高，越是偏远的数据，属于该组数据的可能性就越低 这种方法称为密度估计，表达如下： x= \begin{cases} \text{异常样本}, \text{if } p(x)< \epsilon \\ \text{正常样本}, \text{otherwise} \end{cases}应用识别欺骗例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户 检测一个数据中心特征可能包含：内存使用情况，被访问的磁盘数量，CPU 的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是不是有可能出错 13.2 高斯分布变量x 符合高斯分布X \sim N(\mu,\sigma^2)则其概率密度函数为： p(x; \mu, \sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \begin{aligned} \mu_j &= \frac{1}{m}\sum\limits_{i=1}^mx_j^{(i)} \\ \sigma^2_j &= \frac{1}{m}\sum\limits_{i=1}^m(x_j^{(i)}-\mu_j)^2 \end{aligned}高斯分布样例： 13.3 算法异常检测算法： 对于给定的数据集{x^{(1)},x^{(2)},\cdots,x^{(m)}}, x \in R^n,针对每一个特征计算\mu和\sigma^2的估计值: \begin{aligned} \mu_j &= \frac{1}{m}\sum\limits_{i=1}^mx_j^{(i)} \\ \sigma^2_j &= \frac{1}{m}\sum\limits_{i=1}^m(x_j^{(i)}-\mu_j)^2 \end{aligned} 根据模型计算p(x): \begin{aligned} p(x) &= p(x_1;\mu_1, \sigma^2_1)p(x_2;\mu_2, \sigma^2_2) \cdots p(x_n;\mu_n, \sigma^2_n) \\ &= \prod\limits_{j=1}^np(x_j;\mu_j,\sigma_j^2) \\ &= \prod\limits_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2}) \end{aligned} 当p(x)\epsilon时预测数据为正常数据，否则则为异常 13.4 开发和评价一个异常检测系统当开发一个异常检测系统时，从带标记（异常或正常）的数据着手，选择一部分正常数据用于构建训练集，用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集 例如：有 10000 台正常引擎的数据，有 20 台异常引擎的数据。 分配如下： 6000 台正常引擎的数据作为训练集 2000 台正常引擎和 10 台异常引擎的数据作为交叉检验集 2000 台正常引擎和 10 台异常引擎的数据作为测试集 由于异常样本是非常少的，所以整个数据集是非常偏斜的，不能单纯的用预测准确率来评估算法优劣 具体的评价方法如下： 根据测试集数据，估计特征的平均值和方差并构建p( x )函数 对交叉检验集，尝试使用不同的\epsilon值作为阀值，并预测数据是否异常，根据F1值或者查准率与召回率的比例来选择\epsilon 选出\epsilon后，针对测试集进行预测，计算异常检验系统的F1值，或者查准率与召回率之比 13.5 异常检测与监督学习对比 有监督学习 异常检测 数据分布均匀 数据非常偏斜，异常样本数目远小于正常样本数目 可以根据对正样本的拟合来知道正样本的形态，从而预测新来的样本是否是正样本 异常的类型不一，很难根据对现有的异常样本（即正样本）的拟合来判断出异常样本的形态，未来遇到的异常可能与已掌握的异常非常的不同 应用场景： 有监督学习 异常检测 垃圾邮件检测 故障检测 天气预测（预测雨天、晴天、或是多云天气） 某数据中心对于机器设备的监控 癌症的分类 制造业判断一个零部件是否异常 …… …… 13.6 选择特征如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数：x = log(x+c)，其中 c为非负常数； 或者x = x^c， c 为 0-1 之间的一个分数 误差分析： 一个常见的问题是一些异常的数据可能也会有较高的p(x)值，因而被算法认为是正常的。这种情况下误差分析能够分析那些被算法错误预测为正常的数据，观察能否找出一些问题。可能能从问题中发现需要增加一些新的特征，增加这些新特征后获得的新算法能够更好地进行异常检测 异常检测误差分析： 通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小） 在检测数据中心的计算机状况的例子中，用 CPU负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中 13.7 多元高斯分布服务器运转监控的问题假使有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。 下图中是两个相关特征，洋红色的线（根据 \varepsilon 的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的 x 所代表的数据点很可能是异常值，但是其p (x )值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界： 在一般的高斯分布模型中，计算p (x )的方法是： 通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，将构建特征的协方差矩阵，用所有的特征一起来计算p (x ) 定义多元高斯分布模型被定义为： p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$\mu$表示样本均值，\Sigma表示样本协方差矩阵 参数 改变\Sigma主对角线的数值可以进行不同方向的宽度拉伸： 改变\Sigma次对角线的数值可以旋转分布图像： 改变\mu可以对分布图像进行位移： 算法流程多元高斯分布的异常检测算法流程如下： 选择一些足够反映异常样本的特征x_j 对各个样本进行参数估计： \begin{aligned} \mu &= \frac{1}{m}\sum\limits_{i=1}^mx^{(i)} \\ \Sigma &= \frac{1}{m}\sum\limits_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T=\frac{1}{m}(X-\mu)(X-\mu)^T \end{aligned} 当新的样本x到来时，计算p(x)： p(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)) 如果p(x)< \varepsilon，则认为样本x是异常样本 多元高斯分布模型与一般高斯分布模型的差异一般的高斯分布模型只是多元高斯分布模型的一个约束，它将多元高斯分布的等高线约束到了如下所示同轴分布（概率密度的等高线是沿着轴向的）： 一般高斯模型： 需要手动创建一些特征来描述某些特征的相关性 计算复杂度低，适用于高维特征 在样本数目 m 较小时也工作良好 \begin{aligned} p(x) &= p(x_1;\mu_1, \sigma^2_1)p(x_2;\mu_2, \sigma^2_2) \cdots p(x_n;\mu_n, \sigma^2_n)\\&= \prod\limits_{j=1}^np(xj;\mu_j,\sigma_j^2) \\&= \prod\limits_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})\end{aligned} 多元高斯模型： 利用协方差矩阵\Sigma 获得了各个特征相关性 计算复杂 需要\Sigma可逆，亦即需要 m>n ，通常需要m>10n且各个特征不能线性相关，如不能存在 x_2=3x_1 或者x_3=x_1+2x_2 p(x)=\frac{1}{(2\pi)^{\frac{n}{2}}\Sigma^\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)) 十四、推荐系统( Recommender Systems)14.1 问题形式化对机器学习来说，特征是很重要的，选择的特征，将对学习算法的性能有很大的影响 标记： n_u代表用户的数量 n_m代表电影的数量 $r(i,j)$如果用户j给电影评过分则r(i,j)=1 y^{(i,j)}代表用户j给电影i的评分 m^{(j)}代表用户j评过分的电影的总数 14.2 基于内容的推荐系统假设每部电影都有两个特征，如x _1代表电影的浪漫程度，x_ 2代表电影的动作程度 则每部电影都有一个特征向量，如x ^{(1)}是第一部电影的特征向量为[0.9 0] 参数 \theta^{(j)}用户j的参数向量 x ^{(i)}电影i的特征向量 对于用户j和电影i，预测评分为：y^{(i,j)}=(\theta^{(j)})^Tx^{(i)} 针对用户j，该线性回归模型的代价为预测误差的平方和，加上正则化项： \min_{\theta^{(j)}} = \frac{1}{2} \sum_{i:r(i,j)=1} \left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2 + \frac{\lambda}{2} \sum_{k=1}^n (\theta_k^{(j)})^2$i:r(i,j)=1$表示只计算那些用户j评过分的电影 在一般的线性回归模型中，误差项和正则项应该都是乘以\frac{1}{2m}，在这里将m去掉,并且不对\theta^{(j)}_0进行正则化处理 对于所有用户1, 2, ... , n_u，代价函数J(\theta^{(1)}, \theta^{(2)}, ..., \theta^{(n_u)})： J(\theta^{(1)}, \theta^{(2)}, ..., \theta^{(n_u)})=\min_{\theta^{(1)}, \theta^{(2)}, ..., \theta^{(n_u)}} = \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} \left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2参数更新使用梯度下降法来更新参数： \begin{aligned} & \text{更新偏置(插值):} \\ & \quad \theta_0^{(j)} = \theta_0^{(j)} - \alpha\sum_{i:r(i,j)=1}\big((\theta^{(j)})^T x^{(i)} - y^{(i,j)}\big) x_0^{(i)} \\ & \text{更新权重:} \\ & \quad \theta_k^{(j)} = \theta_k^{(j)} - \alpha\left(\sum_{i:r(i,j)=1}\big((\theta^{(j)})^T x^{(i)} - y^{(i,j)}\big) x_k^{(i)} + \lambda\theta_k^{(j)}\right), \quad k \neq 0 \end{aligned}14.3 协同过滤在现实中，不会有任何网站，任何人有精力，有能力去评估每部电影所具有的一些指数。因此，基于内容的推荐系统从构架初期，可能就会遭遇非常大的阻力 假定先有了各个用户对电影的偏爱评估\theta: \theta^{(1)} = \begin{pmatrix} 0 \\ 5 \\ 0 \end{pmatrix}, \theta^{(2)} = \begin{pmatrix} 0 \\ 5 \\ 0 \end{pmatrix}, \theta^{(3)} = \begin{pmatrix} 0 \\ 0 \\ 5 \end{pmatrix}, \theta^{(4)} = \begin{pmatrix} 0 \\ 5 \\ 0 \end{pmatrix}并且，不知道电影的指数： Movie/User Alice(1) Bob(2) Carol(3) Dave(4) x_1 x_2 Love at last 5 5 0 0 ? ? Romance for ever 5 4 ? 0 ? ? Cute puppies of love ? ? 0 ? ? ? Nonstop car chases 0 0 5 4 ? ? Swords vs. karate 0 0 5 ? ? ? 目标优化通过\theta^{(1)}, ..., \theta^{(n_u)}来学习x^{(i)}： \min_{x^{(i)}} = \frac{1}{2} \sum_{j:r(i,j)=1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i,j)}\right) ^2 + \frac{\lambda}{2} \sum_{k=1}^n (x_k^{(i)})^2对于所有的电影指数x^{(1)},...,x^{(n_m)}： \min_{x^{(1)},...,x^{(i)},...,x^{(n_m)}} = \frac{1}{2} \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i,j)}\right) ^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n (x_k^{(i)})^2协同过滤算法现在，拥有了评价用户的\theta和评价商品的x，并且： 给定\theta及用户对商品的评价，能估计x 给定x，又能估计\theta 因此，就构成了\theta−&gt;x−&gt;\theta−&gt;x...的优化序列，这便构成了协同过滤算法，即同时优化商品和用户具有的参数 协同过滤的目标优化1.推测用户喜好：给定x^{(1)},...,x^{(n_m)}，估计\theta^{(1)},...,\theta^{(n_u)}： \min_{\theta^{(1)}, ..., \theta^{(n_u)}} = \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} \left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^22.推测商品内容：给定\theta^{(1)},...,\theta^{(n_u)}，估计x^{(1)},...,x^{(n_m)}： \min_{x^{(i)},...,x^{(n_m)}} = \frac{1}{2} \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i,j)}\right) ^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n (x_k^{(i)})^23.协同过滤：同时优化x^{(1)},...,x^{(n_m)}及\theta^{(1)},...,\theta^{(n_u)}： \min J(x^{(1)},....x^{(i)},...,x^{(n_m)} ; \theta^{(1)},....\theta^{(j)} ..., \theta^{(n_u)})亦即： \min_{x^{(1)},....x^{(i)},...,x^{(n_m)} ; \theta^{(1)},....,\theta^{(j)}, ..., \theta^{(n_u)}} \frac{1}{2} \sum_{(i,j):r(i,j)=1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i,j)}\right) ^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n (x_k^{(i)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2$\sum_{(i,j):r(i,j)=1}$反映了用户和商品所有有效配对 算法流程使用了协同过滤的推荐算法流程为： 随机初始化x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}为一些较小值 使用梯度下降法来最小化J(x^{(1)},....x^{(i)},...,x^{(n_m)} ; \theta^{(1)},....\theta^{(j)} ..., \theta^{(n_u)})，j=1,2,..,n_u，i=1,2,...,n_m，参数的更新式为： \begin{aligned} & x_k^{(i)} = x_k^{(i)} - \alpha \left( \sum_{j:r(i,j)=1}\big((\theta^{(j)})^T x^{(i)} - y^{(i,j)}\big) \theta_k^{(j)} + \lambda x_k^{(i)}\right) \\ & \theta_k^{(j)} = \theta_k^{(j)} - \alpha\left(\sum_{i:r(i,j)=1}\big((\theta^{(j)})^T x^{(i)} - y^{(i,j)}\big) x_k^{(i)} + \lambda\theta_k^{(j)} \right) \end{aligned}如果用户的偏好向量为\theta，而商品的特征向量为x,则可以预测用户评价为\theta^Tx 因为协同过滤算法\theta和x相互影响，因此，二者都没必要使用偏置\theta_\theta和x _0，即，x \in R^n,\theta \in R^n 获得类似电影当获得了电影i的特征向量后，就可以通过计算||x^{(j)}—x^{(i)}||来比较电影j与电影i的相似度。那么，给予了电影j足够好评的用户，也会被推荐到类似的电影 14.4 向量化：低秩矩阵分解将用户对电影的评分表格： Movie/User Alice(1) Bob(2) Carol(3) Dave(4) Love at last 5 5 0 0 Romance for ever 5 ? ? 0 Cute puppies of love ? 4 0 ? Nonstop car chases 0 0 5 4 Swords vs. karate 0 0 5 ? 用矩阵表示： Y = \begin{bmatrix} 5 & 5 & 0 & 0 \\ 5 & ? & ? & 0 \\ ? & 4 & 0 & ? \\ 0 & 0 & 5 & 4 \\ 0 & 0 & 5 & 0 \end{bmatrix}用预测来描述这个矩阵： Predicated = \begin{bmatrix} (\theta^{(1)})^T x^{(1)} & (\theta^{(2)})^T x^{(1)} & \cdots & (\theta^{(n_u)})^T x^{(1)} \\ (\theta^{(1)})^T x^{(2)} & (\theta^{(2)})^T x^{(2)} & \cdots & (\theta^{(n_u)})^T x^{(2)} \\ \vdots & \vdots & \vdots & \vdots \\ (\theta^{(1)})^T x^{(n_m)} & (\theta^{(2)})^T x^{(n_m)} & \cdots & (\theta^{(n_u)})^T x^{(n_m)} \end{bmatrix}令： X = \begin{bmatrix} (x^{(1)})^T \\ (x^{(2)})^T \\ \vdots \\ (x^{(n_m)})^T \end{bmatrix}, \Theta = \begin{bmatrix} (\theta^{(1)})^T \\ (\theta^{(2)})^T \\ \vdots \\ (\theta^{(n_u)})^T \end{bmatrix}即X的每一行描述了一部电影的内容，\theta^T的每一列描述了用户对于电影内容偏好程度，亦即，将原来稀疏的矩阵分解为了X和\theta。现在预测可以写为： Predicated = X \Theta^T用这个方法求取X和\theta，获得推荐系统需要的参数，称之为低秩矩阵分解，该方法不仅能在编程时直接通过向量化的手法获得参数，还通过矩阵分解节省了内存空间 14.5 推行工作上的细节：均值归一化假定现在新注册了一个用户Eve(5)，他还没有对任何电影作出评价： Movie/User Alice(1) Bob(2) Carol(3) Dave(4) Eve(5) Love at last 5 5 0 0 ? Romance for ever 5 ? ? 0 ? Cute puppies of love ? 4 0 ? ? Nonstop car chases 0 0 5 4 ? Swords vs. karate 0 0 5 ? ? 为 Eve 推荐电影： 先求取各个电影的平均得分\mu： \mu = \begin{pmatrix} 2.5 \\ 2.5 \\ 2 \\ 2.25 \\ 1.25 \end{pmatrix} 并求取Y−\mu，对Y进行均值标准化： Y - \mu = \begin{bmatrix} 2.5 & 2.5 & -2.5 & -2.5 & ? \\ 2.5 & ? & ? & -2.5 & ? \\ ? & -2 & -2 & ? & ? \\ -2.25 & -2.25 & 2.75 & 1.75 & ? \\ -1.25 & -1.25 & 3.75 & -1.25 & ? \end{bmatrix}对于用户j，他对电影i的评分就为： y^{(i, j)} = (\theta^{(i)})^T x^{(j)} + \mu_iEve 对电影的评分就为： y^{(i, 5)} = (\theta^{(5)})^T x^{(i)} + \mu_i = \mu_i即，系统在用户未给出评价时，默认该用户对电影的评价与其他用户的平均评价一致。貌似利用均值标准化让用户的初始评价预测客观了些，但这也是盲目的，不准确的。实际环境中，如果一个电影确实没被评价过，那么它没有任何理由被推荐给用户]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福机器学习笔记(Week 10)]]></title>
    <url>%2F2019%2F03%2F05%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Week-10%2F</url>
    <content type="text"><![CDATA[十五、大规模机器学习(Large Scale Machine Learning)15.1 大型数据集的学习如果有一个低方差的模型，增加数据集的规模可以获得更好的结果 应该怎样应对一个有 100 万条记录的训练集？首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许只用 1000个训练集也能获得较好的效果，可以绘制学习曲线 来帮助判断： 15.2 随机梯度下降法批量梯度下降法（Batch gradient descent） \begin{aligned} &\text{重复直到收敛:} \\ & \quad \theta_j = \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}, \quad for\quad j=0,\cdots ,n \end{aligned}每更新一个参数\theta_j都遍历一遍样本集，在m很大时，该算法就显得比较低效。但是，批量梯度下降法能找到全局最优解： 随机梯度下降法（Stochastic gradient descent）针对大数据集，引入了随机梯度下降法，该算法的执行过程为： \begin{aligned} &\text{重复直到收敛:} \\ & \quad for\quad i=1,\cdots,m: \\ & \quad \quad \theta_j = \theta_j - \alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}, \quad for\quad j=1,\cdots,,n \end{aligned}相较于批量梯度下降法，随机梯度下降法每次更新\theta_j只会用当前遍历的样本。虽然外层循环仍需要遍历所有样本，但是，往往在样本尚未遍历完时就已经收敛，因此，面临大数据集时，随机梯度下降法性能卓越 相较于批量梯度下降法，随机梯度下降法的曲线很曲折，倾向于找到局部最优解而不是全局最优解 15.3 小批量梯度下降小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法 每计算b次训练实例，便更新一次参数\theta,假定 b=10,m=1000,小批量梯度下降法的工作过程如下： \begin{aligned} &\text{重复直到收敛:} \\ & \quad for\quad i=1,11,21,\cdots,991: \\ & \quad \quad \theta_j = \theta_j - \alpha\frac{1}{10}\sum_{k=i}^{i+9}(h_\theta(x^{(k)})-y^{(k)})x_j^{(k)}, \quad for\quad j=0,...,n \end{aligned}通常令b在2-100之间。好处在于可以用向量化的方式来循环b个训练实例 15.4 随机梯度下降收敛通常需要绘制调试曲线来监控随机梯度的工作过程是否正确 在随机梯度下降中，每一次更新\theta之前都计算一次代价，然后每X次迭代后，求出这X次对训练实例计算代价的平均值，然后绘制这些平均值与X次迭代的次数之间的函数图表 假定误差定义为: cost(\theta, (x^{(i)}, y^{(i)})) = \frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2则每完成 1000 次迭代，即遍历了 1000 个样本，求取平均误差并进行绘制，得到误差随迭代次数的变化曲线： 遇到下面的曲线并不意味着学习率出了问题，有可能是平均间隔取的太小： 增加X来使得函数更加平缓，也许便能看出下降的趋势： 可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么模型本身可能存在一些错误 如果面临明显上升态势的曲线，就要考虑降低学习率\alpha： 学习率\alpha可以随着迭代次数进行优化: \alpha = \frac{constant1} {iterationNumber + constant2}随着迭代次数的增多，下降步调就会放缓，避免出现抖动： 随机梯度下降法工作前，需要先乱序数据集，使得遍历样本的过程更加分散 随着不断地靠近全局最小值，通过减小学习率，迫使算法收敛而非在最小值附近徘徊。 但是通常不需要这样做便能有非常好的效果了，对\alpha进行调整所耗费的计算通常不值得 15.5 在线学习在线学习算法指的是对数据流而非离线的静态数据集的学习 在线学习的算法与随机梯度下降算法有些类似：对单一的实例进行学习，而非对一个提前定义的训练集进行循环，唯一的区别的是不会使用一个固定的数据集，而是获取一个用户样本，从那个样本中学习，然后丢弃那个样本并继续下去，如果某一种应用有一个连续的数据流，这样的算法可能会非常值得考虑 \begin{aligned} & \text{Repeat forever (as long as the webside is running) \{ } \\ & \quad \text{获得关于该用户的样本} (x,y),\text{使用该样本更新} \theta: \\ & \quad \quad \theta_j= \theta_j - \alpha(h_\theta(x)-y) x_j , \quad \text{for j=0,...,n}\\\} \end{aligned}一旦对一个数据的学习完成了，便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，算法可以很好适应用户的倾向性，针对用户的当前行为不断地更新模型以适应该用户 优点：如果有一个变化的用户群，又或者尝试预测的事情在缓慢变化，就像用户的品味在缓慢变化，在线学习算法可以慢慢地调试学习到的假设，将其调节更新到最新的用户行为 15.6 映射化简和数据并行将数据集分配给多台计算机，让每一台计算机处理数据集的一个子集，然后将计算的结果汇总再求和，这样的方法叫做映射简化 如果任何学习算法能够表达为：对训练集的函数的求和 那么便能将这个任务分配给多台计算机（或者同一台计算机的不同 CPU 核心），以达到加速处理的目的 假定有 400 个训练实例，可以将批量梯度下降的求和任务分配给 4台计算机进行处理： 首先通过 Map （映射）过程来并行计算式中的求和项，每个机器被分配到 100个样本进行计算： \begin{aligned} temp_j^{(1)} &= \sum_{i=1}^{100}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\ temp_j^{(2)} &= \sum_{i=101}^{200}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\ temp_j^{(3)} &= \sum_{i=201}^{300}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\ temp_j^{(4)} &= \sum_{i=301}^{400}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \end{aligned} 通过 Reduce（规约）操作进行求和： \theta_j = \theta_j - \alpha\frac{1}{400}(temp_j^{(1)}+temp_j^{(2)}+temp_j^{(3)}+temp_j^{(4)}) 使用多台机器进行 Map Reduce，此时，Map 任务被分配到多个机器完成： 使用单机多核心进行 Map Reduce，此时，Map 任务被分配到多个 CPU 核心完成： 十六、应用实例：图片 文字识别(Application Example: Photo OCR)16.1 问题描述和流程图图像文字识别应用所作的事是，从一张给定的图片中识别文字 为了完成这样的工作，需要采取如下步骤： 1.文本检测：获得包含了文本的文本框 2.字符分割：从文本框中分割出各个字符 3.字符分类（识别）：字符分割中得到的只是一个个字符图形，在字符分类阶段，才能真正知道该字符类别。 可以用任务流程图来表达这个问题，每一项任务可以由一个单独的小队来负责解决： 16.2 滑动窗口滑动窗口是一项用来从图像中抽取对象的技术 文本检测中的滑动窗口在文本检测阶段，首先定义正、负样本，正样本图像描述了含有文本的图像，负样本描述了不含文本的图像： 通过在原图像沿行、列滑动定义好的窗口，并让窗口内图像与正负样本进行比较： 当窗口遍历过整幅图像后，获得原图像对应的掩膜，高亮度的区域都为疑似文本框的区域： 掩膜中的文本框断断续续的，因此还考虑使用形态学膨胀操作来让文本框更加完整： 字符分割中的滑动窗口在文本检测阶段，滑动窗口是分别沿着行、列进行扫描的，因此是 2 维的扫描过程。而在字符分割过程中，同样将使用到滑动窗口技术，只是这次将窗口的高度设置为与文本框等高，只进行 1维的行扫描： 同样需要定义正负样本，来让窗口知道哪些是字符，哪些包含了字符的分界： 16.3 获取大量数据和人工数据如果模型是低方差的，那么获得更多的数据用于训练模型，是能够有更好的效果的 在字符识别阶段，为了更好的完成分类识别任务，需要给系统提供尽可能多的训练图像，如果手头上拥有的图像不多，就需要人工合成更多的数据 从零开始创造实例例如，收集不同的字体，并为每种字体的每个字符加上随机背景，就可以人工扩展大量的字符图像： 修改已有的数据只要实际数据有可能和经过这样处理后的数据类似，便可以用这样的方法来创造大量的数据 通过扭曲字符形状来合成新数据，这也会帮助机器更好地处理发生过形态变化的图像： 为数据加上随机噪声一般不会提升模型训练质量： 获得更多数据的几种方法： 人工数据合成 手动收集、标记数据 众包 16.4 上限分析：接下来要做的工作是什么上限分析，就是假定某个组件及其前面组件的精度都达到了 100%，即该组件完美地完成了任务，达到了上限，那么此时整个系统的精度能提升多少 假定整个系统的精度是 72%，令文本检测的精度是 100%（比如人工利用 PS 来定位图片中的文本框），此时，整个系统的精度能提升到 89%。即，如果付出足够多的精力来优化文本检测，那么理想情况下，能将系统的精度提升 17%： 组件 流水线精度 精度提升 整个系统 72% — 文本检测 89% 17% 字符分割 90% 1% 字符识别 100% 10% 可以看出，最值得花费精力的步骤是文本检测，最不值得花费精力的是字符分割，即便完成了 100% 的分割，最多也就对系统提升1%]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福机器学习笔记(Week 8)]]></title>
    <url>%2F2019%2F03%2F05%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Week-8%2F</url>
    <content type="text"><![CDATA[十一、聚类(Clustering)11.1 无监督学习：简介在无监督学习中，需要将一系列无标签的训练数据，输入到一个算法中，让计算机学习无标签数据，找到这个数据的内在结构 图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到圈出的这些点集的算法，就被称为聚类算法 非监督学习算法可以找到其他类型的结构或者其他的一些模式，而不只是簇 一些应用： 11.2 K- 均值算法步骤描述K-均值是一个迭代算法，假设想要将数据聚类成 n 个组，其方法为: 首先选择 k 个随机的点，称为 聚类中心（cluster centroids）； 对于数据集中的每一个数据，按照距离 K 个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类 计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置 重复步骤 2-4 直至中心点不再变化 聚类示例： 伪码描述用\mu_1,\mu_2,...,\mu_k \in R^n来表示聚类中心，用c^{(1)},c^{(2)},...,c^{(m)}来存储与第 i 个实例数据最近的聚类中心的索引，K-均值算法的伪代码如下： 分配过程 for i=1 to m： c^{(i)} = \text{距} x^{(i)} \text{最近的聚类中心}距离的计算式如下： \min_k||x^{(i)}-\mu_k||^2移动过程： for k = 1 to K： \mu_k\text{(第} k \text{个聚类中心的新位置)} = \text{第} k \text{簇的平均位置}假设\mu_2聚类中心下分配了4个样本： x^{(1)},x^{(5)},x^{(6)},x^{(10)}亦即： c^{(1)}=c^{(5)}=c^{(6)}=c^{(10)}=2那么\mu_2将会移动到这四个样本的中心位置： \mu_2=\frac{1}{4}(x^{(1)}+x^{(5)}+x^{(6)}+x^{(10)})11.3 优化目标K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和 K-均值的代价函数（又称 畸变函数 Distortion function）为： J(c^{(1)},c^{(2)},...,c^{(m)};\mu_1,\mu_2,...,\mu_k)=\frac{1}{m}\sum_{i=1}^m||x^{(i)}-\mu_{c_{(i)}}||^2 \mu_{c^{(i)}} = \text{样本} x^{(i)} \text{最近的聚类中心点}优化目标便是找出使得代价函数最小的c^{(1)},c^{(2)},...,c^{(m)}和\mu_1,\mu_2,...,\mu_k 最小化代价函数的过程： 样本分配时： 第一个循环是用于减小c^{(i)}引起的代价,固定住了\mu_1,\mu_2,...,\mu_k，而关于c^{(1)},c^{(2)},...,c^{(m)}最小化了J 中心移动时： 第二个循环则是用于减小\mu_i 引起的代价,关于\mu_1,\mu_2,...,\mu_k最小化了J 由于 K-Means 每次迭代过程都在最小化J，所以下面的代价函数变化曲线不会出现： 11.4 随机初始化随机初始化所有的聚类中心点： 选择 K\lt m，即聚类中心点的个数要小于所有训练集实例的数量 随机选择 K 个训练实例，然后令 K 个聚类中心分别与这 K 个训练实例相等 $K-$均值的一个问题在于可能会停留在一个局部最小值处，而这取决于初始化的情况 为了解决这个问题，只能尝试不同的初始化： for i =1 to 100： 随机初始化，执行 K-Means，得到每个所属的簇c^{(i)}，以及各聚类的中心位置\mu： c^{(1)},c^{(2)},...,c^{(m)},\mu_1,\mu_2,...,\mu_k 计算失真函数J： 选择这 100 次中，J最小的作为最终的聚类结果 这种方法在 k 较小的时候(2--10)还是可行的，但是如果 k较大，这么做也可能不会有明显地改善 11.5 选择聚类数没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的 肘部法则 “肘关节”部分对应的K值就是最恰当的K值，但是并不是所有代价函数曲线都存在明显的“肘关节”，例如下面的曲线： 十二、降维(Dimensionality Reduction)12.1 动机一：数据压缩数据压缩不仅允许压缩数据，因而使用较少的计算机内存或磁盘空间，而且也加快学习算法 使用了一条绿色直线，将各个样本投影到该直线，原来二维的特征x = \text{(厘米,英尺)}被降低为了一维x = \text{(直线上的相对位置)}： 将三维特征投影到二维平面，从而将三维特征降到了二维： 12.2 动机二：数据可视化 使用降维的方法将其从50 维降至 2 维，将其可视化： 降维的算法只负责减少维数，新产生的特征的意义必须自己去发现 12.3 主成分分析(PCA)问题主成分分析（Principle Component Analysis）是最常见的降维算法 在 PCA中，要做的是找到一个方向向量（Vector direction），把所有的数据都投射到该向量上时，希望投射平均均方误差能尽可能地小 方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度 主成分分析问题的描述： 问题是要将 n 维数据降至k 维，目标是找到向量u^{(1)}, u^{(2)},...,u^{(k)}使得总的投射误差最小 主成分分析与线性回归主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。主成分分析不作任何预测, 线性回归的目的是预测结果 左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影） PCA 将 n 个特征降维到 k 个，可以用来进行数据压缩，如果 100 维的向量最后可以用 10维来表示，那么压缩率为 90%。 PCA要保证降维后数据的特性损失最小 优点和缺点优点 对数据进行降维。可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息 完全无参数限制。在 PCA 的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的 缺点 如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高 12.4 主成分分析算法算法流程将特征维度从n维降到k维。 PCA 的执行流程如下： 特征标准化，平衡各个特征尺度： x_j=\frac{x_j-\mu_j}{S_j}, \mu_j \text{为特征} j \text{的均值,}S_j \text{为特征} j \text{的标准差} 计算协方差矩阵\Sigma： \Sigma = \frac{1}{m}\sum\limits_{i=1}^{m}(x^{(i)})(x^{(i)})^T = \frac{1}{m} \cdot X^TX 通过奇异值分解（SVD），求取\Sigma的特征向量（eigenvectors）： (U,S,V^T) = SVD(\Sigma) 从U中取出前k个左奇异向量，构成一个约减矩阵U{reduce}： U_{reduce} = (u^{(1)},u^{(2)},\cdots,u^{(k)}) 计算新的特征向量z^{(i)}： z^{(i)}=U_{reduce}^T \cdot x^{(i)}其中 x 是 n\times1 维的，因此结果为 k\times1 维度 12.5 选择主成分的数量使用如下的流程的来评估k值选取优异： 求各样本的投影均方误差: \min\frac{1}{m}\sum\limits_{i=1}^{m}||x^{(i)}-x_{approx}^{(i)}||^2 求数据的总方差： \frac{1}{m}\sum\limits_{i=1}^{m}||x^{(i)}||^2 评估下式是否成立: \frac{\min\frac{1}{m}\sum\limits_{i=1}^{m}||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum\limits_{i=1}^{m}||x^{(i)}||^2} \leq \epsilon$\epsilon$的取值可以为0.01,0.05,0.10,\cdots，假设\epsilon=0.01=1\%，就说“特征间 99%的差异性得到保留” 奇异值分解(SVD)奇异值分解（singular Value Decomposition），简称SVD，线性代数中矩阵分解的方法。假如有一个矩阵A，对它进行奇异值分解，可以得到三个矩阵： A = U\Sigma V^T这三个矩阵的大小： 部分奇异值分解： $r$是一个远小于m,n的数，矩阵的乘法看起来像是下面的样子： 更好的方式来选择 k在 Python 中调用“svd”函数的时候，获得三个参数： 1U, S, VT = np.linalg.svd(A) $S$ 是一个 n\times n 的矩阵，只有对角线上有值 可以使用这个矩阵来计算平均均方误差与训练集方差的比例： \frac{\frac{1}{m}\sum\limits_{i=1}^{m}||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum\limits_{i=1}^{m}||x^{(i)}||^2} =1-\frac{\sum\limits_{i=1}^{k}S_{ii}}{\sum\limits_{i=1}^{n}S_{ii}}\leq 1\%也就是： \frac{\sum\limits_{i=1}^{k}S_{ii}}{\sum\limits_{i=1}^{n}S_{ii}}\geq99\%12.6 重建的压缩表示因为PCA仅保留了特征的主成分，所以PCA是一种有损的压缩方式，假定获得新特征向量为： z^{(i)} = U_{reduce}^Tx^{(i)}还原后的特征x_{approx}为： x_{approx}^{(i)}=U_{reduce}z^{(i)},x_{approx}\approx x 把这个过程称为重建原始数据 12.7 主成分分析法的应用建议不要提前优化由于 PCA 减小了特征维度，因而有可能带来过拟合的问题。 PCA 不是必须的，在机器学习中，一定谨记不要提前优化，只有当算法运行效率不尽如如人意时，再考虑使用 PCA 或者其他特征降维手段来提升训练速度 不只是加速学习降低特征维度不只加速模型的训练速度，还有助于在低维空间分析数据 例如，一个在三维空间完成的聚类问题，可以通过 PCA 将特征降低到二维平面进行可视化分析。 例如下表中有将近几十个特征来描述国家的经济水平，但很难直观的看出各个国家的经济差异： 借助于 PCA，将特征降低到了二维，并在二维空间进行观察，很清楚的就能发现美国和新加坡具有很高的经济水平：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福机器学习笔记(Week 6)]]></title>
    <url>%2F2019%2F03%2F05%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Week-6%2F</url>
    <content type="text"><![CDATA[八、应用机器学习的建议(Advice for Applying Machine Learning)8.1 决定下一步做什么当运用训练好了的模型来预测未知数据的时候发现有较大的误差，下一步可以做什么？ 获得更多的训练实例——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。 尝试减少特征的数量 尝试获得更多的特征 尝试增加多项式特征 尝试减少正则化程度 \lambda 尝试增加正则化程度 \lambda 不应该随机选择上面的某种方法来改进算法，而是运用一些机器学习诊断法来知道上面哪些方法对算法是有效的 8.2 评估一个假设为了检验算法是否过拟合，将数据分成训练集和测试集，通常用 70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常要对数据进行“洗牌”，然后再分成训练集和测试集 引入如下符号： (x^{(1)},y^{(1)})：训练样本 (x_{test}^{(1)},y_{test}^{(1)})：测试样本 m：训练集样本容量 m_{test}：测试集样本容量 测试集评估： 通过训练集让模型学习得出其参数后，对测试集运用该模型，有两种方式计算误差： 1.对于线性回归模型，利用测试集数据计算代价函数 J J_{test} = \frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_{\theta}(x_{test}^{(i)})-y_{test}^{(i)})^22.对于逻辑回归模型，测试集的代价函数为： J_{test}(\theta)=-\frac{1}{m_{test}}\sum\limits_{i=1}^{m_{test}}[y_{test}^{(i)}logh_\theta(x_{test}^{(i)})+(1-y_{test}^{(i)})log(1-h_\theta(x_{test}^{(i)}))]由于逻辑回归处理的是 0/1 分类问题，其预测结果只有正确与错误之分，所以引入误分率（Misclassification Error）,对于每一个测试集实例，计算： err(h_\theta(x),y)= \begin{cases} 1,&\text{if} h_\theta(x) \geq 0.5,y=0 \quad or \quad h_\theta(x) \lt 0.5,y=1 \\ 0, & \text{otherwise} \end{cases}然后对计算结果求平均,则逻辑回归中的测试误差可以表示为： Test_{error} = \frac{1}{m_{test}}\sum\limits_{1}^{m_{test}}err(h_\theta(x_{test}^{(i)}),y_{test}^{(i)})8.3 模型选择和交叉验证集 训练集：60%，确定参数\theta 交叉验证集：20%，进行模型选择 测试集：20%，评价模型预测能力 模型选择的方法为： 使用训练集训练出 10 个模型 用 10 个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值） 选取代价函数值最小的模型 用步骤 3 中选出的模型对测试集计算得出推广误差（代价函数的值） 三者的误差函数如下： 训练集误差： J_{train}(\theta) = \frac{1}{2m}\sum\limits_{i=1}^{m_{train}}(h_\theta(x_{train}^{(i)})-y_{train}^{(i)})^2 交叉验证集误差： J_{cv}(\theta) = \frac{1}{2m}\sum\limits_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2 测试集误差： J_{test}(\theta) = \frac{1}{2m}\sum\limits_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)})-y_{test}^{(i)})^28.4 偏差与方差在机器学习中，偏差（bias）反映了模型无法描述数据规律，而方差（variance）反映了模型对训练集过度敏感，而丢失了数据规律 多项式回归中偏差与方差当运行一个学习算法时，如果这个算法的表现不理想，多半是出现两种情况：要么是偏差比较大，要么是方差比较大。即出现的情况要么是欠拟合，要么是过拟合问题 对于训练集，当d较小时，模型拟合程度低，误差较大；随着d的增长，拟合程度提高，误差减小 对于交叉验证集，当d较小时，模型拟合程度低，误差较大；但是随着d的增长，误差呈现先减小后增大的趋势，转折点是模型开始过拟合训练数据集的时候 训练集误差和交叉验证集误差近似时：偏差/欠拟合 交叉验证集误差远大于训练集误差时：方差/过拟合 8.5 正则化和偏差/方差正规化（Regularization）解决过拟合问题： $\lambda$取值越大，对参数\theta的惩罚力度就越大，能够帮助解决过拟合问题，如果惩罚过重，也会造成欠拟合问题，即会出现高偏差。如果\lambda取值较小，则意味着没有惩罚\theta，也就不能解决过拟合问题，会出现高方差： 选择 \lambda 的方法为： 使用训练集训练出 12 个不同程度正则化的模型 用 12 模型分别对交叉验证集计算得出交叉验证误差 选择得出交叉验证误差最小的模型 运用步骤 3 中选出模型对测试集计算得出推广误差，也可以同时将训练集和交叉验证集模型的代价函 数误差与\lambda的值绘制在一张图表上： 当\lambda较小时，训练集误差较小（过拟合），而交叉验证集误差较大 随着\lambda的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加 8.6 学习曲线学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（m）的函数绘制的图表 当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据 利用学习曲线识别高偏差/欠拟合：用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观： 在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助 利用学习曲线识别高方差/过拟合： 假设使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果 在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果 8.7 决定下一步做什么降低预测误差，即提高预测精度，往往会采用这些手段： 获得更多的训练实例——解决高方差 尝试减少特征的数量——解决高方差 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减小正则化程度 \lambda——解决高偏差 尝试增加正则化程度 \lambda——解决高方差 神经网络的方差和偏差 使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小 使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据 通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好,对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络 九、机器学习系统的设计(Machine Learning System Design)9.1 首先要做什么令向量x表示垃圾邮件的特征向量，该向量包含了 100 个按字母序排序的单词特征，这些单词通常为垃圾邮件常出现的词汇：discount，deal，now 等等： x_j = \begin{cases} 1 & \text{第 j 个单词出现} \\ 0 & \text{未出现} \end{cases}令y标签表示该邮件是否是垃圾邮件： y = \begin{cases} 1 & \text{x是垃圾邮件} \\ 0 & \text{x不是垃圾邮件} \end{cases}那么垃圾邮件分类就是一个 0/1 分类问题，可以用逻辑回归完成 如何降低分类错误率： 收集更多的数据，有更多的垃圾邮件和非垃圾邮件的样本 基于邮件的路由信息开发一系列复杂的特征 基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理 为探测刻意的拼写错误（把 watch 写成 w4tch）开发复杂的算法 9.2 误差分析构建一个学习算法的推荐方法为： 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法 绘制学习曲线，确定面临的问题是高偏差还是高方差，决定是添加更多特征，或者增加更多数据，还是其他选择 进行误差分析：人工检查交叉验证集算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势 假定交叉验证集有 500 个样本，即m_{cv}=500，模型错分了其中 100 个样本，通过下述手段进行错误分析： 需要知道哪些邮件被错分了，是假冒伪劣的推销邮件？医药邮件？还是钓鱼邮件？ 需要知道提供什么线索（特征）能帮助模型区分出这些邮件？ 例如，在这 100 个错分样本中，发现有 53 个样本是钓鱼邮件，就需要考虑为模型注入识别的钓鱼邮件的能力。在这 53 封钓鱼邮件中，故意使用错误拼写的邮件有 5 封，来源可疑（发送人可疑）的邮件有16 封，使用了大量煽动性标点符号的邮件有 32 封 对于识别钓鱼邮件来说，更适合将煽动性标点符号添加为特征，而不用再考虑去识别错误拼写 9.3 查准率（Precision）和 召回率（Recall）将算法预测的结果分成四种情况： 正确肯定（True Positive,TP）：预测为真，实际为真 正确否定（True Negative,TN）：预测为假，实际为假 错误肯定（False Positive,FP）：预测为真，实际为假 错误否定（False Negative,FN）：预测为假，实际为真 查准率（Precision）： Precision = \frac{TruePos}{PredicatedPos} = \frac{TruePos}{TruePos+FalsePos}例，在所有预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好 召回率（Recall）： Recall = \frac{TruePos}{ActualPos} = \frac{TruePos}{TruePos+FalseNeg}例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好 实际1 实际0 预测1 True Positive False Positive 预测0 False Negative True Negative 9.4 查准率和召回率之间的权衡如果希望在非常确信的情况下预测为真（肿瘤为恶性），即希望更高的查准率，可以使用比 0.5 更大的阀值，如 0.7，0.9。这样会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。如果希望提高召回率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，可以使用比 0.5 更小的阀值，如 0.3。 选择阀值的方法：计算 F 1 值（F1 Score） F_1 Score = 2\frac{PR}{P+R}分子是查准率和召回率的乘积，只有二者都较高时，F_1Score才会较高，特别地： \begin{aligned} F_1 Score &= 0, \quad \text{if P=0 or R=0} \\ F_1 Score &= 1, \quad \text{if P=1 and R=1} \end{aligned}机器学习的数据 什么时候采用大规模的数据集： 一定要保证模型拥有足够的参数（线索），对于线性回归/逻辑回归来说，就是具备足够多的特征，而对于神经网络来说，就是更多的隐藏单元。这样，足够多的特征避免了高偏差（欠拟合）问题，而足够大数据集避免了多特征容易引起的高方差（过拟合）问题。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福机器学习笔记(Week 3)]]></title>
    <url>%2F2019%2F03%2F04%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Week-3%2F</url>
    <content type="text"><![CDATA[四 、逻辑回归( Logistic Regression）4.1 分类问题将因变量(dependant variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量y∈{0,1} 其中 0 表示负向类，1 表示正向类 逻辑回归算法是分类算法，性质是：它的输出值永远在 0 到 1 之间 4.2 假说表示逻辑回归模型的假设是： h_\theta(x)=g(\theta^TX)其中： X代表特征向量 g代表逻辑函数，常用的逻辑函数为S形函数（Sigmoid函数） 公式为： g(z)=\frac{1}{1+e^{-z}}123import numpy as np def sigmoid(z): return 1 / (1 + np.exp(-z)) 逻辑回归模型的假设： h_\theta(x)=\frac{1}{1+e^{-\theta^TX}}$h_\theta(x)$的作用是，对于给定的输入变量，根据选择的参数计算输出变量 = 1的可能性（估计的概率），即： h_\theta(x)=P(y=1|x;\theta)例如，如果对于给定的x，通过已经确定的参数计算得出h_\theta(x)= 0.7，则表示有70％的几率y为正向类，相应地y为负向类的几率为0.3 4.3 决策边界参数\theta是向量[-3 1 1]。当-3+x_1+x_2大于等于0，即x_1+x_2大于等于3时，模型将预测y = 1绘制直线x_1+x_2= 3，这条线便是模型的分界线，称为决策边界，将预测为1的区域和预测为0的区域分隔开 需要用曲线才能分隔y = 0的区域和y = 1的区域： 假设参数：$h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_2^2+\theta_4x_2^2)$是[-1 0 0 1 1]，则得到的判定边界恰好是圆点在原点且半径为1的圆形 可以用非常复杂的模型来适应非常复杂形状的决策边界 决策边界不是训练集的属性，而是假设本身及其参数的属性 4.4 代价函数线性回归模型，定义的代价函数是所有模型误差的平方和 当将h_\theta(x)\frac{1}{1+e^{-\theta^Tx}}代入到这样定义了的代价函数中时，得到的代价函数将是一个非凸函数（non-convex function） 代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值 重新定义逻辑回归的代价函数为： J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})其中 Cost(h_\theta(x),y) =\begin{cases} -log(h_\theta(x)) &\text{if } \quad y=1\\ -log(1-h_\theta(x)) &\text{if } \quad y=0 \end{cases}.当实际的 y=1 且h_\theta(x)也为 1 时误差为0，当y=1 但h_\theta(x)不为 1 时误差随着h_\theta(x)的变小而变大 当实际的 y=0 且h_\theta(x)也为0 时代价为 0，当 y=0 但h_\theta(x)不为 0 时误差随着h_\theta(x)的变大而变大 简化如下： Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))代入代价函数得到： J(\theta)=\frac{1}{m}\sum_{i=1}^{m}[-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})*log(1-h_\theta(x^{(i)}))]即： J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]123456def cost(theta,X,y): theta=np.matrix(theta) y=np.matrix(y) first=np.multiply(-y,np.log(sigmoid(X*theta.T))) second=np.multiply((1-y),np.log(1-sigmoid(X*theta.T))) return np.sum(first-second)/(len(X)) 用梯度下降算法来求得能使代价函数最小的参数: \begin{aligned} Repeat&\quad \{\\ \theta_j&=\theta_j-\alpha\frac{\partial }{\partial \theta_j}J(\theta)\\ (&simultaneously\quad update\quad all\quad \theta_j)\\\} \end{aligned}求导后得到： \begin{aligned} Repeat&\quad \{\\ \theta_j&=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\\ (&simultaneously\quad update\quad all\quad \theta_j)\\\} \end{aligned}代价函数J(\theta)会是一个凸函数，并且没有局部最优值 推导过程： J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))] 注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的h_\theta(x)=g(\theta^TX)与线性回归中不同，所以实际上是不一样的。在运行梯度下降算法之前，进行特征缩放依旧是非常必要的 4.5 简化的成本函数和梯度下降逻辑回归的代价函数： \begin{aligned} Cost(h_\theta(x),y)=&-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x)) \\ =&-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(h_\theta(x))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))] \end{aligned}最小化代价函数的方法，是使用梯度下降法(gradient descent) 代价函数： J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]要最小化这个关于\theta的函数值，这是通常用的梯度下降法的模板 求导后得到： 线性回归假设函数： h_\theta(x)=\theta^TX=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n逻辑函数假设函数： h_\theta(x)=\frac{1}{1+e^{-\theta^TX}}即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西 如果特征范围差距很大的话，应用特征缩放的方法，让逻辑回归中的梯度下降收敛更快 4.6 高级优化 更高级的优化算法:共轭梯度法 BFGS (变尺度法) 和 L-BFGS (限制变尺度法) 优点： 不需要手动选择学习率 \alpha 算法有一个智能的内部循环,称为线性搜索(line search)算法，可以自动尝试不同的学习速率 \alpha，并自动选择一个好的学习速率 \alpha，甚至可以为每次迭代选择不同的学习速率 4.7 多类别分类：一对 多多类分类问题，数据集看起来像这样： 用三种不同的符号来代表三个类别 用三角形表示 y=1，方框表示 y=2，叉叉表示 y=3 下面要做的就是使用一个训练集，将其分成三个二元分类问题 创建一个新的”伪”训练集，类型 2 和类型 3 定为负类，类型 1 设定为正类 三角形是正样本，圆形代表负样本，设置三角形的值为 1，圆形的值为 0 要拟合出一个合适的分类器 训练一个标准的逻辑回归分类器： 将多个类中的一个类标记为正向类（y=1），然后将其他所有类都标记为负向类，这个模型记作h_\theta^{(1)}(x)，选择另一个类标记为正向类（y=2），再将其它类都标记为负向类，将这个模型记作h_\theta^{(2)}(x),依此类推 最后得到一系列的模型简记为： h_\theta^{(i)}(x)=p(y=i|x;\theta),i=(1,2,3,...k) 最后，在需要做预测时，将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量 现在要做的就是训练这个逻辑回归分类器：h_\theta^{(i)}(x)​，其中 i 对应每一个可能的 y=i，最后，为了做出预测，输入一个新的 x 值，用这个做预测，要做的就是在三个分类器里面输入 x，然后选择一个让h_\theta^{(i)}(x)​最大的 i，即\underset{\ \ \ \ i}{max \ h^{(i)}(x)}​ 五 、正则化( Regularization）5.1 过拟合的问题回归问题的例子： 第一个模型是一个线性模型，欠拟合或高偏离，不能很好地适应训练集，第三个模型是一个四次方的模型，过拟合或高方差，过于强调拟合原始数据，而丢失了算法的本质：预测新数据 分类问题中也存在这样的问题： 以多项式理解，x 的次数越高，拟合的越好，但相应的预测的能力就可能变差 发现了过拟合问题，应该如何处理？ 丢弃一些不能帮助正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如 PCA） 正则化。 保留所有的特征，但是减少参数的大小（magnitude） 5.2 代价函数上面的回归问题中如果模型是： h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\theta_4x_4高次项导致了过拟合的产生，如果能让这些高次项的系数接近于 0 的话，就能很好的拟合 要做的就是在一定程度上减小这些参数\theta的值，这就是正则化的基本方法 在\theta_3,\theta_4设置一点惩罚。尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小的一些\theta_3,\theta_4 修改后的代价函数如下： min_\theta\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+1000\theta_3^2+10000\theta_4^2]假如有非常多的特征，但并不知道哪些特征要惩罚，对所有的特征进行惩罚，让代价函数最优化的软件来选择这些惩罚的程度。 这样的结果是得到了一个较为简单的能防止过拟合问题的假设： J(\theta)=\frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2]即为正则化线性回归的代价函数 λ 又称为正则化参数（Regularization Parameter），右边的项称为正则化项 注：根据惯例，不对\theta_0 进行惩罚 经过正则化处理的模型与原模型的可能对比如下图所示： 如果选择的正则化参数\lambda 过大，则会把所有的参数都最小化了，导致模型变成h_\theta(x)=\theta_0，也就是上图中红色直线所示的情况，造成欠拟合 为什么增加的一项\lambda \sum_{j=1}^{n}\theta_j可以使\theta的值减小： 如果令\lambda的值很大的话，为了使 Cost Function 尽可能的小，所有的\theta值（不包括\theta_0）都会在一定程度上减小 若\lambda的值太大了，那么\theta（不包括\theta_0 ）都会趋近于 0，所得到的只能是一条平行于 x 轴的直线。 对于正则化，要取一个合理的\lambda的值，才能更好的应用正则化\theta 5.3 正则化线性回归正则化线性回归的代价函数为： J(\theta)=\frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2]如果使用梯度下降法令这个代价函数最小化，因为未对\theta_0​进行正则化，所以梯度下降算法将分两种情形： \begin{aligned} Repeat \quad until \quad converagence\{\\ \theta_0&=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^{(i)})-y^{(i)}]x_0^{(i)}\\ \theta_j&=\theta_j-\alpha\frac{1}{m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\theta_j]\\\} \end{aligned}对上面的算法中 j=1,2,…,n 时的更新式子进行调整可得： \theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)}))x_j^{(i)}可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令\theta值减少了一个额外的值 5.4 正则化的逻辑回归模型 给代价函数增加一个正则化的表达式，得到代价函数： J(\theta)=\frac{1}{m}\sum_{i=1}^{m}[-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2123456import numpy as npdef costReg(theta, X, y, learningRate): first = np.multiply(-y, np.log(sigmoid(np.dot(X, theta.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(np.dot(X, theta.T))) reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2)) return np.sum(first - second) / (len(X)) + reg 最小化该代价函数，通过求导，得出梯度下降算法为： \begin{aligned} Repeat \quad until \quad converagence\{\\ \theta_0&=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^{(i)})-y^{(i)}]x_0^{(i)}\\ \theta_j&=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]\\ for& \quad j=1,2,...n\\\} \end{aligned}注意： 虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者h_\theta(x)的不同,所以还是有很大差别 \theta_0不参与其中的任何一个正则化]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福机器学习笔记(Week 5)]]></title>
    <url>%2F2019%2F03%2F04%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Week-5%2F</url>
    <content type="text"><![CDATA[七、神经网络的学习( Neural Networks: Learning)7.1 代价函数假设神经网络的训练样本有 m个，每个包含一组输入 x和一组输出信号 y，L 表示神经网络结构总层数，S_l表示第l层的单元个数，S_L代表最后一层中处理单元的个数 将神经网络的分类定义为两种情况： 二类分类：S_L = 1, y=0\quad or \quad1表示哪一类； K 类分类：S_L = K,y_i=1表示分到第 i 类（K &gt; 2） 逻辑回归问题中代价函数为： J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2在逻辑回归中，只有一个输出变量，又称标量（scalar），也只有一个因变量 y，但是在神经网络中，可以有很多输出变量，h_\theta(x)是一个维度为 K 的向量,训练集中的因变量也是同样维度的一个向量，因此代价函数会比逻辑回归更加复杂一些: h_\theta(x)\in \mathbb{R}^K,\quad(h_\theta(x))_i = i^{th}\quad output J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{S_l} \sum_{j=1}^{S_{l+1}} ( \Theta_{j,i}^{(l)})^2对于每一行特征，都会给出k个预测，基本上可以利用循环，对每一行特征都预测 K个不同结果，然后利用循环在 K个预测中选择可能性最高的一个，将其与y中的实际数据进行比较 正则化的那一项只是排除了每一层\theta_0后，每一层的\theta 矩阵的和。最里层的循环j 循环所有的行（由S_{l+1}层的激活单元数决定），循环i 则循环所有的列，由该层(S_l层）的激活单元数所决定。即： $h_\theta(x)$与真实值之间的距离为每个样本—每个类输出的加和，对参数进行regularization 的 bias 项处理所有参数的平方和 7.2 反向传播算法为了计算代价函数的偏导数\frac{\partial }{\partial \Theta_{ij}^{(l)}}J(\Theta),需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层 假设训练集只有一个实例(x^{(1)} , y^{(1)}),神经网络是一个四层的神经网络，其中K = 4,S_L = 4,L =4 前向传播算法： \begin{aligned} a^{(1)} &= x\\\\ z^{(2)} &= \Theta^{(1)}a^{(1)} \\\\ a^{(2)} &= g(z^{(2)})\quad(add\quad a_0^{(2)}) \\\\ z^{(3)} &= \Theta^{(2)}a^{(2)} \\\\ a^{(3)} &= g(z^{(3)}) \quad(add\quad a_0^{(3)}) \\\\ z^{(4)} &= \Theta^{(3)}a^{(3)} \\\\ a^{(4)} &= h_\Theta(x) = g(z^{(4)}) \end{aligned} 从最后一层的误差开始计算，误差是激活单元的预测(a_j^{(4)})与实际值(y_j)之间的误差，（j=1:K） 用\delta来表示误差，则： \delta^{(4)} = a^{(4)}- y各层的预测误差为向量 \delta^{(l)} = \begin{cases} a^{(l)} - y & \text{l=L}\\ (\Theta^{(l)})^T\delta^{(l+1)} .*g'(z^{(l)}) & \text{l=2,3,...,L-1} \end{cases} g'(z^{(l)}) = a^{(l)} .* (1-a^{(l)})$(\Theta^{(l)})^T\delta^{(l+1)}$是权重导致的误差的和 利用这个误差值来计算前一层的误差： \delta^{(3)} = (\Theta^{(3)})^T\delta^{(4)}.*g'(z^{(3)}) \delta^{(2)} = (\Theta^{(2)})^T\delta^{(3)}.*g'(z^{(2)})第一层是输入变量，不存在误差 计算代价函数的偏导数: 假设\lambda=0，不做任何正则化处理时有： \frac{\partial }{\partial \Theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)}\delta_i^{(l+1)}$l$代表目前所计算的是第几层 $j$ 代表目前计算层中的激活单元的下标，也将是下一层的第j个输入变量的下标 $i$ 代表下一层中误差单元的下标，是受到权重矩阵中第i行影响的下一层中的误差单元的下标 如果考虑正则化处理，并且训练集是一个特征矩阵而非向量。在上面的特殊情况中，需要计算每一层的误差单元来计算代价函数的偏导数。在更为一般的情况中，同样需要计算每一层的误差单元，但是需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，用\vartriangle_{ij}^{(l)} 来表示这个误差矩阵。第l层的第i个激活单元受到第j个参数影响而导致的误差 算法表示为： 假定有训练集{(x^{(1)}, y^{(1)}),...,(x^{(m)},y^{(m)})}，使用了反向传播的神经网络训练过程如下： 1.for all l,i,j，初始化权值梯度\Delta^{(l)}: \Delta_{ij}^{(l)} = 02. \begin{aligned} &for\quad i =1\quad to\quad m\\ \{\\ &\text{令}a^{(1)} = x^{i} \\ & \text{执行前向传播算法,计算各层的激活向量:}a^{(l)}\quad l =1,2,3,...L \\ & \text{计算输出层的误差向量:}\delta^{(L)} = a^{(L)} - y^{(i)} \\ & \text{反向依次计算其他层误差向量:}\delta^{(L-1)},\delta^{(L-2)},...,\delta^{(2)} \text{求} \Delta_{ij}^{(l) := \Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}} \\ \} \end{aligned}即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差 3.求出了各层权值的更新增量\vartriangle_{ij}^{(l)}之后，便可以计算代价函数的偏导数: D^{(l)}_{i,j} = \begin{cases} \dfrac{1}{m}\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j},&\text{if } j \neq 0 \\ \frac{1}{m}\Delta_{ij}^{(l)}& \text{if }j = 0 \end{cases}4.更新各层的权值矩阵\Theta^{(l)}，其中\alpha为学习率： \Theta^{(l)} = \Theta^{(l)} + \alpha D^{(l)}7.3 反向传播算法的直观理解前向传播算法： 反向传播算法做的是： 7.4 实现注意：展开参数 代价函数只支持传递向量作为参数，需要先将矩阵元素平铺开为一个长向量： 1234import numpy as npthetaVector = np.r_[Theta1.reshape(-1,1), Theta2.reshape(-1,1), Theta3.reshape(-1,1)]deltaVector = np.r_[ D1.reshape(-1,1), D2.reshape(-1,1), D3.reshape(-1,1) ] 还原： 12345import numpy as np# ...Theta1 = thetaVector[0:110].reshape(10,11)Theta2 = thetaVector[110:220].reshape(10,11)Theta3 = thetaVector[220:231].reshape(1,11) 7.5 梯度检验对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的\theta ，计算出在\theta-\varepsilon处和\theta+\varepsilon的代价值（\varepsilon是一个非常小的值，通常选取0.001），然后求两个代价的平均，用以估计在 \theta处的代价值 斜边的斜率可以近似等于蓝色线段的斜率，可以通过求取红色斜边的斜率来近似\frac{d}{d\Theta}J(\Theta): \frac{d}{d\Theta}J(\Theta) \approx \frac{J(\Theta+\epsilon)-J(\Theta-\epsilon)}{2\epsilon}当\theta是一个向量时，则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验 包含有梯度校验的BP算法如下： 1.首先由反向传播算法获得展开的DVec: DVec = [D^{(1)},D^{(2)},D^{(3)},...D^{(n)}]2计算梯度近似gradApprox,对\theta_j进行检验,\theta_j是 \Theta_j 的展开: \begin{aligned} & \dfrac{\partial}{\partial\theta_j}J(\theta) \approx \dfrac{J(\theta_1, ..., \theta_j + \epsilon, ..., \theta_n) - J(\theta_1, ..., \theta_j - \epsilon, ..., \theta_n)}{2\epsilon},\text{for j=1 to n} \\ & gradApprox = [\dfrac{\partial}{\partial\theta_1}J(\theta), \dfrac{\partial}{\partial\theta_2}J(\theta), ..., \dfrac{\partial}{\partial\theta_n}J(\theta)] \end{aligned}3.比较gradApprox与DVec的相似程度（比如可以用欧氏距离）： gradApprox \approx DVec如果上式成立，则证明网络中BP算法有效，此时关闭梯度校验算法（因为梯度的近似计算效率很慢），继续网络的训练过程。 根据上面的算法，计算出的偏导数存储在矩阵D_{ij}^{(l)}中。检验时，要将该矩阵展开成为向量，同时也将\theta矩阵展开为向量，针对每一个\theta都计算一个近似的梯度值，将这些值存储于一个近似梯度矩阵中，最终将得出的这个矩阵同D_{ij}^{(l)}进行比较 7.6 随机初始化0 值初始化在逻辑回归中，通常会初始化所有权值为0，假如在如下的神经网络也采用0 值初始化： 则可以得到： \begin{aligned} & a_1^{(2)} = a_2^{(2)} \\ & \text{则} \delta_1^{(2)} = \delta_2^{(2)} \\ & \text{则} \frac{\partial}{\partial \Theta_{01}^{(1)}}J(\Theta) = \frac{\partial}{\partial \Theta_{02}^{(1)}}J(\Theta) \\ & \text{则更新后的权值:}\Theta_{01}^{(1)} = \Theta_{02}^{(1)} \end{aligned}每次迭代，所有权值的数值都一样，意味着隐含层的神经元激活值也将一样无论隐含层层数有多少，各层的神经元有多少，由于各层的神经元激活值大小一样，也就相当于各层只有一个有效神经元（特征），这就失去了神经网络进行特征扩展和优化的本意 随机初始化固定值初始化将会使神经网络丧失其特性，因此，对于各层的权值矩阵，采用随机初始化策略。 随机值产生的区间定义为[-\varepsilon ,+\varepsilon ]，并假定： \Theta^{(1)} \in R^{10 \times 11}, \Theta^{(2)} \in R^{1 \times 11}1234import numpy as np# ...Theta1 = np.random.rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILONTheta2 = np.random.rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON 7.7 综合起来使用神经网络时的步骤： 网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。 第一层的单元数即训练集的特征数量。最后一层的单元数是训练集的结果的类的数量。 如果隐藏层数大于 1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好 真正要决定的是隐藏层的层数和每个中间层的单元数 训练神经网络： 参数的随机初始化 利用正向传播方法计算所有的 h_\theta(x) 编写计算代价函数 J 的代码 利用反向传播方法计算所有偏导数 利用数值检验方法检验这些偏导数 使用优化算法来最小化代价函数]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福机器学习笔记(Week 4)]]></title>
    <url>%2F2019%2F03%2F04%2F%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Week-4%2F</url>
    <content type="text"><![CDATA[六、神经网络：表述( Neural Networks: Representation)6.1 非线性假设无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大 训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车），一种方法是用很多汽车的图片和很多非汽车的图片，用这些图片上一个个像素的值（饱和度或亮度）来作为特征 假如只选用灰度图片，每个像素则只有一个值（而非 RGB 值），可以选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车： 假使采用的都是 50x50 像素的小图片，并且将所有的像素视为特征，则会有2500 个特征，如果要进一步将两两特征组合构成一个多项式模型，则会有约2500 ²/2个（接近 3 百万个）特征。 普通的逻辑回归模型，不能有效地处理这么多的特征，这时候需要神经网络 6.2 模型表示神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。在神经网络中，参数又被称为权重（weight） 下图是一个以逻辑回归模型作为自身学习模型的神经元示例： $x_1,x_2,x_3$是输入单元（input units），将原始数据输入给它们 $a_1,a_2,a_3$是中间单元，负责将数据进行处理，然后呈递到下一层，最后是输出单元，负责计算h_\theta(x) 神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量 下图为一个 3 层的神经网络，第一层是输入层（Input Layer），最后一层是输出层（Output Layer），中间一层是隐藏层（Hidden Layers）。 为每一层都增加一个偏差单位（bias unit）： $a_i^{(j)}$ 代表第j层的第i个激活单元。 $\theta^{(j)}$代表从第j层映射到第j+1层时的权重的矩阵。 其尺寸为：以第j+1层的激活单元数量为行数，以第j层的激活单元数加一为列数的矩阵。 上图所示的模型，激活单元和输出分别表达为： \begin{aligned} &a_1^{(2)} = g(\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 + \theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3)\\\quad \\ &a_2^{(2)} = g(\theta_{20}^{(1)}x_0 + \theta_{21}^{(1)}x_1 + \theta_{22}^{(1)}x_2 + \theta_{23}^{(1)}x_3)\\\quad \\&a_3^{(2)} = g(\theta_{30}^{(1)}x_0 + \theta_{31}^{(1)}x_1 + \theta_{32}^{(1)}x_2 + \theta_{33}^{(1)}x_3)\\\quad \\ &h_\theta(x) = g(\theta_{10}^{(2)}a_0^{(2)} + \theta_{11}^{(2)}a_1^{(2)} + \theta_{12}^{(2)}a_2^{(2)} + \theta_{13}^{(2)}a_3^{(2)}) \end{aligned}每一个a都是由上一层所有的x和每一个x所对应的\theta决定的 这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION ) $x,\theta,a$分别用矩阵表示： X = \begin{bmatrix} x_0\\ x_1\\ x_2\\ x_3 \end{bmatrix} , \theta = \begin{bmatrix} \theta_{10}&...&...&...\\ ...&...&...&...\\ ...&...&...&\theta_{33} \end{bmatrix} , a = \begin{bmatrix} a_0\\ a_1\\ a_2\\ a_3 \end{bmatrix}可以得到 \theta\cdot X = a 计算第二层的值： x = \begin{bmatrix} x_0\\ x_1\\ x_2\\ x_3 \end{bmatrix}\quad\quad z^{(2)} = \begin{bmatrix} z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)} \end{bmatrix} g\begin{pmatrix}\begin{bmatrix} \theta_{10}^{(1)} & \theta_{11}^{(1)} & \theta_{12}^{(1)}&\theta_{13}^{(1)} \\ \theta_{20}^{(1)}&\theta_{21}^{(1)}&\theta_{22}^{(1)}&\theta_{23}^{(1)} \\ \theta_{30}^{(1)} &\theta_{31}^{(1)} &\theta_{32}^{(1)} &\theta_{33}^{(1)} \end{bmatrix}\times\begin{bmatrix} x_0 \\ x_1 \\ x_2 \\ x_3 \end{bmatrix}\end{pmatrix} = g\begin{pmatrix}\begin{bmatrix} \theta_{10}^{(1)}x_0+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3 \\ \theta_{20}^{(1)}x_0+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3 \\ \theta_{30}^{(1)}x_0 +\theta_{31}^{(1)}x_1 +\theta_{32}^{(1)}x_2 +\theta_{33}^{(1)}x_3 \end{bmatrix}\end{pmatrix} = \begin{bmatrix} a_1^{(2)} \\ a_2^{(2)} \\ a_3^{(2)} \end{bmatrix}令z^{(2)} = \theta^{(1)}x, 则 a^{(2)} = g(z^{(2)}), 计算后添加 a_0^{(2)} = 1 计算输出的值为： g\begin{pmatrix}\begin{bmatrix} \theta_{10}^{(2)} &\theta_{11}^{(2)} &\theta_{12}^{(2)} &\theta_{13}^{(2)} \end{bmatrix}\times\begin{bmatrix} a_0^{(2)} \\ a_1^{(2)} \\ a_2^{(2)} \\ a_3^{(2)} \end{bmatrix}\end{pmatrix} = g\begin{pmatrix} \theta_{10}^{(2)}a_0^{(2)} +\theta_{11}^{(2)}a_1^{(2)} +\theta_{12}^{(2)}a_2^{(2)} +\theta_{13}^{(2)}a_3^{(2)} \end{pmatrix} = h_\theta(x)令 z^{(3)} = \theta^{(2)}a^{(2)}, 则 h_\theta(x) =a^{(3)} = g(z^{(3)}) 如果要对整个训练集进行计算，需要将训练集特征矩阵进行转置，使得同一个实例的特征都在同一列里。 即： z^{(2)} = \theta^{(1)}\times X^T a^{(2)} = g(z^{(2)})把左半部分遮住： 右半部分其实就是以a_0,a_1,a_2,a_3, 按照 Logistic Regression 的方式输出h_\theta(x): h_\theta(x) = g\begin{pmatrix} \theta_{10}^{(2)}a_0^{(2)} + \theta_{11}^{(2)}a_1^{(2)}+ \theta_{12}^{(2)}a_2^{(2)} + \theta_{13}^{(2)}a_3^{(2)} \end{pmatrix}神经网络就像是 logistic regression，只不过把 logistic regression 中的输入向量[x_1 \sim x_3]变成了中间 层的[a_1^{(2)} \sim a_3^{(2)}], 即: h_\theta(x) = g\begin{pmatrix} \theta_{0}^{(2)}a_0^{(2)} + \theta_{1}^{(2)}a_1^{(2)}+ \theta_{2}^{(2)}a_2^{(2)} + \theta_{3}^{(2)}a_3^{(2)} \end{pmatrix}把a_0,a_1,a_2,a_3看成更为高级的特征值，也就是x_0,x_1,x_2,x_3的进化体，并且它们是由x与\theta决定的，因为是梯度下降的，所以a是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将x次方厉害，也能更好的预测新数据。这就是神经网络相比于逻辑回归和线性回归的优势 6.3 样本和直观理解I神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑AND、逻辑或 OR 逻辑与 AND:下图中左半部分是神经网络的设计与 output 层表达式，右边上部分是 sigmod 函数，下半部分是真值表 其中\theta_0 = -30, \theta_1 =20, \theta_2 = 20 输出函数h_\theta(x): h_\theta(x) = g(-30 + 20x_1 + 20x_2)g(x)$$的图像是： ![](https://baozou.gitbooks.io/-stanford-machine-learning/content/assets/92import.png) ![](https://baozou.gitbooks.io/-stanford-machine-learning/content/assets/93import.png) 所以：$$h_\theta(x) \approx x_1\quad AND \quad x_2OR 函数： OR 与 AND 整体一样，区别只在于的取值不同 II二元逻辑运算符（BINARY LOGICAL OPERATORS）当输入特征为布尔值（0 或 1）时，可以用一个单一的激活层作为二元逻辑运算符，为了表示不同的运算符，需要选择不同的权重 下图的神经元（两个权重分别为 10，-20）可以被视为作用等同于逻辑非（NOT）： 利用神经元来组合成更为复杂的神经网络以实现更复杂的运算 XNOR 功能（输入的两个值必须一样，均为 1 或均为 0），即: XNOR = (x_1 \quad AND\quad x_2)\quad OR \quad((NOTx_1) \quad AND\quad (NOTx_2))表达 (NOTx_1) \quad AND\quad (NOTx_2)部分的神经元: 将表示 AND 的神经元和表示 (NOTx_1) \quad AND\quad (NOTx_2)的神经元以及表示OR的神经元进行组合： 得到了一个能实现 XNOR 运算符功能的神经网络 这种方法可以逐渐构造出越来越复杂的函数，也能得到更加厉害的特征值 6.4 多类分类输入向量 x 有三个维度，两个中间层，输出层 4 个神经元分别用来表示 4 类，也就是每一个数据在输出层都会出现[a\quad b\quad c\quad d]^T,且 a,b,c,d 中仅有一个为 1，表示当前类 该神经网络的可能结构示例： 神经网络算法的输出结果为四种可能情形之一： \begin{bmatrix} 1\\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 0\\ 1 \\ 0 \\ 0 \end{bmatrix}\begin{bmatrix} 0\\ 0 \\ 1 \\ 0 \end{bmatrix},\begin{bmatrix} 0\\ 0 \\ 0 \\ 1 \end{bmatrix}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福机器学习笔记(Week 1)]]></title>
    <url>%2F2019%2F03%2F04%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Week-1%2F</url>
    <content type="text"><![CDATA[一 、引言1.1 Supervised Learning&amp;Uupervised Learning监督学习监督学习，其基本思想是我们数据集中的每个样本都有相应的“正确答案”，再根据这些样本作出预测回归（regression）问题，即通过回归来推出一个连续的输出分类（classification）问题，其目标是推出一组离散的结果 垃圾邮件问题。如果有标记好的数据，区别好是垃圾还是非垃圾邮件，把这个当作监督学习问题 无监督学习针对数据集，无监督学习就能判断出数据有两个不同的聚集簇，叫做聚类算法无监督学习，是学习策略，交给算法大量的数据，让算法从数据中找出某种结构 细分市场，可以当作无监督学习，因为只是拿到算法数据，再让算法去自动地发现细分市场 二 、单变量线性回归 ( Linear Regression with One Variable)2.1 模型表示在监督学习中我们有一个数据集，这个数据集被称训练集 将训练集“喂”给学习算法，进而学习得到一个假设h，然后将要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果一种可能的表达方式为：h_\theta(x)=\theta_0+\theta_1x，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题 2.2 代价函数模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差 目标便是选择出可以使得建模误差的平方和能够最小的模型参数。即使得代价函数J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2最小 可以看出在三维空间中存在一个使得J(\theta_0,\theta_1)最小的点 代价函数也被称作平方误差函数，有时也被称为平方误差代价函数 2.3 代价函数的直观理解 \begin{aligned} Hypothesis:\\h_\theta(x)&=\theta_0+\theta_1x\\Parameters:\\\theta_0,\theta_1\\Cost\quad Function:\\J(\theta_0,\theta_1)&=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2\\Goal:\quad minimize\quad J(\theta_0,\theta_1) \end{aligned} 2.4 梯度下降梯度下降背后的思想是：开始时随机选择一个参数的组合(\theta_0,\theta_1,......\theta_n)​，计算代价函数，然后寻找下一个能让代价函数值下降最多的参数组合。直到到一个局部最小值（local minimum），因为并没有尝试完所有的参数组合，所以不能确定得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值 批量梯度下降（batch gradient descent）算法的公式为： \begin{aligned} Repeat \quad until \quad converagence\{\\&\theta_j:=\theta_j-\alpha\frac{\partial }{\partial \theta_j}J(\theta_0,\theta_1)\quad \\&(for\quad j=0 \quad and \quad j=1 )\\\} \end{aligned}其中\alpha是学习率（learning rate），它决定了沿着能让代价函数下降程度最大的方向迈出的步子有多大，在批量梯度下降中，每一次都同时让所有的参数减去学习速率乘以代价函数的导数 如果\alpha太小的话，需要很多步才能到达全局最低点 如果\alpha太大，它会导致无法收敛，甚至发散 2.5 梯度下降的线性回归梯度下降算法和线性回归算法比较如图： \frac{\partial }{\partial \theta_j}J(\theta_0,\theta_1)=\frac{\partial }{\partial \theta_j}\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2j=0 时： \frac{\partial }{\partial \theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})j=1 时： \frac{\partial }{\partial \theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^{(i)})x^{(i)})算法改写成： \begin{aligned} Repeat\quad \{\\\theta_0=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)}),\\\theta_1=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^{(i)})x^{(i)})\\\} \end{aligned}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[斯坦福机器学习笔记(Week 2)]]></title>
    <url>%2F2019%2F03%2F04%2F%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Week-2%2F</url>
    <content type="text"><![CDATA[三 、多变量线性回归( Linear Regression with Multiple Variables)3.1 多变量梯度下降在多变量线性回归中的代价函数是所有建模误差的平方和，即： J(\theta_0,\theta_1,...\theta_n)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 h_\theta(x)=\theta^TX=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n 多变量线性回归的批量梯度下降算法为： \frac{\partial }{\partial \theta_j}J(\theta_j)=0 \begin{aligned} Repeat&\quad \{\\ \theta_j&=\theta_j-\alpha\frac{\partial }{\partial \theta_j}J(\theta_0,\theta_1,...\theta_n)\\ (&simultaneously\quad update\quad all\quad \theta_j)\\ \}& \end{aligned} \begin{aligned} Repeat&\quad\{\\ \theta_j&=\theta_j-\alpha\frac{\partial }{\partial \theta_j}\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2\\ \} \end{aligned} \begin{aligned} Repeat&\quad \{\\ \theta_j&=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)})\\ (&simultaneously\quad update\quad all\quad \theta_j\quad for\quad j=0,1,...n)\\ \} \end{aligned}当n >= 1时： \begin{aligned} \theta_0=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^{(i)})x_0^{(i)})\\ \theta_1=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^{(i)})x_1^{(i)})\\ \theta_2=\theta_2-\alpha\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^{(i)})x_2^{(i)}) \end{aligned}开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛 12345def computeCost(X,y,theta): inner=np.power((np.dot(theta.T, X) - y), 2) return np.sum(inner)/(2*len(X)) 3.2 梯度下降法实践特征缩放 最简单的方法是令： x_n=\frac{x_n-\mu_n}{S_n}其中\mu_n是平均值，S_n是标准差 学习率 通常可以考虑尝试些学习率： $\alpha$ = 0.01，0.03，0.1，0.3，1，3，10 3.3 特征和多项式回归线性回归并不适用于所有数据，有时需要曲线来适应数据，比如一个二次方模型： h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2^2或者三次方模型： h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2^2+\theta_3x_3^2 令： \begin{aligned} x_2=x_2^2\\x_3=x_3^3 \end{aligned}从而将模型转化为线性回归模型 根据函数图形特性，还可以使： h_\theta(x)=\theta_0+\theta_1(size)+\theta_2(size)^2或者 h_\theta(x)=\theta_0+\theta_1(size)+\theta_2{\sqrt{size}}注：如果采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要 3.4 正规方程正规方程是通过求解下面的方程来找出使得代价函数最小的参数： \frac{\partial }{\partial \theta_j}J(\theta_j)=0假设训练集特征矩阵为 X（包含了X_0并且训练集结果为向量 y，则利用正规方程解出向量： \theta=(X^TX)^{-1}X^Ty 运用正规方程方法求解参数： 注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的 梯度下降与正规方程的比较： 梯度下降 正规方程 需要选择学习率 \alpha 不需要 需要多次迭代 一次运算得出 当特征数量 n 大时也能较好适用 需要计算(X^TX)^{-1}，如果特征数量 n 较大则运算代价大，因为矩阵逆的计算时间复杂度为O(n^3),通常来说当n小于 10000 时还是可以接受的 适用于各种类型的模型 只适用于线性模型，不适合逻辑回归模型 只要特征变量数量小于一万，通常使用标准方程法，而不使用梯度下降法 123456import numpy as npdef normalEqn(X, y): theta = np.linalg.inv(X.T@X)@X.T@y #X.T@X 等价于 X.T.dot(X) return theta]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第三周 序列模型和注意力机制（Sequence models & Attention mechanism）(Course 5)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%B8%89%E5%91%A8-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Sequence-models-Attention-mechanism%EF%BC%89-Course-5%2F</url>
    <content type="text"><![CDATA[3.1 基础模型（Basic Models）机器翻译用x^{} 到x^{< 5>}表示输入句子的单词，用y^{}到y^{}表示输出句子的单词： 首先，建立一个RNN编码网络（encoder network）（编号1），单元可以是GRU或LSTM。每次只向该网络中输入一个法语单词，将输入序列接收完毕后，这个RNN网络会输出一个向量来代表这个输入序列 之后建立一个解码网络（编号2），以编码网络的输出作为输入，之后它可以被训练为每次输出一个翻译后的单词，一直到它输出序列的结尾或者句子结尾标记 在给出足够的法语和英语文本的情况下，训练模型，通过输入一个法语句子来输出对应的英语翻译，这个模型将会非常有效。这个模型简单地用一个编码网络来对输入的法语句子进行编码，然后用一个解码网络来生成对应的英语翻译 图像描述 给出猫的图片，能自动地输出该图片的描述：一只猫坐在椅子上 通过输入图像来输出描述： 将图片输入到卷积神经网络中（一个预训练的AlexNet结构）（编号2），然后让其学习图片的编码，或者学习图片的一系列特征。去掉最后的softmax单元（编号3），这个预训练的AlexNet结构会输出4096维的特征向量，向量表示的就是这只猫的图片，这个预训练网络可以是图像的编码网络 接着把这个向量输入到RNN中（编号4），RNN要做的就是生成图像的描述，每次生成一个单词：输入一个描述输入的特征向量，然后让网络一个一个地输出单词序列 3.2 选择最可能的句子（Picking the most likely sentence）seq2seq机器翻译模型和第一周所用的语言模型之间有很多相似的地方，但也有许多重要的区别： 可以把机器翻译想成是建立一个条件语言模型，能够估计句子的可能性 绿色（编号2）表示encoder网络，紫色（编号3）表示decoder网络。不同在于语言模型总是以零向量（编号4）开始，而encoder网络会计算出一系列向量（编号2）来表示输入的句子，以这个向量作为输入，这叫做条件语言模型（conditional language model） 相比语言模型输出任意句子的概率，翻译模型会输出句子的英文翻译（编号5），这取决于输入的法语句子（编号6）。即估计一个英文翻译的概率，比如估计”Jane is visiting Africa in September.“翻译的概率，这句翻译是取决于法语句子，”Jane visite I’Afrique en septembre.“，这就是英语句子相对于输入的法语句子的可能性，是一个条件语言模型 模型将法语翻译成英文，通过输入的法语句子模型将会告诉你各种英文翻译所对应的可能性 $x$是法语句子”Jane visite l’Afrique en septembre.“，它将告诉你不同的英语翻译所对应的概率：从这个分布中进行取样得到P(y|x)，但不是从得到的分布中进行随机取样，而是要找到一个英语句子y（编号1），使得条件概率最大化： max\ P(y^{},y^{},\cdots,y^{}|x^{},x^{},\cdots,x^{})而解决这种问题最通用的算法就是束搜索(Beam Search) 为什么不用贪心搜索(Greedy Search)： 贪心搜索生成第一个词的分布以后，将会根据条件语言模型挑选出最有可能的第一个词进入机器翻译模型中，在挑选出第一个词之后将会继续挑选出最有可能的第二个词……这种算法就叫做贪心搜索，但是真正需要的是一次性挑选出整个单词序列，从y^{}、y^{}到y^{}来使得整体的概率最大化。所以贪心算法先挑出最好的第一个词，在这之后再挑最好的第二词，然后再挑第三个，这种方法并不管用 第一串（编号1）翻译明显比第二个（编号2）好，但如果贪心算法挑选出了”Jane is“作为前两个词，因为在英语中going更加常见，于是对于法语句子来说”Jane is going“相比”Jane is visiting“会有更高的概率作为法语的翻译，所以如果仅仅根据前两个词来估计第三个词的可能性，得到的更可能是going，最终得到一个欠佳的句子 当想得到单词序列y^{}、y^{}一直到最后一个词总体的概率时，一次仅仅挑选一个词并不是最佳的选择。如果字典中有10,000个单词，翻译有10个词，可能的组合就有10,000的10次方这么多，从这样大一个字典中来挑选单词，句子数量非常巨大，大大增加了运算成本，降低运算速度，不可能去计算每一种组合的可能性 所以最常用的办法就是用一个近似的搜索算法，它会尽力地将挑选出句子y使得条件概率最大化，尽管不能保证找到的y值一定可以使概率最大化 机器翻译模型和之前的语言模型一个主要的区别就是：相比之前的模型随机地生成句子，该模型是找到最有可能的翻译 3.3 集束搜索（Beam Search）“Jane visite l’Afrique en Septembre.”翻译成英语”Jane is visiting Africa in September“.，集束搜索算法首先做的就是挑选要输出的英语翻译中的第一个单词。这里列出了10,000个词的词汇表，忽略大小写，在集束搜索的第一步中用这个网络来评估第一个单词的概率值，给定输入序列x，即法语作为输入，第一个输出y的概率值是多少 集束搜索会考虑多个选择，集束搜索算法会有一个参数B，叫做集束宽（beam width）。这个例子中集束宽设成3，意味着集束搜索一次会考虑3个可能结果，比如对第一个单词有不同选择的可能性，最后找到in、jane、september，是英语输出的第一个单词的最可能的三个选项，然后集束搜索算法会把结果存到计算机内存里以便后面尝试用这三个词。为了执行集束搜索的第一步，需要输入法语句子到编码网络，然后解码这个网络，softmax层会输出10,000个概率值，然后取前三个存起来，概率表示为： P(\hat y^{} | x)集束搜索算法的第二步：针对每个第一个单词考虑第二个单词是什么 为了评估第二个词的概率值，把y^{}设为单词in（编号3），输出就是y^{}（编号5），有了这个连接（编号6），这个网络就可以用来评估：在给定法语句子和翻译结果的第一个单词in的情况下第二个单词的概率 在第二步更关心的是要找到最可能的第一个和第二个单词对，所以不仅仅是第二个单词有最大的概率，而是第一个、第二个单词对有最大的概率（编号7）。可以表示成第一个单词的概率（编号8）乘以第二个单词的概率（编号9）： P(\hat y^{},\hat y^{}|x)=P(\hat y^{} | x)\cdot P(\hat y^{}|x,\hat y^{})jane、september跟上面一样 注意，如果集束搜索找到了第一个和第二个单词对最可能的三个选择是“in September”或者“jane is”或者“jane visits”，就意味着去掉了september作为英语翻译结果的第一个单词的选择，第一个单词现在减少到了两个可能结果，但是集束宽是3，还是有y^{}，y^{}对的三个选择 接着，再预测第三个单词。分别以in september，jane is，jane visits为条件，计算每个词汇表单词作为预测第三个单词的概率。从中选择概率最大的3个作为第三个单词的预测值，得到：in september jane，jane is visiting，jane visits africa 概率表示为： P(\hat y^{}|x,\hat y^{},\hat y^{})此时，得到的前三个单词的3种情况的概率为： P(\hat y^{},\hat y^{},\hat y^{}|x)=P(\hat y^{} | x)\cdot P(\hat y^{}|x,\hat y^{})\cdot P(\hat y^{}|x,\hat y^{},\hat y^{})以此类推，每次都取概率最大的三种预测。最后，选择概率最大的那一组作为最终的翻译语句： Jane is visiting Africa in September. 如果参数B=1，则就等同于greedy search。实际应用中，可以根据不同的需要设置B为不同的值。一般B越大，机器翻译越准确，但同时也会增加计算复杂度 3.4 改进集束搜索（Refinements to Beam Search）长度归一化（Length normalization）是对束搜索算法稍作调整的一种方式，使之得到更好的结果 束搜索就是最大化概率P(y^{< 1 >}\ldots y^{< T_{y}>}|X)，表示成: P(y^{}|X)P(y^{< 2 >}|X,y^{< 1 >})P(y^{< 3 >}|X,y^{< 1 >},y^{< 2>})\cdots P(y^{< T_{y} >}|X,y^{},y^{}\ldots y^{< T_{y} - 1 >})即乘积概率（the product probabilities）： arg\ max\prod_{t=1}^{T_y} P(\hat y^{< t>}|x,\hat y^{},\cdots,\hat y^{})这些概率值通常都远小于1，会造成数值下溢（numerical underflow），即数值太小了，导致电脑的浮点表示不能精确地储存 因此在实践中,不会最大化这个乘积，而是取log值： arg\ max \sum_{t=1}^{T_y}\log P(\hat y^{}|x,\hat y^{},\cdots,\hat y^{})会得到一个数值上更稳定的算法，不容易出现数值的舍入误差（rounding errors）或者说数值下溢（numerical underflow） 参照原来的目标函数（this original objective），如果有一个很长的句子，那么这个句子的概率会很低，因为乘了很多项小于1的数字来估计句子的概率，就会得到一个更小的概率值，所以可能不自然地倾向于简短的翻译结果，因为短句子的概率是由更少数量的小于1的数字乘积得到的，所以乘积不会那么小。概率的log值也有同样的问题 解决：可以把它归一化，通过除以翻译结果的单词数量。即取每个单词的概率对数值的平均，这样很明显地减少了对输出长的结果的惩罚： arg\ max\ \frac{1}{T_y}\sum_{t=1}^{T_y}\log P(\hat y^{< t>}|x,\hat y^{},\cdots,\hat y^{})在实践中，会用一个更柔和的方法（a softer approach），在T_{y}上加上指数\alpha： arg\ max\ \frac{1}{T_y^{\alpha}}\sum_{t=1}^{T_y}\log P(\hat y^{< t>}|x,\hat y^{},\cdots,\hat y^{})$\alpha$可以等于0.7。如果\alpha等于1，就相当于完全用长度来归一化，如果\alpha等于0，T_{y}的0次幂就是1，就相当于完全没有归一化，\alpha就是算法另一个超参数（hyper parameter） 总结一下如何运行束搜索算法： 当运行束搜索时，会看到很多长度分别等于1、2、3…的句子等等，针对这些所有的可能的输出句子，取概率最大的几个句子，然后对这些句子计算目标函数arg\ max\ \frac{1}{T_y^{\alpha}}\sum_{t=1}^{T_y}\log P(\hat y^{}|x,\hat y^{},\cdots,\hat y^{})，最后从经过评估的这些句子中挑选出在归一化的log 概率目标函数上得分最高的一个，也叫作归一化的对数似然目标函数（a normalized log likelihood objective） 如何选择束宽B： B越大，考虑的选择越多，找到的句子可能越好，但是算法的计算代价越大，因为要把很多的可能选择保存起来，内存占用增大 如果用小的束宽B，结果会没那么好，因为在算法运行中，保存的选择更少，但是算法运行的更快，内存占用也小 在产品中，经常可以看到把束宽设到10，当B很大的时候，性能提高会越来越少。对于很多应用来说，从束宽1，也就是贪心算法，到束宽为3、到10，会看到一个很大的改善。但是当束宽从1000增加到3000时，效果就没那么明显 相对广度优先搜索（BFS, Breadth First Search algorithms），深度优先搜索（DFS, Depth First Search）这些精确的搜索算法（exact search algorithms），束搜索运行的更快，但是不能保证一定能找到argmax的准确的最大值 3.5 集束搜索的误差分析（Error analysis in beam search）束搜索算法是一种近似搜索算法（an approximate search algorithm），也被称作启发式搜索算法（a heuristic search algorithm），它不总是输出可能性最大的句子，它仅记录着B为前3或者10或是100种可能 人工标记为y^*。束搜索算法翻译结果标记为\hat y，是一个十分糟糕的翻译，改变了句子的原意： 模型有两个主要部分，一个是神经网络模型，或说是序列到序列模型（sequence to sequence model），称作是RNN模型，另一部分是束搜索算法，以某个集束宽度B运行 RNN(循环神经网络)实际上是个编码器和解码器（the encoder and the decoder），它会计算P(y|x)。如对于句子：Jane visits Africa in September，将Jane visits Africa填入这里（上图编号1），忽略字母的大小写，后面也是一样，计算得到P(y^*|x)，P(\hat y|x) 同样如此，然后比较一下这两个值哪个更大 若P(y^*|x) 大于P(\hat y|x)，可束搜索算法却选择了\hat y ， 因此能够得出束搜索算法实际上不能给出使P(y|x)最大化的y值，因为束搜索算法的任务就是寻找一个y的值来使这项更大，但是它却选择了\hat y，而y^*实际上能得到更大的值。因此这种情况下束搜索算法出错 若P(y^*|x)小于或等于P(\hat y|x)，y^* 是比 \hat y更好的翻译结果，不过根据RNN模型的结果，P(y^*) 是小于P(\hat y)的，即相比于\hat y，y^*成为输出的可能更小。因此在这种情况下是RNN模型出了问题 以上都忽略了长度归一化（length normalizations）的细节，如果用了某种长度归一化，那么要比较长度归一化后的最优化目标函数值 误差分析过程： 先遍历开发集，找出算法产生的错误 假如P(y^*|x)的值为2 x 10^{-10}，而P(\hat y|x)的值为 1 x10^{-10}，得知束搜索算法实际上选择了比y^*可能性更低的\hat y，则束搜索算法出错，缩写为B 接着继续遍历第二个错误，若对于第二个例子是RNN模型出现了问题，用缩写R来代表RNN 接着遍历更多的例子，有时是束搜索算法出现了问题，有时是模型出现了问题，等等 执行误差分析，得出束搜索算法和RNN模型出错的比例是多少。对开发集中每一个错误例子，即算法输出了比人工翻译更差的结果的情况，尝试确定是搜索算法出了问题，还是生成目标函数(束搜索算法使之最大化)的RNN模型出了问题。找到这两个部分中哪个是产生更多错误的原因 只有当发现是束搜索算法造成了大部分错误时，才值得花费努力增大集束宽度B；如果发现是RNN模型出了更多错，那么可以进行更深层次的分析，来决定是需要增加正则化还是获取更多的训练数据，抑或是尝试一个不同的网络结构 3.6 Bleu 得分（选修）（Bleu Score (optional)）机器翻译（machine translation）的一大难题是一个法语句子可以有多种英文翻译而且都同样很好，常见的解决办法是通过一个BLEU得分（the BLEU score）的东西来解决，BLEU得分是一个有用的单一实数评估指标，用于评估生成文本的算法，判断输出的结果是否与人工写出的参考文本的含义相似 一般有多个人工翻译： BLEU得分做的就是给定一个机器生成的翻译，它能够自动地计算一个分数来衡量机器翻译的好坏。只要机器生成的翻译与任何一个人工翻译的结果足够接近，那么它就会得到一个高的BLEU分数。BLEU代表bilingual evaluation understudy(双语评估替补)。且这些人工翻译的参考会包含在开发集或是测试集中 假设机器翻译 (MT)的输出是：the the the the the the the，是一个十分糟糕的翻译。衡量机器翻译输出质量的方法之一是观察输出结果的每一个词，看其是否出现在参考中，这被称做是机器翻译的精确度（a precision of the machine translation output）。这个情况下，机器翻译输出了七个单词并且这七个词中的每一个都出现在了参考1或是参考2，因此输出的精确度就是7/7，分母为机器翻译单词数目，分子为相应单词是否出现在参考翻译中。但是，这种方法很不科学，并不可取 改良后的精确度评估方法（the modified precision measure）：把每一个单词的记分上限定为它在参考句子中出现的最多次数。在参考1中，单词the出现了两次，在参考2中，单词the只出现了一次。单词the的得分上限为2。输出句子的得分为2/7，分母是7个词中单词the总共出现的次数，分子是单词the出现的计数，在达到上限时截断计数 上述都只是关注单独的单词，在BLEU得分中，另外一种更科学的打分方法是bleu score on bigrams（二元词组），bigram的意思就是相邻的两个单词 定义截取计数（the clipped count），也就是Count_clip：给算法设置得分上限，上限值为二元词组出现在参考1或2中的最大次数 假定机器翻译输出了稍微好一点的翻译，对MT output进行分解，得到的bigrams及其出现在MT output中的次数count为： 相应的bigrams precision为4/6也就是2/3，为二元词组改良后的精确度 将改良后的一元词组精确度定义为P_1，P代表的是精确度。下标1的意思是一元词组，即考虑单独的词，P_n 定义为n元词组精确度，用n-gram替代掉一元词组。即机器翻译输出中的n元词组的countclip之和除以n元词组的出现次数之和 如果机器翻译输出与参考1或是参考2完全一致，那么所有的P_1、P_2等等的值，都会等于1.0 最终的BLEU得分： 将得到的P_1，P_2， P_3…P_n 相加再取平均值 BLEU得分被定义为： p=exp (\frac{1}{n}\sum\limits_{i=1}^{n}{P_i}) 然后用BP（“简短惩罚”brevity penalty） 的惩罚因子（the BP penalty）来调整。它能够惩罚输出了太短翻译结果的翻译系统： p=BP\cdot exp(\frac1n\sum_{i=1}^np_i)BLEU得分被用来评估许多生成文本的系统（systems that generate text），比如说机器翻译系统（machine translation systems），图像描述系统（image captioning systems）。不过它并没有用于语音识别（speech recognition）。因为在语音识别当中，通常只有一个答案 3.7 注意力模型直观理解（Attention Model Intuition）给定一个很长的法语句子，在神经网络中，绿色的编码器要做的就是读整个句子，然后记忆整个句子，再在感知机中传递。紫色的神经网络，即解码网络（the decoder network）将生成英文翻译 对于短句子效果非常好，会有一个相对高的Bleu分（Bleu score），但是对于长句子而言，比如说大于30或者40词的句子，它的表现就会变差。Bleu评分随着单词数量变化，短的句子会难以翻译，因为很难得到所有词。对于长的句子，效果也不好，因为在神经网络中，记忆非常长句子是非常困难的。 而注意力模型翻译得很像人类，一次翻译句子的一部分。且机器翻译系统只会翻译句子的一部分，不会有一个巨大的下倾（huge dip），这个下倾衡量了神经网络记忆一个长句子的能力 对于句子里的每五个单词，使用双向的RNN（a bidirectional RNN），使用另一个RNN生成英文翻译： S^{}$由原语句附近单元共同决定，注意力权重（attention weights）\alpha^{< t,t'>} 表示尝试生成第t个英文词时应该花多少注意力在第t'个法语词上面。直到最终生成< EOS>。离得越近，注意力权重越大，相当于当前的注意力区域有个滑动窗。c表示编码器激活函数在注意力权重加权后的结果，将c输入到解码器用来生成翻译语句， 同时上一个时间步输出的翻译结果也加入 3.8 注意力模型（Attention Model）注意力模型让一个神经网络只注意到一部分的输入句子。当它在生成句子的时候，更像人类翻译 假定有一个输入句子，并使用双向的RNN，或者双向的GRU或者双向的LSTM，去计算每个词的特征： 用a^{}\cdot a^{}是输出\hat y^{}在t{'}时对RNN单元花在a^{< t{ '}>}上的注意力权重因子。即在t处生成输出词应该花多少注意力在第t{'}个输入词上面 为了让\alpha^{}，使得： a^{})}{\sum^{T_x}_{t'=1} \text{exp}(e^{}，就能得到\alpha^{}： 建立一个简单的神经网络 输入s^{}，即神经网络在上个时间步的状态和a^{} 和\alpha^{]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周 循环序列模型（Recurrent Neural Networks）(Course 5)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%B8%80%E5%91%A8-%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%88Recurrent-Neural-Networks%EF%BC%89-Course-5%2F</url>
    <content type="text"><![CDATA[1.1 为什么选择序列模型？（Why Sequence Models?）语音识别：给定一个输入音频片段 X，要求输出对应的文字记录 Y。输入和输出数据都是序列模型，因为 X是一个按时播放的音频片段，输出 Y是一系列单词音乐生成问题：只有输出数据 Y是序列，而输入数据可以是空集，也可以是个单一的整数，这个数可能指代想要生成的音乐风格，或者想要生成的那首曲子的头几个音符 处理情感分类：输入数据 X是序列，会得到类似这样的输入：“There is nothing to like in this movie.”，你认为这句评论对应几星？ DNA序列分析：DNA用A、C、G、T四个字母来表示。给定一段DNA序列，能够标记出哪部分是匹配某种蛋白质？ 机器翻译：输入句：“Voulez-vou chante avecmoi?”（法语：要和我一起唱么？），要求输出另一种语言的翻译结果 视频行为识别：得到一系列视频帧，要求识别其中的行为 命名实体识别：给定一个句子，要求识别出句中的人名 这些问题都可以被称作使用标签数据 (X,Y)作为训练集的监督学习。但序列问题有很多不同类型。有些问题里，输入数据 X和输出数据Y都是序列，但就算在那种情况下，X和Y有时也会不一样长。或者像上图编号1和编号2所示的X和Y有相同的数据长度。在另一些问题里，只有 X或者只有Y是序列 1.2 数学符号（Notation）序列模型的输入语句是：“Harry Potter and Herminoe Granger invented a new spell.”。假如想要建立一个能够自动识别句中人名位置的序列模型，那么这就是一个命名实体识别问题 该句话包含9个单词，输出y即为1 x 9向量，每位表征对应单词是否为人名的一部分，对应的输出y表示为： y=[1\ \ 1\ \ 0\ \ 1\ \ 1\ \ 0\ \ 0\ \ 0\ \ 0]^T$y^{&lt; t&gt;}$表示序列对应位置的输出，T_y表示输出序列长度，1\leq t\leq T_y 对于输入x，表示为： [x^{}\ \ x^{}\ \ x^{}\ \ x^{}\ \ x^{}\ \ x^{}\ \ x^{}\ \ x^{}\ \ x^{}]$x^{&lt; t&gt;}$表示序列对应位置的输入，T_x表示输入序列长度。此例中T_x=T_y，但是也存在T_x\neq T_y 如何表示每个x^{}： 建立一个词汇库vocabulary，尽可能包含更多的词汇。例如一个包含10000个词汇的词汇库为： \left[ \begin{matrix} a \\ and \\ \cdot \\ \cdot \\ \cdot \\ harry \\ \cdot \\ \cdot \\ \cdot \\ potter \\ \cdot \\ \cdot \\ \cdot \\ zulu \end{matrix} \right]然后使用one-hot编码，词汇表中与x^{}对应的位置为1，其它位置为0。如果出现词汇表之外的单词，可以使用UNK或其他字符串来表示 对于多样本：对应的命名规则可表示为：X^{(i)}，Y^{(i)}，T_x^{(i)}，T_y^{(i)}，i表示第i个样本。不同样本的T_x^{(i)}或T_y^{(i)}都有可能不同 1.3 循环神经网络模型（Recurrent Neural Network Model）序列模型从左到右，依次传递，此例中，T_x=T_y。x^{}到\hat y^{}之间是隐藏神经元。a^{}会传入到第t+1元素中，作为输入。其中，a^{}一般为零向量 循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的，用W_{\text{ax}}来表示管理着从x^{}到隐藏层的连接的一系列参数，每个时间步使用的都是相同的参数W_{\text{ax}}。而激活值是由参数W_{aa}决定，同时每一个时间步都使用相同的参数W_{aa}，同样的输出结果由W_{\text{ya}}决定： 预测{\hat{y}}^{< 3 >}时，不仅要使用x^{}的信息，还要使用来自x^{}和x^{}的信息，而这个循环神经网络的一个缺点是只使用了这个序列中之前的信息来做出预测，因为如果给定了这个句子，“Teddy Roosevelt was a great President.”，为了判断Teddy是否是人名的一部分，仅仅知道句中前两个词是完全不够的，还需要知道句中后部分的信息，因为句子也可能是这样的，“Teddy bears are on sale!”。因此如果只给定前三个单词，是不可能确切地知道Teddy是否是人名的一部分，第一个例子是人名，第二个例子就不是，所以不可能只看前三个单词就能分辨出其中的区别 所以这样特定的神经网络结构的一个限制是它在某一时刻的预测仅使用了从序列之前的输入信息，并没有使用序列中后部分的信息 RNN的正向传播（Forward Propagation）过程为： a^{}=g(W_{aa}\cdot a^{}+W_{ax}\cdot x^{}+b_a) \hat y^{}=g(W_{ya}\cdot a^{}+b_y) 为了简化表达式，可以对a^{}项进行整合： W_{aa}\cdot a^{}+W_{ax}\cdot x^{}=[W_{aa}\ \ W_{ax}]\left[ \begin{matrix} a^{} \\ x^{} \end{matrix} \right]\rightarrow W_a[a^{},x^{}]则正向传播可表示为： a^{}=g(W_a[a^{},x^{}]+b_a) \hat y^{}=g(W_{y}\cdot a^{}+b_y)) RNN前向传播示意图： 1.4 通过时间的反向传播（Backpropagation through time）反向传播计算方向与前向传播基本上是相反： 识别人名的例子，经过RNN正向传播，单个元素的Loss function为： L^{}(\hat y^{},y^{})=-y^{}log\ \hat y^{}-(1-y^{})log\ (1-\hat y^{}) 这是 binary classification 的 Loss Function，注意与1.6 的softmax Loss Function区别 该样本所有元素的Loss function为： L(\hat y,y)=\sum_{t=1}^{T_y}L^{}(\hat y^{},y^{})反向传播（Backpropagation）过程就是从右到左分别计算L(\hat y,y)对参数W_{a}，W_{y}，b_a，b_y的偏导数，这种从右到左的求导过程被称为Backpropagation through time RNN反向传播示意图： 1.5 不同类型的循环神经网络（Different types of RNNs）RNN模型包含以下几个类型： 一对一，当去掉a^{}时它就是一种标准类型的神经网络 一对多，比如音乐生成或者序列生成 多对一，如是情感分类的例子，首先读取输入，一个电影评论的文本，然后判断他们是否喜欢电影还是不喜欢 多对多，如命名实体识别，T_{x}=T_{y} 多对多，如机器翻译，T_x\neq T_y 1.6 语言模型和序列生成（Language model and sequence generation）在语音识别中，某句语音有两种翻译： the apple and pair salad the apple and pear salad 语言模型会计算出这两句话各自的出现概率： P( \text{The apple and pair salad}) = 3.2 \times 10^{-13} P\left(\text{The apple and pear salad} \right) = 5.7 \times 10^{-10} 选择概率最大的语句作为正确的翻译 概率计算的表达式为： P(y^{},y^{},\cdots,y^{})如何使用RNN构建语言模型： 首先需要一个足够大的训练集，训练集由大量的单词语句语料库（corpus）构成。然后，对corpus的每句话进行切分词（tokenize），建立vocabulary，对每个单词进行one-hot编码。例如下面这句话： The Egyptian Mau is a bread of cat. 每句话结束末尾，需要加上&lt; EOS &gt;作为语句结束符。若语句中有词汇表中没有的单词，用&lt; UNK &gt;表示。假设单词“Mau”不在词汇表中，则上面这句话可表示为： The Egyptian &lt; UNK &gt; is a bread of cat. &lt; EOS &gt; 准备好训练集并对语料库进行切分词等处理之后，接下来构建相应的RNN模型： $x^{}$和a^{}均为零向量。Softmax输出层\hat y^{}表示出现该语句第一个单词的概率，softmax输出层\hat y^{}表示在第一个单词基础上出现第二个单词的概率，即条件概率，以此类推，最后是出现&lt; EOS &gt;的条件概率 单个元素的softmax loss function为： L^{}(\hat y^{},y^{})=-\sum_iy_i^{}log\ \hat y_i^{} 这是softmax Loss Function ，注意与1.4 binary classification 的 Loss Function区别 该样本所有元素的Loss function为： L(\hat y,y)=\sum_tL^{}(\hat y^{},y^{})对语料库的每条语句进行RNN模型训练，最终得到的模型可以根据给出语句的前几个单词预测其余部分，将语句补充完整。例如给出“Cats average 15”，RNN模型可能预测完整的语句是“Cats average 15 hours of sleep a day.” 整个语句出现的概率等于语句中所有元素出现的条件概率乘积。例如某个语句包含y^{},y^{},y^{}，则整个语句出现的概率为： P(y^{},y^{},y^{})=P(y^{})\cdot P(y^{}|y^{})\cdot P(y^{}|y^{},y^{})1.7 对新序列采样（Sampling novel sequences）基于词汇的RNN模型序列模型模拟了任意特定单词序列的概率，要做的是对这些概率分布进行采样来生成一个新的单词序列。编号1所示的网络已经被上方所展示的结构训练过，编号2是进行采样 第一步要做的是对想要模型生成的第一个词进行采样，输入x^{} =0，a^{} =0，第一个时间步得到的是所有可能的输出，是经过softmax层后得到的概率，然后根据这个softmax的分布进行随机采样。对这个向量使用np.random.choice，来根据向量中这些概率的分布进行采样，就能对第一个词进行采样 然后继续下一个时间步，\hat y^{}作为输入（编号4），然后softmax层就会预测\hat y^{}是什么。假如第一个词进行抽样后得到的是The，现在要计算出在第一词是The的情况下，第二个词应该是什么（编号5），然后再用采样函数来对\hat y^{}进行采样 即无论得到什么样的用one-hot码表示的选择结果，都把它传递到下一个时间步，然后进行采样，直到最后一个时间步 怎样知道一个句子结束： 如果代表句子结尾的标识在字典中，可以一直进行采样直到得到EOS标识（编号6），代表着已经抵达结尾，可以停止采样 如果字典中没有这个词，可以决定从20个或100个或其他个单词进行采样，然后一直将采样进行下去直到达到所设定的时间步。不过这种过程有时候会产生一些未知标识（编号7），如果要确保算法不会输出这种标识，要做的是拒绝采样过程中产生任何未知的标识，一旦出现就继续在剩下的词中进行重采样，直到得到一个不是未知标识的词 这就是如何从RNN语言模型中生成一个随机选择的句子。以上所建立的是基于词汇的RNN模型，意思就是字典中的词都是英语单词（下图编号1） 基于字符的RNN结构用以上字符组成字典（上图编号2所示） 序列\hat y^{}，\hat y^{}，\hat y^{}在训练数据中是单独的字符，对于“Cats average 15 hours of sleep a day.”，C是\hat y^{}，a就是\hat y^{}，t就是\hat y^{}等等 优点： 不必担心会出现未知的标识，基于字符的语言模型会将Mau这样的序列也视为可能性非零的序列。而基于词汇的语言模型，如果Mau不在字典中，只能当作未知标识UNK 缺点： 最后会得到太多太长的序列，基于字符的语言模型在捕捉句子中的依赖关系也就是句子较前部分如何影响较后部分不如基于词汇的语言模型那样可以捕捉长范围的关系，并且基于字符的语言模型训练起来计算成本比较高 1.8 循环神经网络的梯度消失（Vanishing gradients withRNNs） 编号1cat是单数，应该用was，编号2 cats是复数，用were 这个例子中的句子有长期的依赖，最前面的单词对句子后面的单词有影响。但基本的RNN模型（编号3）不擅长捕获长期依赖效应 RNN反向传播很困难，会有梯度消失的问题，后面层的输出误差（编号6）很难影响前面层（编号7）的计算。即很难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式的was或者were。且在英语里面中间的内容（编号8）可以任意长，所以基本的RNN模型会有很多局部影响，输出\hat y^{}主要受附近的值（编号10）的影响，编号6所示的输出很难受到序列靠前的输入（编号10）的影响，因为不管输出是什么，对的还是错的，这个区域都很难反向传播到序列的前面部分，也因此网络很难调整序列前面的计算 在反向传播的时候，随着层数的增多，梯度不仅可能指数型的下降，也可能指数型的上升。梯度消失在训练RNN时是首要的问题，不过梯度爆炸也会出现，但是梯度爆炸很明显，因为指数级大的梯度会让参数变得极其大，以至于网络参数崩溃。参数大到崩溃会看到很多NaN，或者不是数字的情况，这意味着网络计算出现了数值溢出 解决方法：用梯度修剪。梯度向量如果大于某个阈值，缩放梯度向量，保证不会太大 1.9 GRU单元（Gated Recurrent Unit（GRU））门控循环单元：改变了RNN的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题 简化的GRU模型RNN隐藏层的单元的可视化： $a^{&lt; t&gt;}$表达式为： a^{}=tanh(W_a[a^{},x^{}]+b_a)为了解决梯度消失问题，对上述单元进行修改，添加了记忆单元，构建GRU，如下图所示： 表达式为： \tilde c^{}=tanh(W_c[c^{},x^{}]+b_c) \Gamma_u=\sigma(W_u[c^{},x^{}]+b_u) c^{}=\Gamma_u*\tilde c^{}+(1-\Gamma_u)*c^{}$c^{}=a^{}$，c^{}=a^{}。符号c表示记忆细胞的值，a表示输出的激活值，\tilde c^{}是个候选值，替代了c的值，\Gamma_u（0到1）意为gate，u表示“update”，当\Gamma_u=1时，代表更新；当\Gamma_u=0时，代表记忆，保留之前的模块输出。\Gamma_u能够保证RNN模型中跨度很大的依赖关系不受影响，消除梯度消失问题 完整的GRU完整的GRU添加了另外一个gate，即\Gamma_r，表达式如下： \tilde c^{}=tanh(W_c[\Gamma_r*c^{},x^{}]+b_c) \Gamma_u=\sigma(W_u[c^{},x^{}]+b_u) \Gamma_r=\sigma(W_r[c^{},x^{}]+b_r) c^{}=\Gamma_u*\tilde c^{}+(1-\Gamma_u)*c^{} a^{}=c^{}$\Gamma_{r}$门：计算出的下一个c^{}的候选值{\tilde{c}}^{}跟c^{}有多大的相关性 $c^{&lt; t&gt;}$可以是一个向量（编号1），如果有100维的隐藏的激活值，那么c^{}、{\tilde{c}}^{}、\Gamma_{u}还有画在框中的其他值也是100维 $*$实际上就是元素对应的乘积（c^{< t>}=\Gamma_u*\tilde c^{< t>}+(1-\Gamma_u)*c^{}），若\Gamma_{u}（\Gamma_u=\sigma(W_u[c^{},x^{< t>}]+b_u)）是一个100维的向量，而里面的值几乎都是0或者1，则这100维的记忆细胞c^{< t>}（c^{< t>}=a^{< t>}，编号1）就是要更新的比特 1.10 长短期记忆（LSTM（long short term memory）unit）LSTM是另一种更强大的解决梯度消失问题的方法。它对应的RNN隐藏层单元结构如下图所示： 相应的表达式为： \tilde c^{}=tanh(W_c[a^{},x^{}]+b_c) \Gamma_u=\sigma(W_u[a^{},x^{< t>}]+b_u) \Gamma_f=\sigma(W_f[a^{},x^{< t>}]+b_f) \Gamma_o=\sigma(W_o[a^{},x^{< t>}]+b_o) c^{< t>}=\Gamma_u*\tilde c^{< t>}+\Gamma_f*c^{} a^{< t>}=\Gamma_o*c^{< t>}LSTM包含三个gates：\Gamma_u,\Gamma_f,\Gamma_o，分别对应update gate，forget gate和output gate 在LSTM中不再有a^{< t>} = c^{< t>}的情况 红线显示了只要正确地设置了遗忘门和更新门，LSTM很容易把c^{}的值一直往下传递到右边，比如c^{} = c^{}。这就是为什么LSTM和GRU非常擅长于长时间记忆某个值 “窥视孔连接”（peephole connection）:门值不仅取决于a^{}和x^{< t>}，也取决于上一个记忆细胞的值（c^{}），即c^{}也能影响门值 如果考虑c^{}对\Gamma_u,\Gamma_f,\Gamma_o的影响，可加入“窥视孔连接”，对LSTM的表达式进行修改： \tilde c^{< t>}=tanh(W_c[a^{},x^{< t>}]+b_c) \Gamma_u=\sigma(W_u[a^{},x^{< t>},c^{}]+b_u) \Gamma_f=\sigma(W_f[a^{},x^{< t>},c^{}]+b_f) \Gamma_o=\sigma(W_o[a^{},x^{< t>},c^{}]+b_o) c^{< t>}=\Gamma_u*\tilde c^{< t>}+\Gamma_f*c^{} a^{< t>}=\Gamma_o*c^{}LSTM主要的区别：比如（上图编号13）有一个100维的隐藏记忆细胞单元，第i个c^{}的元素只会影响第i个元素对应的那个门，所以关系是一对一的，并不是任意这100维的c^{}可以影响所有的门元素 LSTM前向传播图： GRU：模型简单，更容易创建一个更大的网络，只有两个门，在计算上运行得更快，且可以扩大模型的规模 LSTM：更加强大和灵活，因为它有三个门而不是两个 1.11 双向循环神经网络（Bidirectional RNN）双向RNN模型在序列的某点处不仅可以获取之前的信息，还可以获取未来的信息 用只有4个单词的句子，x^{}到x^{}。这个网络有一个前向的循环单元a^{\rightarrow }，a^{\rightarrow }，a^{\rightarrow }，a^{\rightarrow }，这四个循环单元都有一个当前输入x输入进去，得到预测的\hat y^{}，\hat y^{}，\hat y^{}和\hat y^{} 再增加一个反向循环层：a^{\leftarrow }，a^{\leftarrow }，a^{\leftarrow }，a^{\leftarrow } 给定一个输入序列x^{}到x^{}，这个序列先后计算前向的a^{\rightarrow }，a^{\rightarrow }，a^{\rightarrow }，a^{\rightarrow }，而反向序列从a^{\leftarrow }开始，计算完了反向的a^{\leftarrow }，可以用这些激活值计算反向a^{\leftarrow },a^{\leftarrow },a^{\leftarrow } 值得注意的是计算的是网络激活值，这不是反向传播而是前向的传播，图中前向传播一部分计算是从左到右，一部分计算是从右到左。把所有激活值都计算完了就可以计算预测结果 预测结果： \hat y^{} =g(W_{g}\left\lbrack a^{\rightarrow },a^{\leftarrow } \right\rbrack +b_{y})这些基本单元不仅仅是标准RNN单元，也可以是GRU单元或者LSTM单元 双向RNN网络模型的缺点是需要完整的数据的序列才能预测任意位置。比如要构建一个语音识别系统，双向RNN模型需要等待整个语音说完，获取整个语音表达才能处理这段语音，并进一步做语音识别 1.12 深层循环神经网络（Deep RNNs） $a^{\lbrack l\rbrack }$表示第l层的激活值，&lt;t&gt;表示第t个时间点 激活值a^{[l]< t>}有两个输入: a^{[l]< t>}=g(W_a^{[l]}[a^{[l]},a^{[l-1]< t>}]+b_a^{[l]})对于RNN来说，有三层就已经不少了。由于时间的维度，RNN网络会变得相当大，即使只有很少的几层 另外一种Deep RNNs结构是每个输出层上还有一些垂直单元： 即把输出去掉（编号1），在每一个上面堆叠循环层，然后换成一些深的层，这些层并不水平连接，只是一个深层的网络，然后用来预测y^{< t>} 这些单元（编号3）没必要是标准的RNN，也可以是GRU单元或者LSTM单元，也可以构建深层的双向RNN网络，但深层的RNN训练需要很多计算资源，需要很长的时间]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）(Course 5)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%BA%8C%E5%91%A8-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%EF%BC%88Natural-Language-Processing-and-Word-Embeddings%EF%BC%89-Course-5%2F</url>
    <content type="text"><![CDATA[2.1 词汇表征（Word Representation）one-hot向量表示：单词Man，Woman，King，Queen，Apple，Orange分别出现在词汇表的第5391，9853，4914，7157，456，6257的位置，则它们分别用O_{5391},O_{9853} 等表示，O代表one-hot： 缺点是把每个词孤立起来，使得算法对相关词的泛化能力不强 因为任何两个one-hot向量的内积都是0，例如king和queen，词性相近，但是单从one-hot编码上来看，内积为零，无法知道二者的相似性 因此用特征表征（Featurized representation）的方法对每个单词进行编码。也就是使用一个特征向量表征单词，特征向量的每个元素都是对该单词某一特征的量化描述，量化范围可以是[-1,1]之间，而单词使用这种高维特征表示时，就叫做词嵌入（word embedding）， 词嵌入可以让算法自动的理解一些类似的词，比如男人对女人，国王对王后： 以上举例的特征实际上并不是手工设计的，而是算法（word embedding）学习而来；而且这些学习的特征，可能并不具有良好的解释性，但无论如何，算法都可以快速找到哪些单词是类似的 特征向量的长度依情况而定，特征元素越多则对单词表征得越全面。这里的特征向量长度设定为300。使用特征表征之后，词汇表中的每个单词都可以使用对应的300 x 1的向量来表示，该向量的每个元素表示该单词对应的某个特征值。每个单词用e+词汇表索引的方式标记，例如e_{5391}，e_{9853}，e_{4914}，e_{7157}，e_{456}，e_{6257} 用这种表示方法来表示apple和orange这些词，那么apple和orange的这种表示肯定会非常相似，可能有些特征不太一样，如颜色口味，但总的来说apple和orange的大部分特征实际上都一样，或者说都有相似的值。这样对于已经知道orange juice的算法很大几率上也会明白apple juice这个东西，这样对于不同的单词算法会泛化的更好 如果能够学习到一个300维的特征向量，或者说300维的词嵌入，把这300维的数据嵌入到一个二维空间里，就可以可视化了。常用的可视化算法是t-SNE算法，会发现man和woman这些词聚集在一块，king和queen聚集在一块等等 在对这些概念可视化的时候，词嵌入算法对于相近的概念，学到的特征也比较类似，最终把它们映射为相似的特征向量 2.2 使用词嵌入（Using Word Embeddings）之前Named entity识别的例子（即找出语句中的人名），每个单词采用的是one-hot编码。RNN模型能确定Sally Johnson是一个人名而不是一个公司名，是因为“orange farmer”是份职业，很明显“Sally Johnson”是一个人名（输出1） 如果用特征化表示方法，即用词嵌入作为输入训练好的模型，如果一个新的输入：“Robert Lin is an apple farmer.”，因为知道orange和apple很相近，那么算法很容易就知道Robert Lin也是一个人的名字 featurized representation的优点是可以减少训练样本的数目，前提是对海量单词建立特征向量表述。即使训练样本不够多，测试时遇到陌生单词，例如“durian cultivator”，根据之前海量词汇特征向量就判断出“durian”也是一种水果，与“apple”类似，而“cultivator”与“farmer”也很相似。从而得到与“durian cultivator”对应的应该也是一个人名。这种做法将单词用不同的特征来表示，即使是训练样本中没有的单词，也可以根据word embedding的结果得到与其词性相近的单词，从而得到与该单词相近的结果，有效减少了训练样本的数量 词嵌入能够达到这种效果，原因是学习词嵌入的算法会考察非常大的文本集 词嵌入做迁移学习的步骤： 先从大量的文本集中学习词嵌入，或者下载网上预训练好的词嵌入模型 用这些词嵌入模型迁移到新的只有少量标注训练集的任务中，比如用300维的词嵌入来表示单词。好处就是可以用更低维度的特征向量代替原来的10000维的one-hot向量。尽管one-hot向量很快计算，但学到的用于词嵌入的300维的向量会更加紧凑 当在新的任务上训练模型，而在命名实体识别任务上只有少量的标记数据集，可以选择要不要继续微调，用新的数据调整词嵌入。但实际中只有第二步中有很大的数据集才会这样做，如果标记的数据集不是很大，通常不会在微调词嵌入上费力气 当任务的训练集相对较小时，词嵌入的作用最明显，所以它广泛用于NLP领域 词嵌入和人脸编码有很多相似性，训练了一个Siamese网络结构，这个网络会学习不同人脸的一个128维表示，然后通过比较编码结果来判断两个图片是否是同一个人脸，在人脸识别领域用编码指代向量f(x^{\left(i \right)})，f(x^{\left( j\right)})，词嵌入的意思和这个差不多 人脸识别领域和词嵌入不同就是： 在人脸识别中训练一个网络，任给一个人脸照片，甚至是没有见过的照片，神经网络都会计算出相应的一个编码结果 学习词嵌入则是有一个固定的词汇表，比如10000个单词，学习向量e_{1}到e_{10000}，学习一个固定的编码，即每一个词汇表的单词的固定嵌入 人脸识别中的算法未来可能涉及到海量的人脸照片，而自然语言处理有一个固定的词汇表，没有出现过的单词就记为未知单词 2.3 词嵌入的特性（Properties of Word Embeddings） 该例中，假设用的是四维的嵌入向量，假如向量e_{\text{man}}和e_{\text{woman}}、e_{\text{king}}和e_{\text{queen}} 分别进行减法运算，相减结果表明，“Man”与“Woman”的主要区别是性别，“King”与“Queen”也是一样 所以当算法被问及man对woman相当于king对什么时，算法所做的就是计算e_{\text{man}}-e_{\text{woman}}，然后找出一个向量也就是找出一个词，使得： e_{\text{man}}-e_{\text{woman}}\approx e_{\text{king}} - e_{?}即当这个新词是queen时，式子的左边会近似地等于右边 ]在图中，词嵌入向量在一个可能有300维的空间里，箭头代表的是向量在gender（性别）这一维的差，为了得出类比推理，计算当man对于woman，king对于什么，要做的就是找到单词w来使得 e_{\text{man}}-e_{\text{woman}}\approx e_{\text{king}} - e_{w}等式成立，即找到单词w来最大化e_{w}与e_{\text{king}} - e_{\text{man}} + e_{\text{woman}}的相似度，即 Find\ word\ w:argmax\ Sim(e_{w},e_{\text{king}} - e_{\text{man}} + e_{\text{woman}})即把e_{w}全部放到等式的一边，另一边是e_{\text{king}}- e_{\text{man}} + e_{\text{woman}}。应用相似度函数，通过方程找到一个使得相似度最大的单词，如果结果理想的话会得到单词queen t-SNE算法所做的就是把这些300维的数据用一种非线性的方式映射到2维平面上，可以得知t-SNE中这种映射很复杂而且很非线性。在大多数情况下，由于t-SNE的非线性映射，不能总是期望使等式成立的关系会像左边那样成一个平行四边形 关于相似函数，比较常用的是余弦相似度，假如在向量u和v之间定义相似度： Sim(u,v)=\frac{u^Tv}{||u||\cdot ||v||}分子是u和v的内积。如果u和v非常相似，那么它们的内积将会很大，把整个式子叫做余弦相似度，是因为该式是u和v的夹角的余弦值 参考资料： 给定两个向量u和v，余弦相似度定义如下： {CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta)$u.v$ 是两个向量的点积（或内积），||u||_2是向量u的范数（或长度）， \theta 是向量u和v之间的角度。这种相似性取决于角度在向量u和v之间。如果向量u和v非常相似，它们的余弦相似性将接近1; 如果它们不相似，则余弦相似性将取较小的值 两个向量之间角度的余弦是衡量它们有多相似的指标，角度越小，两个向量越相似 还可以计算Euclidian distance来比较相似性，即||u-v||^2。距离越大，相似性越小 2.4 嵌入矩阵（Embedding Matrix）当应用算法来学习词嵌入时，实际上是学习一个嵌入矩阵 假设某个词汇库包含了10000个单词，每个单词包含的特征维度为300，那么表征所有单词的embedding matrix维度为300 x 10000，用E来表示。某单词w的one-hot向量表示为O_w，维度为10000 x 1 则该单词的嵌入向量(embedding vector)表达式为： e_w=E\cdot O_w只要知道了embedding matrixE，就能计算出所有单词的embedding vector e_w 不过上述这种矩阵乘积运算E\cdot O_w效率并不高，矩阵维度很大，且O_w大部分元素为零。通常做法是直接从E中选取第w列作为e_w 2.5 学习词嵌入（Learning Word Embeddings）embedding matrix E可以通过构建自然语言模型，运用梯度下降算法得到。若输入样本是： I want a glass of orange (juice). 通过这句话的前6个单词，预测最后的单词“juice”。E未知待求，每个单词可用embedding vector e_w表示。构建的神经网络模型结构如下图所示： 神经网络输入层包含6个embedding vectors，每个embedding vector维度是300，则输入层总共有1800个输入。Softmax层有10000个概率输出，与词汇表包含的单词数目一致。正确的输出label是“juice”。其中$E,W^{[1]},b^{[1]},W^{[2]},b^{[2]}$为待求值。对足够的训练例句样本，运用梯度下降算法，迭代优化，最终求出embedding matrixE 这种算法的效果还不错，能够保证具有相似属性单词的embedding vector相近 为了让神经网络输入层数目固定，可以选择只取预测单词的前4个单词作为输入，例如该句中只选择“a glass of orange”四个单词作为输入。这里的4是超参数，可调 把输入叫做context，输出叫做target。对应到上面这句话里： context: a glass of orange target: juice 关于context的选择有多种方法： target前n个单词或后n个单词，n可调 target前1个单词 target附近某1个单词（Skip-Gram）E 事实证明，不同的context选择方法都能计算出较准确的embedding matrix E 2.6 Word2Vec选择context和target的方法中，比较流行的是采用Skip-Gram模型 Skip-Gram模型的做法是：首先随机选择一个单词作为context，例如“orange”；然后使用一个宽度为5或10（自定义）的滑动窗，在context附近选择一个单词作为target，可以是“juice”、“glass”、“my”等等。最终得到了多个context—target对作为监督式学习样本： 训练的过程是构建自然语言模型，经过softmax单元的输出为： \hat y=\frac{e^{\theta_t^T\cdot e_c}}{\sum_{j=1}^{10000}e^{\theta_j^T\cdot e_c}}$\theta_t$为target对应的参数，e_c为context的embedding vector，且e_c=E\cdot O_c 相应的loss function为： L(\hat y,y)=-\sum_{i=1}^{10000}y_ilog\ \hat y_i 由于 y是一个one-hot向量，所以上式实际上10000个项里面只有一项是非0的 然后，运用梯度下降算法，迭代优化，最终得到embedding matrix E 然而，这种算法计算量大，影响运算速度。主要因为softmax输出单元为10000个，\hat y计算公式中包含了大量的求和运算 解决的办法之一是使用hierarchical softmax classifier，即树形分类器： [] 这种树形分类器是一种二分类。它在每个数节点上对目标单词进行区间判断，最终定位到目标单词。最多需要\log_2 N步就能找到目标单词，N为单词总数 实际应用中，对树形分类器做了一些改进。改进后的树形分类器是非对称的，通常选择把比较常用的单词放在树的顶层，而把不常用的单词放在树的底层。这样更能提高搜索速度 关于context的采样：如果使用均匀采样，那么一些常用的介词、冠词，例如the, of, a, and, to等出现的概率更大一些。但是这些单词的embedding vectors通常不是最关心的，更关心的例如orange, apple， juice等这些名词。所以实际应用中一般不选择随机均匀采样的方式来选择context，而是使用其它算法来处理这类问题 2.7 负采样（Negative Sampling）算法要做的是构造一个新的监督学习问题：给定一对单词，比如orange和juice，去预测这是否是一对上下文词-目标词（context-target） 在这个例子中orange和juice就是个正样本，用1作为标记，orange和king就是个负样本，标为0。要做的就是采样得到一个上下文词和一个目标词，中间列叫做词（word）。然后： 生成一个正样本，先抽取一个context，在一定词距内比如说正负10个词距内选一个target，生成这个表的第一行，即orange– juice -1的过程 生成一个负样本，用相同的context，再在字典中随机选一个词，如king、book、the、of，标记为0。因为如果随机选一个词，它很可能跟orange没关联 如果从字典中随机选到的词，正好出现在了词距内，比如说在上下文词orange正负10个词之内，也没关系，如of被标记为0，即使of的确出现在orange词的前面 接下来将构造一个监督学习问题，学习算法输入x，即输入这对词（编号7），要去预测目标的标签（编号8），即预测输出y 如何选取K： 小数据集的话，K从5到20，数据集越小K就越大 如果数据集很大，K就选的小一点。对于更大的数据集K就从2到5 学习从x映射到y的监督学习模型： 编号2是新的输入x，编号3是要预测的值y。记号c表示context，记号t表示可能的target，y表示0和1，即是否是一对context-target。要做的是定义一个逻辑回归模型，给定输入的c，t对的条件下，y=1的概率，即： P\left( y = 1 \middle| c,t \right) = \sigma(\theta_{t}^{T}e_{c})如果输入词是orange，即词6257，要做的就是输入one-hot向量，和E相乘获得嵌入向量e_{6257}，最后得到10,000个可能的逻辑回归分类问题，其中一个（编号4）将会是用来判断目标词是否是juice的分类器，其他的词比如下面的某个分类器（编号5）是用来预测king是否是目标词 negative sampling中某个固定的正样本对应k个负样本，即模型总共包含了k+1个binary classification。对比之前10000个输出单元的softmax分类，negative sampling转化为k+1个二分类问题，每次迭代并不是训练10000个，而仅训练其中k+1个，计算量要小很多，大大提高了模型运算速度 这种方法就叫做负采样（Negative Sampling）: 选择一个正样本，随机采样k个负样本 选取了context orange之后，如何选取负样本： 通过单词出现的频率进行采样：导致一些类似a、the、of等词的频率较高 均匀随机地抽取负样本：没有很好的代表性 （推荐）： P(w_{i}) = \frac{f( w_{i})^{\frac{3}{4}}}{\sum_{j = 1}^{10,000}{f( w_{j} )^{\frac{3}{4}}}}这种方法处于上面两种极端采样方法之间，即不用频率分布，也不用均匀分布，而采用的是对词频的\frac{3}{4}除以词频\frac{3}{4}整体的和进行采样的。其中，f(w_j)是语料库中观察到的某个词的词频 2.8 GloVe 词向量（GloVe Word Vectors） GloVe代表用词表示的全局变量（global vectors for word representation） 假定X_{ij}是单词i在单词j上下文中出现的次数，i和j 与t和c的功能一样，可以认为X_{ij}等同于X_{tc}。根据context和target的定义，会得出X_{ij}等于X_{ji} 如果将context和target的范围定义为出现于左右各10词以内的话，就有对称关系X_{ij}=X_{ji} 如果对context的选择是context总是目target前一个单词，那么X_{ij}\neq X_{ji} 对于GloVe算法，可以定义context和target为任意两个位置相近的单词，假设是左右各10词的距离，那么X_{ij}就是一个能够获取单词i和单词j彼此接近的频率计数器 GloVe模型做的就是进行优化，将差距进行最小化处理： \text{mini}\text{mize}\sum_{i = 1}^{10,000}{\sum_{j = 1}^{10,000}}{f\left( X_{ij} \right)\left( \theta_{i}^{T}e_{j} + b_{i} + b_{j}^{'} - \log X_{ij} \right)^{2}}$\theta_{i}^{T}e_{j}$即\theta_{t}^{T}e_{c}。对于\theta_{t}^{T}e_{c}，这两个单词同时出现的频率是多少受X_{ij}影响，若两个词的embedding vector越相近，同时出现的次数越多，则对应的loss越小 当X_{ij}=0时，权重因子f(X_{ij})=0。这种做法直接忽略了无任何相关性的context和target，只考虑X_{ij}>0的情况 出现频率较大的单词相应的权重因子f(X_{ij})较大，出现频率较小的单词相应的权重因子f(X_{ij})较小一些 因为\theta和e是完全对称的，所以\theta_{i}和e_{j}是对称的。因此训练算法的方法是一致地初始化\theta和e，然后使用梯度下降来最小化输出，当每个词都处理完之后取平均值： e_{w}^{(final)}= \frac{e_{w} +\theta_{w}}{2}GloVe算法不能保证嵌入向量的独立组成部分： 通过上面的很多算法得到的词嵌入向量，无法保证词嵌入向量的每个独立分量是能够理解的。但能够确定是每个分量和所想的一些特征是有关联的，可能是一些我们能够理解的特征的组合而构成的一个组合分量 使用上面的GloVe模型，从线性代数的角度解释如下： \Theta_{i}^{T}e_{j} = \Theta_{i}^{T}A^{T}A^{-T}e_{j}=(A\Theta_{i})^{T}(A^{-T}e_{j})加入的A项，可能构成任意的分量组合 2.9 情感分类（Sentiment Classification）情感分类任务就是看一段文本，然后分辨这个人是否喜欢他们在讨论的这个东西，最大的挑战就是可能标记的训练集没有那么多，但是有了词嵌入，即使只有中等大小标记的训练集，也能构建一个不错的情感分类器 输入x是一段文本，输出y是要预测的相应情感。比如一个餐馆评价的星级 情感分类一个最大的挑战就是可能标记的训练集没有那么多。对于情感分类任务来说，训练集大小从10,000到100,000个单词都很常见，甚至有时会小于10,000个单词，采用了词嵌入能够带来更好的效果，尤其是只有很小的训练集 给定四个词（”dessert is excellent“），通常用10,000个词的词汇表，找到相应的one-hot向量，再乘以嵌入矩阵E，E可以从一个很大的文本集里学习到，比如它可以从一亿个词或者一百亿个词里学习嵌入，然后用来提取单词the的嵌入向量e_{8928}，对dessert、is、excellent做同样的步骤 然后取这些向量（编号2），如300维度的向量，通过平均值计算单元（编号3），求和并平均，再送进softmax分类器，然后输出\hat y。这个softmax能够输出5个可能结果的概率值，从一星到五星 这个算法适用于任何长短的评论，因为即使评论是100个词长，也可以对这一百个词的特征向量求和取平均，得到一个300维的特征向量，然后送进softmax分类器 但问题是没考虑词序，如负面的评价，”Completely lacking in good taste, good service, and good ambiance.“，good这个词出现了很多次，但算法忽略词序，仅仅把所有单词的词嵌入加起来或者平均下来，最后的特征向量会有很多good的表示，分类器很可能认为这是一个好的评论，尽管事实上这是一个差评，只有一星的评价 为了解决这一问题，情感分类的另一种模型是RNN： 首先取这条评论，”Completely lacking in good taste, good service, and good ambiance.“，找出每一个one-hot向量，乘以词嵌入矩阵E，得到词嵌入表达e，然后把它们送进RNN RNN的工作就是在最后一步（编号1）计算一个特征表示，用来预测\hat y。这样的算法考虑词的顺序效果更好，能意识到”things are lacking in good taste“是个负面的评价，“not good”也是一个负面的评价。而不像原来的算法一样，只是把所有的加在一起得到一个大的向量，根本意识不到“not good”和 “good”不是一个意思，”lacking in good taste“也是如此，等等 如果训练一个这样的算法，最后会得到一个很合适的情感分类的算法。由于词嵌入是在一个更大的数据集里训练的，这样会更好的泛化一些没有见过的新的单词。比如”Completely absent of good taste, good service, and good ambiance.“，即使absent这个词不在标记的训练集里 如果是在一亿或者一百亿单词集里训练词嵌入，它仍然可以正确判断，并且泛化的很好，甚至这些词是在训练集中用于训练词嵌入，但不在专门做情感分类问题标记的训练集 2.10 词嵌入除偏（Debiasing Word Embeddings）根据训练模型所使用的文本，词嵌入能够反映出性别、种族、年龄、性取向等其他方面的偏见： 假设已经完成一个词嵌入的学习，各个词的位置如图： 首先做的事就是辨别出想要减少或想要消除的特定偏见的趋势 怎样辨别出偏见相似的趋势： 一、对于性别歧视，对所有性别对立的单词求差值，再平均： bias\ direction=\frac1N ((e_{he}-e_{she})+(e_{male}-e_{female})+\cdots)二、中和步骤，对于定义不确切的词可以将其处理一下，避免偏见。像doctor和babysitter使之在性别方面中立。将它们在这个轴（编号1）上进行处理，减少或是消除他们的性别歧视趋势的成分，即减少在水平方向上的距离（编号2方框内所示的投影） 三、均衡步，babysitter和grandmother之间的距离或者说是相似度实际上是小于babysitter和grandfather之间的（编号1），因此这可能会加重不良状态，或者非预期的偏见，也就是说grandmothers相比于grandfathers最终更有可能输出babysitting。所以在最后的均衡步中，想要确保的是像grandmother和grandfather这样的词都能够有一致的相似度，或者说是相等的距离，做法是将grandmother和grandfather移至与中间轴线等距的一对点上（编号2），现在性别歧视的影响也就是这两个词与babysitter的距离就完全相同了（编号3） 最后，掌握哪些单词需要中立化非常重要。一般来说，大部分英文单词，例如职业、身份等都需要中立化，消除embedding vector中性别这一维度的影响]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第四周 特殊应用：人脸识别和神经风格转换(Special applications： Face recognition & Neural style transfer)(Course 4)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E5%9B%9B%E5%91%A8-%E7%89%B9%E6%AE%8A%E5%BA%94%E7%94%A8%EF%BC%9A%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E5%92%8C%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E6%8D%A2%EF%BC%88Special-applications-Face-recognition-Neural-style-transfer%EF%BC%89-Course-4%2F</url>
    <content type="text"><![CDATA[4.1 什么是人脸识别？（What is face recognition?） 人脸验证（face verification）问题：如果有一张输入图片以及某人的ID或者是名字，系统要做的是验证输入图片是否是这个人，也被称作1对1问题，只需要弄明白这个人是否和他声称的身份相符 人脸识别（face recognition）问题：（1对多问题（1:K））输入一张人脸图片，验证输出是否为K个模板中的某一个，即一对多问题 一般人脸识别比人脸验证更难。因为假设人脸验证系统的错误率是1%，那么在人脸识别中，输出分别与K个模板都进行比较，则相应的错误率就会增加，约K%。模板个数越多，错误率越大一些 什么是人脸识别？（What is face recognition?）人脸验证（face verification）问题：如果有一张输入图片以及某人的ID或者是名字，系统要做的是验证输入图片是否是这个人，也被称作1对1问题，只需要弄明白这个人是否和他声称的身份相符人脸识别（face recognition）问题：（1对多问题（1:K））输入一张人脸图片，验证输出是否为K个模板中的某一个，即一对多问题一般人脸识别比人脸验证更难。因为假设人脸验证系统的错误率是1%，那么在人脸识别中，输出分别与K个模板都进行比较，则相应的错误率就会增加，约K%。模板个数越多，错误率越大一些 4.2 One-Shot学习（One-shot learning） 要让人脸识别能够做到一次学习，要做的是学习Similarity函数 让神经网络学习用d表示的函数： d(img1,img2) = degree\ of\ difference\ between\ images以两张图片作为输入，然后输出这两张图片的差异值 如果这两张图片的差异值小于某个阈值\tau，就能预测这两张图片是同一个人 如果差异值大于τ，就能预测这是不同的两个人 对于人脸识别问题，只需计算测试图片与数据库中K个目标的相似函数，取其中d(img1,img2)最小的目标为匹配对象。若所有的d(img1,img2)都很大，则表示数据库没有这个人 如果之后有新人加入了团队（编号5），只需将他的照片加入数据库，系统依然能照常工作 4.3 Siamese 网络（Siamese network）函数d的作用是输入两张人脸，然后输出相似度。实现这个功能的一个方式是用Siamese网络 向量（编号1）是由网络深层的全连接层计算出来的，叫做f(x^{(1)})。可以把f(x^{(1)})看作是输入图像x^{(1)}的编码，即取输入图像（编号2），然后表示成128维的向量 如果要比较两个图片，要做的是把第二张图片喂给有同样参数的同样的神经网络，得到一个不同的128维的向量（编号3），第二张图片的编码叫做f(x^{(2)}) 然后定义d，将x^{(1)}和x^{(2)}的距离定义为两幅图片的编码之差的范数： d( x^{( 1)},x^{( 2)}) =|| f( x^{( 1)}) - f( x^{( 2)})||_{2}^{2}对于两个不同的输入，运行相同的卷积神经网络，然后比较它们，就叫做Siamese网络架构 训练Siamese神经网络：不同图片的CNN网络所有结构和参数都是一样的。所以要做的是训练一个网络，利用梯度下降算法不断调整网络参数，使得属于同一人的图片之间d(x^{(1)},x^{(2)}) 很小，而不同人的图片之间d(x^{(1)},x^{(2)})很大 即神经网络的参数定义了一个编码函数f(x^{(i)})，如果给定输入图像x^{(i)}，这个网络会输出x^{(i)}的128维的编码。然后要做的就是学习参数 使得如果两个图片x^{( i)}和x^{( j)}是同一个人，那么得到的两个编码的距离就小 如果x^{(i)}和x^{(j)}是不同的人，那么编码距离就大 如果改变这个网络所有层的参数，会得到不同的编码结果，要做的是用反向传播来改变这些所有的参数，以确保满足这些条件 4.4 Triplet 损失（Triplet 损失）要想通过学习神经网络的参数来得到优质的人脸图片编码，方法之一就是定义三元组损失函数然后应用梯度下降 三元组损失每个样本包含三张图片：靶目标（Anchor）、正例（Positive）、反例（Negative），简写成A、P、N 网络的参数或者编码应满足： 让|| f(A) - f(P) ||^{2}很小，即： || f(A) - f(P)||^{2} \leq ||f(A) - f(N)||^{2} ||f(A)-f(P)||^2-||f(A)-F(N)||^2\leq 0$|| f(A) - f(P) ||^{2}$是d(A,P)，|| f(A) - f(N) ||^{2}是d(A,N) 如果所有的图片都是零向量，即f(A)=0,f(P)=0,f(N)=0那么上述不等式也满足。但是对进行人脸识别没有任何作用，所以添加一个超参数\alpha，且\alpha>0，对上述不等式做出如下修改： ||f(A)-f(P)||^2-||f(A)-F(N)||^2\leq -\alpha ||f(A)-f(P)||^2-||f(A)-F(N)||^2+\alpha \leq 0 间隔参数\alpha也被称为边界margin，类似于支持向量机中的margin，拉大了Anchor和Positive图片对和Anchor与Negative图片对之间的差距。若d(A,P)=0.5，\alpha=0.2，则d(A,N)\geq0.7 损失函数的定义基于三元图片组，即取这个和0的最大值： L( A,P,N) = max(|| f( A) - f( P)||^{2} -|| f( A) - f( N)||^{2} + \alpha,0)$max$函数的作用是只要|| f( A) - f( P)||^{2} -|| f( A) - f( N)||^{2} + \alpha\leq0，损失函数就是0 如果|| f( A) - f( P)||^{2} -|| f( A) - f( N)||^{2} + \alpha\leq0，最终会得到|| f(A) - f( P)||^{2} -|| f( A) - f( N)||^{2} +\alpha，即正的损失值。通过最小化这个损失函数达到的效果就是使这部分|| f( A) - f( P)||^{2} -||f( A) - f( N)||^{2} +\alpha成为0，或者小于等于0 整个网络的代价函数是训练集中单个三元组损失的总和 如何选择三元组来形成训练集：如果从训练集中随机地选择A​、P​和N​，遵守A​和P​是同一个人，而A​和N​是不同的人这一原则。那么约束条件（d(A,P) + \alpha \leq d(A,N)​）很容易达到，因为随机选择的图片，A​和N​比A​和P​差别很大的概率很大，而且差距远大于\alpha​，这样网络并不能从中学到什么 所以为了构建一个数据集，要做的就是尽可能选择难训练的三元组A、P和N： 想要所有的三元组都满足条件（d(A,P) + a \leq d(A,N)），A、P和N的选择应使得d(A,P)很接近d(A,N)，即d(A,P) \approx d(A,N)，这样学习算法会竭尽全力使右边式子变大（d(A,N)），或者使左边式子（d(A,P)）变小，这样左右两边至少有一个\alpha的间隔。并且选择这样的三元组还可以增加学习算法的计算效率 总结： 训练三元组损失需要把训练集做成很多三元组，这就是一个三元组（编号1），有一个Anchor图片和Positive图片，这两个（Anchor和Positive）是同一个人，还有一张另一个人的Negative图片。这是另一组（编号2），其中Anchor和Positive图片是同一个人，但是Anchor和Negative不是同一个人，等等。 定义了这些包括A、P和N图片的数据集之后，还需要用梯度下降最小化代价函数J，这样做的效果就是反向传播到网络中的所有参数来学习到一种编码，使得如果两个图片是同一个人，那么它们的d就会很小，如果两个图片不是同一个人，它们的d 就会很大 4.5 面部验证与二分类（Face verification and binary classification）另一个训练神经网络的方法是选取一对神经网络，选取Siamese网络，使其同时计算这些嵌入，比如说128维的嵌入（编号1），或者更高维，然后将其输入到逻辑回归单元进行预测，如果是相同的人，那么输出是1，若是不同的人，输出是0。这就把人脸识别问题转换为一个二分类问题，训练这种系统时可以替换Triplet loss的方法 最后的逻辑回归单元怎么处理： 比如说sigmoid函数应用到某些特征上，输出\hat y会变成： \hat y = \sigma(\sum_{k = 1}^{128}{w_{i}\| f( x^{( i)})_{k} - f( x^{( j)})_{k}\| + b})把这128个元素当作特征，然后把他们放入逻辑回归中，最后的逻辑回归可以增加参数w_{i}和b，就像普通的逻辑回归一样。然后在这128个单元上训练合适的权重，用来预测两张图片是否是一个人 $\hat y$的另外一种表达式为： \hat y=\sigma(\sum_{k=1}^Kw_k\frac{(f(x^{(i)})_k-f(x^{(j)})_k)^2}{f(x^{(i)})_k+f(x^{(j)})_k}+b)这个公式也被叫做\chi^{2}公式，也被称为\chi平方相似度 上面神经网络拥有的参数和下面神经网络的相同（编号3和4所示的网络），两组参数是绑定的，这样的系统效果很好 如果这是一张新图片（编号1），当员工走进门时，希望门可以自动为他们打开，这个（编号2）是在数据库中的图片，不需要每次都计算这些特征（编号6），可以提前计算好，当一个新员工走近时，使用上方的卷积网络来计算这些编码（编号5），和预先计算好的编码进行比较，然后输出预测值\hat y 总结：把人脸验证当作一个监督学习，创建一个只有成对图片的训练集，不是三个一组，而是成对的图片，目标标签是1表示一对图片是一个人，目标标签是0表示图片中是不同的人。利用不同的成对图片，使用反向传播算法去训练Siamese神经网络 4.6 什么是深度卷积网络？（What are deep ConvNets learning?）假如训练了一个Alexnet轻量级网络，不同层之间隐藏单元的计算结果如下： 从第一层的隐藏单元开始，将训练集经过神经网络，然后弄明白哪一张图片最大限度地激活特定的单元。在第一层的隐藏单元，只能看到小部分卷积神经，只有一小块图片块是有意义的，因为这就是特定单元所能看到的全部 然后选一个另一个第一层的隐藏单元，重复刚才的步骤： 对其他隐藏单元也进行处理，会发现其他隐藏单元趋向于激活类似于这样的图片： 以此类推，这是9个不同的代表性神经元，每一个不同的图片块都最大化地激活了。可以理解为第一层的隐藏单元通常会找一些简单的特征，比如说边缘或者颜色阴影 在深层部分，一个隐藏单元会看到一张图片更大的部分，在极端的情况下，可以假设每一个像素都会影响到神经网络更深层的输出，靠后的隐藏单元可以看到更大的图片块 第一层，第一个被高度激活的单元： 第二层检测的特征变得更加复杂： 第三层明显检测到更复杂的模式 第四层，检测到的模式和特征更加复杂： 第五层检测到更加复杂的事物： 4.7 代价函数（Cost function）为了实现神经风格迁移，需要定义一个关于G的代价函数J用来评判某个生成图像的好坏，使用梯度下降法去最小化J(G)，以便于生成图像 代价函数定义为两个部分： J_{\text{content}}(C,G)，被称作内容代价，是一个关于内容图片和生成图片的函数，用来度量生成图片G的内容与内容图片C的内容有多相似 然后把结果加上一个风格代价函数J_{\text{style}}(S,G)，用来度量图片G的风格和图片S的风格的相似度 J( G) = \alpha J_{\text{content}}( C,G) + \beta J_{\text{style}}(S,G)最后用两个超参数\alpha和\beta来来确定内容代价和风格代价 对于代价函数J(G)，为了生成一个新图像，要做的是随机初始化生成图像G，可能是100×100×3、500×500×3，或任何想要的尺寸 然后使用之前定义的代价函数J(G)，用梯度下降的方法将其最小化，更新： G:= G - \frac{\partial}{\partial G}J(G)即更新图像G的像素值，也就是100×100×3，比如RGB通道的图片 比如从内容图片（编号1）和风格（编号2）图片开始，当随机初始化G，生成图像就是随机选取像素的白噪声图（编号3）。接下来运行梯度下降算法，最小化代价函数J(G)，逐步处理像素，慢慢得到一个生成图片（编号4、5、6），越来越像用风格图片的风格画出来的内容图片 4.8 内容代价函数（Content cost function）$J(G)$的第一部分J_{content}(C,G)，它表示内容图片C与生成图片G之间的相似度 使用的CNN网络是之前训练好的模型，例如Alex-Net。C，S，G共用相同模型和参数 CNN的每个隐藏层分别提取原始图片的不同深度特征，由简单到复杂。如果l太小，则G与C在像素上会非常接近，没有迁移效果；如果l太深，则G上某个区域将直接会出现C中的物体。所以在实际中，层l在网络中既不会选的太浅也不会选的太深 衡量内容图片和生成图片在内容上的相似度： 令a^{[l][C]}和a^{[l][G]}代表图片C和G的l层的激活函数值。如果这两个激活值相似，意味着两个图片的内容相似 定义： J_{content}(C,G) = \frac{1}{4 \times n_H \times n_W \times n_C}\sum _{ \text{all entries}} (a^{[l][C]} - a^{[l][C]})^2为两个激活值不同或者相似的程度 后面如果对J(G)做梯度下降来找G的值时，整个代价函数会激励这个算法来找到图像G，使得隐含层的激活值和内容图像的相似 4.9 风格代价函数（Style cost function）利用CNN网络模型，图片的风格可以定义成第l层隐藏层不同通道间激活函数的乘积（相关性） 选取第l层隐藏层，各通道使用不同颜色标注。因为每个通道提取图片的特征不同，比如1通道（红色）提取的是图片的垂直纹理特征，2通道（黄色）提取的是图片的橙色背景特征。那么这两个通道的相关性越大，表示原始图片及既包含了垂直纹理也包含了该橙色背景；相关性越小，表示原始图片并没有同时包含这两个特征。即计算不同通道的相关性，反映了原始图片特征间的相互关系，从某种程度上刻画了图片的“风格” 接下来定义图片的风格矩阵（style matrix）为： G_{kk^{'}}^{[l]} = \sum_{i = 1}^{n_{H}^{[l]}}{\sum_{j = 1}^{n_{W}^{[l]}}{a_{i, j,k}^{[l]}a_{i, j, k^{'}}^{[l]}}}a_{i, j, k}^{[l]}$$为隐藏层$$l$$中$$(i,j,k)$$位置的激活项，$$i$$，$$j$$，$$k$$分别代表该位置的高度、宽度以及对应的通道数，k，$$k^{'}$$分别表示不同通道。风格矩阵$$G_{kk^{'}}^{[l]}$$计算第$$l$$层隐藏层不同通道对应的所有激活函数输出和，$$l$$层风格图像的矩阵$$G^{[l]}$$是一个$$n_{c} \times n_{c}$$的矩阵： ![upload successful](http://pnlb0i3oh.bkt.clouddn.com/pasted-155.png) 若两个通道之间相似性高，则对应的$$G_{kk^{'}}^{[l]}$$较大；若两个通道之间相似性低，则对应的$$G_{kk^{'}}^{[l]}$$较小 风格矩阵$$G_{kk'}^{[l](S)}$$表征了风格图片$$S$$第$$l$$层隐藏层的“风格”。生成图片$$G$$也有$$G_{kk'}^{[l](G)}$$，$$G_{kk'}^{[l](S)}$$与$$G_{kk'}^{[l](G)}$$越相近，则表示$$G$$的风格越接近$$S$$。即$$J^{[l]}_{style}(S,G)$$定义为：J_{style}^{[l]}(S,G) = \frac{1}{4 \times {n_C}^2 \times (n_H \times n_W)^2} \sum _{i=1}^{n_C}\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2 然后使用梯度下降算法，不断迭代修正$$G$$的像素值，使$$J^{[l]}_{style}(S,G)$$不断减小 为了提取更多的“风格”，可以使用多层隐藏层，然后相加，表达式为：J_{style}(S,G)=\sum_l\lambda^{[l]}\cdot J^{[l]}_{style}(S,G) $$\lambda^{[l]}$$表示累加过程中各层$$J^{[l]}_{style}(S,G)$$的权重系数，为超参数 最终的cost function为：J(G)=\alpha \cdot J_{content}(C,G)+\beta \cdot J_{style}(S,G) $$ 之后用梯度下降法，或者更复杂的优化算法来找到一个合适的图像G，并计算J(G)的最小值，这样将能够得到非常好看的结果 4.10 一维到三维推广（1D and 3D generalizations of models）1D卷积将2D卷积推广到1D卷积： 二维数据的卷积是将同一个5×5特征检测器应用于图像中不同的位置（编号1所示），最后得到10×10的输出结果。1维过滤器可以在不同的位置中应用类似的方法（编号3，4，5所示） 当对这个1维信号使用卷积，将一个14维的数据与5维数据进行卷积，并产生一个10维输出： 如果有16个过滤器，最后会获得一个10×16的数据： 对于卷积网络的下一层，如果输入一个10×16数据，可以使用一个5维过滤器进行卷积，需要16个通道进行匹配，如果有32个过滤器，另一层的输出结果就是6×32： 3D卷积当进行CT扫描时，人体躯干的不同切片数据本质上是3维的 如果有一个3D对象是14×14×14： 过滤器也是3D的，如果使用5×5×5过滤器进行卷积，将会得到一个10×10×10的结果输出，如果使用16个过滤器，输出将是10×10×10×16 如果下一层卷积使用5×5×5×16维度的过滤器再次卷积，如果有32个过滤器，最终将得到一个6×6×6×32的输出]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第三周 目标检测（Object detection)(Course 4)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%B8%89%E5%91%A8-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88Object-detection-Course-4%2F</url>
    <content type="text"><![CDATA[3.1 目标定位（Object localization）定位分类问题：不仅要用算法判断图片中是不是一辆汽车，还要在图片中标记出它的位置，用边框或红色方框把汽车圈起来，“定位”的意思是判断汽车在图片中的具体位置 定位分类问题通常只有一个较大的对象位于图片中间位置，对它进行识别和定位。对象检测问题中图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象 构建汽车自动驾驶系统，对象可能包括以下几类：行人、汽车、摩托车和背景 定位图片中汽车的位置：让神经网络输出一个边界框，标记为b_{x},b_{y},b_{h}和b_{w}，是被检测对象的边界框的参数化表示 红色方框的中心点表示为(b_{x},b_{y})，边界框的高度为b_{h}，宽度为b_{w}。训练集不仅包含神经网络要预测的对象分类标签，还要包含表示边界框的这四个数字，接着采用监督学习算法，输出一个分类标签，还有四个参数值，从而给出检测对象的边框位置 如何为监督学习任务定义目标标签 y： 目标标签y​的定义： y= \begin{bmatrix} p_{c} \\ b_{x} \\ b_{y}\\ b_{h}\\ b_{w} \\ c_{1}\\ c_{2}\\ c_{3} \end{bmatrix}$p_{c}​$表示是否含有对象，如果对象属于前三类（行人、汽车、摩托车），则p_{c}= 1​，如果是背景，则p_{c} =0​。p_{c}​表示被检测对象属于某一分类的概率，背景分类除外 如果检测到对象，就输出被检测对象的边界框参数b_{x}​、b_{y}​、b_{h}​和b_{w}​。p_{c}=1​，同时输出c_{1}​、c_{2}​和c_{3}​，表示该对象属于行人，汽车还是摩托车 如果图片中没有检测对象: $p_{c} =0$，y的其它参数全部写成问号，表示“毫无意义”的参数 神经网络的损失函数，如果采用平方误差策略： L\left(\hat{y},y \right) = \left( \hat{y_1} - y_{1} \right)^{2} + \left(\hat{y_2} - y_{2}\right)^{2} + \ldots+\left( \hat{y_8} - y_{8}\right)^{2}损失值等于每个元素相应差值的平方和 如果图片中存在定位对象，y_{1} =p_{c}=1，损失值是不同元素的平方和 $y_{1}= p_{c} = 0$，损失值是\left(\hat{y_1} - y_{1}\right)^{2}，只需要关注神经网络输出p_{c}的准确度 这里用平方误差简化了描述过程。实际可以每个分量使用不同的损失函数, 通常做法是对p_{c}应用逻辑回归函数，边界框坐标应用平方差, 分类标签使用softmax损失函数 3.2 特征点检测（Landmark detection）仅对目标的关键特征点坐标进行定位，这些关键点被称为landmarks 选定特征点个数，并生成包含特征点的标签训练集，利用神经网络输出脸部关键特征点的位置 具体做法:准备一个卷积网络和一些特征集，将人脸图片输入卷积网络，输出1表示有人脸，0表示没有人脸，然后输出（l_{1x}，l_{1y}）……直到（l_{64x}，l_{64y}），l代表一个特征，即该网络模型共检测人脸上64处特征点，加上是否为face的标志位，输出label共有64x2+1=129个值，即有129个输出单元，由此实现对图片的人脸检测和定位 检测人体姿势动作： 特征点的特性在所有图片中必须保持一致 3.3 目标检测（Object detection）通过卷积网络进行对象检测，采用的是基于滑动窗口的目标检测算法 构建汽车检测算法步骤： 首先创建一个标签训练集，x和y表示适当剪切的汽车图片样本，一开始可以使用适当剪切的图片，就是整张图片x几乎都被汽车占据，使汽车居于中间位置，并基本占据整张图片 开始训练卷积网络，输入这些适当剪切过的图片（编号6），卷积网络输出y，0或1表示图片中有汽车或没有汽车 训练完这个卷积网络，用它来实现滑动窗口目标检测，具体步骤如下： 1.首先选定一个特定大小的窗口，将红色小方块输入卷积神经网络，卷积网络开始判断红色方框内有没有汽车 [ 2.滑动窗口目标检测算法继续处理第二个图像，红色方框稍向右滑动之后的区域，并输入给卷积网络，再次运行卷积网络，然后处理第三个图像，依次重复操作，直到这个窗口滑过图像的每一个角落 思路是以固定步幅移动窗口，遍历图像的每个区域，把这些剪切后的小图像输入卷积网络，对每个位置按0或1进行分类 3.重复上述操作，选择一个更大的窗口，截取更大的区域，并输入给卷积神经网络处理，输出0或1 4.再以某个固定步幅滑动窗口，重复以上操作，遍历整个图像，输出结果 5.第三次重复操作，选用更大的窗口 这样不论汽车在图片的什么位置，总有一个窗口可以检测到 这种算法叫作滑动窗口目标检测：以某个步幅滑动这些方框窗口遍历整张图片，对这些方形区域进行分类，判断里面有没有汽车 滑动窗口目标检测算法缺点：计算成本 如果选用的步幅很大，会减少输入卷积网络的窗口个数，粗糙间隔尺寸可能会影响性能 如果采用小粒度或小步幅，传递给卷积网络的小窗口会特别多，这意味着超高的计算成本 3.4 卷积的滑动窗口实现（Convolutional implementation of sliding windows）把神经网络的全连接层转化成卷积层 前几层和之前的一样，下一层全连接层用5×5×16的过滤器来实现，数量是400个（编号1），输入图像大小为5×5×16，输出维度是1×1×400，这400个节点中每个节点都是上一层5×5×16激活值经过某个任意线性函数的输出结果 再添加另外一个卷积层（编号2），用1×1卷积，假设有400个1×1的过滤器，在这400个过滤器的作用下，下一层的维度是1×1×400，是上个网络中的这一全连接层经由1×1过滤器的处理，得到一个softmax激活值，通过卷积网络，最终得到1×1×4的输出层，而不是这4个数字（编号3） 以上就是用卷积层代替全连接层的过程，结果这几个单元集变成了1×1×400和1×1×4的维度 通过卷积实现滑动窗口对象检测算法假设向滑动窗口卷积网络输入14×14×3的图片，神经网络最后的输出层，即softmax单元的输出是1×1×4 假设测试集图片是16×16×3，给输入图片加上黄色条块，在最初的滑动窗口算法中，把蓝色区域输入卷积网络（红色笔标记）生成0或1分类。接着滑动窗口，步幅为2个像素，向右滑动2个像素，将绿框区域输入给卷积网络，运行整个卷积网络，得到另外一个标签0或1。继续将这个橘色区域输入给卷积网络，卷积后得到另一个标签，最后对右下方的紫色区域进行最后一次卷积操作。在这个16×16×3的小图像上滑动窗口，卷积网络运行了4次，于是输出了了4个标签 这4次卷积操作中很多计算都是重复的。执行滑动窗口的卷积时使得卷积网络在这4次前向传播过程中共享很多计算，尤其是在编号1，卷积网络运行同样的参数，使用相同的5×5×16过滤器进行卷积操作，得到12×12×16的输出层。然后执行同样的最大池化（编号2），输出结果6×6×16。照旧应用400个5×5的过滤器（编号3），得到一个2×2×400的输出层，现在输出层为2×2×400，应用1×1过滤器（编号4）得到另一个2×2×400的输出层。再做一次全连接的操作（编号5），最终得到2×2×4的输出层，在输出层4个子方块中，蓝色的是图像左上部分14×14的输出（红色箭头标识），右上角方块是图像右上部分（绿色箭头标识）的对应输出，左下角方块是输入层左下角（橘色箭头标识），右下角是卷积网络处理输入层右下角14×14区域(紫色箭头标识)的结果 具体的计算步骤：以绿色方块为例，假设剪切出这块区域（编号1），传递给卷积网络，第一层的激活值就是这块区域（编号2），最大池化后的下一层的激活值是这块区域（编号3），这块区域对应着后面几层输出的右上角方块（编号4，5，6） 该卷积操作的原理是不需要把输入图像分割成四个子集，分别执行前向传播，而是把它们作为一张图片输入给卷积网络进行计算，其中的公共区域可以共享很多计算 假如对一个28×28×3的图片应用滑动窗口操作，以14×14区域滑动窗口，以大小为2的步幅不断地向右移动窗口，直到第8个单元格，得到输出层的第一行。然后向图片下方移动，最终输出8×8×4的结果 总结滑动窗口的实现过程： 在图片上剪切出一块区域，假设大小是14×14，把它输入到卷积网络。继续输入下一块区域，大小同样是14×14，重复操作，直到某个区域识别到汽车 但是不能依靠连续的卷积操作来识别图片中的汽车，可以对大小为28×28的整张图片进行卷积操作，一次得到所有预测值，如果足够幸运，神经网络便可以识别出汽车的位置 在卷积层上应用滑动窗口算法提高了整个算法的效率，缺点是边界框的位置可能不够准确 3.5 Bounding Box预测（Bounding box predictions）滑动窗口法的卷积实现算法效率很高，但不能输出最精准的边界框 输入图像是100×100的，用3×3网格，实际实现时会用更精细的网格（19×19）。使用图像分类和定位算法 编号1什么也没有，左上格子的标签向量y是\begin{bmatrix}0\ ?\ ?\ ?\ ?\ ?\ ?\ ? \end{bmatrix}。其他什么也没有的格子都一样 图中有两个对象，YOLO算法做的是取两个对象的中点，将对象分配给包含对象中点的格子。即使中心格子（编号5）同时有两辆车的一部分，分类标签y也为y= \begin{bmatrix}0\ ?\ ?\ ?\ ?\ ?\ ?\ ? \end{bmatrix}。编号4目标标签y= \begin{bmatrix} 1\ b_{x}\ b_{y}\ b_{h}\ b_{w}\ 0\ 1\ 0 \end{bmatrix}，编号6类似 3×3中9个格子都对应一个8维输出目标向量y，其中一些值可以是dont care-s（即？）所以总的目标输出尺寸就是3×3×8 如果要训练一个输入为100×100×3的神经网络，输入图像通过普通的卷积网络，卷积层，最大池化层等等，最后映射到一个3×3×8输出尺寸。然后用反向传播训练神经网络，将任意输入x映射到输出向量y 这个算法的优点在于神经网络可以输出精确的边界框，测试的时候有要做的是喂入输入图像x，然后跑正向传播，直到得到输出y。然后3×3位置对应的9个输出，只要每个格子中对象数目没有超过1个，这个算法应该是没问题的。但实践中会使用更精细的19×19网格，输出就是19×19×8，多个对象分配到同一个格子得概率就小得多 即使对象可以横跨多个格子，也只会被分配到9个格子其中之一，或者19×19网络的其中一个格子。在19×19网格中，两个对象的中点（图中蓝色点所示）处于同一个格子的概率就会更低。 优点： 显式地输出边界框坐标，可以具有任意宽高比，并且能输出更精确的坐标，不会受到滑动窗口分类器的步长大小限制 并没有在3×3网格上跑9次算法，而是单次卷积实现，但在处理这3×3计算中很多计算步骤是共享的，所以这个算法效率很高 因为是卷积实现，运行速度非常快，可以达到实时识别 如何编码这些边界框b_{x}、b_{y}、b_{h}和b_{w}： 在YOLO算法中，编号1约定左上点是(0,0)，右下点是(1,1)，橙色中点的位置b_{x}大概是0.4，b_{y}大概是0.3，b_{w}是0.9，b_{h}是0.5。b_{x}、b_{y}、b_{h}和b_{w}单位是相对于格子尺寸的比例，所以b_{x}和b_{y}必须在0和1之间，因为从定义上看，橙色点位于对象分配到格子的范围内，如果它不在0和1之间，即它在方块外，那么这个对象就应该分配到另一个格子上。这个值（b_{h}和b_{w}）可能会大于1，特别是如果有一辆汽车的边界框是这样的（编号3所示），那么边界框的宽度和高度有可能大于1 3.6 交并比（Intersection over union）并交比函数可以用来评价对象检测算法 交并比（loU）函数是计算两个边界框交集和并集之比。两个边界框的并集是两个边界框绿色阴影区域，而交集是这个橙色阴影区域，交并比就是交集的大小（橙色阴影面积）除以绿色阴影的并集面积 一般约定，在计算机检测任务中，如果loU≥0.5，就说检测正确，如果预测器和实际边界框完美重叠，loU就是1，因为交集就等于并集 3.7 非极大值抑制（Non-max suppression）对象检测中的一个问题是算法可能对同一个对象做出多次检测，非极大值抑制可以确保算法对每个对象只检测一次 实践中当运行对象分类和定位算法时，对于每个格子都运行一次，编号1、2、3可能会认为这辆车中点应该在格子内部 这个算法做的是： 1.首先看哪个检测结果相关的概率p_{c}（实际上是p_{c}乘以c_{1}、c_{2}或c_{3}）概率最大，右边车辆中是0.9，即最可靠的检测，用高亮标记，之后非极大值抑制逐一审视剩下的矩形，所有和这个最大的边框有很高交并比，高度重叠的其他边界框输出就会被抑制 2.逐一审视剩下的矩形，找出概率p_{c}最高的一个，在这种情况下是0.8，就认为检测出一辆车（左边车辆），然后非极大值抑制算法就会去掉其他loU值很高的矩形。现在每个矩形都会被高亮显示或者变暗，如果直接抛弃变暗的矩形，就剩下高亮显示的那些是最后得到的两个预测结果 非最大值意味着只输出概率最大的分类结果，但抑制很接近，不是最大的其他预测结果 算法的细节： 首先在19×19网格上执行算法，会得到19×19×8的输出尺寸。简化成只做汽车检测，会得到输出预测概率（p_{c}）和边界框参数（b_{x}、b_{y}、b_{h}和b_{w}） 1.将所有的预测值p_{c}小于或等于某个阈值，如p_{c}\le 0.6的边界框去掉 2.剩下的边界框就一直选择概率p_{c}最高的边界框，把它输出成预测结果，取一个边界框，让它高亮显示，就可以确定输出有一辆车的预测 3.去掉所有剩下的边界框 如果同时检测三个对象，比如说行人、汽车、摩托，输出向量就会有三个额外的分量。正确的做法是独立进行三次非极大值抑制，对每个输出类别都做一次 3.8 Anchor Boxes对象检测存在的一个问题是每个格子只能检测出一个对象，如果想让一个格子检测出多个对象，可以使用anchor box 行人的中点和汽车的中点都落入到同一个格子中 anchor box的思路是：预先定义两个不同形状的anchor box，把预测结果和这两个anchor box关联起来 定义类别标签： y= \begin{bmatrix} p_{c} & b_{x} & b_{y} &b_{h} & b_{w} & c_{1} & c_{2} & c_{3} & p_{c} & b_{x} & b_{y} & b_{h} & b_{w} &c_{1} & c_{2} & c_{3} \end{bmatrix}^{T}前面的p_{c},b_{x},b_{y},b_{h},b_{w},c_{1},c_{2},c_{3}（绿色方框标记的参数）是和anchor box 1关联的8个参数，后面的8个参数（橙色方框标记的元素）是和anchor box 2相关联 行人：p_{c}= 1,b_{x},b_{y},b_{h},b_{w},c_{1} = 1,c_{2} = 0,c_{3} = 0 车子的边界框更像anchor box 2，(p_{c}= 1,b_{x},b_{y},b_{h},b_{w},c_{1} = 0,c_{2} = 1,c_{3} = 0) 现在每个对象都分配到对象中点所在的格子中，以及分配到和对象形状交并比最高的anchor box中。然后观察哪个anchor box和实际边界框（编号1，红色框）的交并比更高 编号1对应同时有车和行人，编号3对应只有车： anchor box是为了处理两个对象出现在同一个格子的情况，实践中这种情况很少发生，特别用的是19×19网格 怎么选择anchor box： 一般手工指定anchor box形状，可以选择5到10个anchor box形状，覆盖到想要检测的对象的各种形状 更高级的是使用k-means算法，将两类对象形状聚类，选择最具有代表性的一组anchor box 3.9 YOLO 算法（Putting it together: YOLO algorithm）假设要在图片中检测行人、汽车，同时使用两种不同的Anchor box 训练集： 输入X：同样大小的完整图片 目标Y：使用3\times3网格划分，输出大小3\times3\times2\times8，或者3\times3\times16 对不同格子中的小图，定义目标输出向量Y 编号2目标向量y =\begin{bmatrix} 0 & ? & ? & ? & ? & ? & ? & ? & 1 & b_{x} & b_{y} & b_{h} &b_{w} & 0 & 1 & 0 \end{bmatrix}^{T}，假设训练集中对于车子有一个边界框（编号3），水平方向更长一点，红框和anchor box 2的交并比更高，车子和向量的下半部分相关 模型预测： 输入与训练集中相同大小的图片，然后训练一个卷积网络，遍历9个格子，得到每个格子中不同的输出结果：3\times3\times2\times8 运行非最大值抑制（NMS）： 假设使用了2个Anchor box，每一个网格都会得到预测输出的2个bounding boxes，其中一个P_{c}比较高 抛弃概率P_{c}值低的预测bounding boxes 对每个对象分别使用NMS算法得到最终的预测边界框 如果有三个对象检测类别，希望检测行人，汽车和摩托车：对于每个类别单独运行非极大值抑制，处理预测结果所属类别的边界框，用非极大值抑制来处理行人、车子、摩托车类别，运行三次来得到最终的预测结果 3.10 候选区域（选修）（Region proposals (Optional)）滑动窗法会对原始图片的每个区域都进行扫描，即使是一些空白的或明显没有目标的区域，这样会降低算法运行效率，耗费时间 R-CNN算法，即带区域的卷积网络，或者带区域的CNN。这个算法尝试选出一些区域，在少数窗口上运行卷积网络分类器 选出候选区域的方法是运行图像分割算法，找出各个尺度的色块，然后在色块上运行分类器，即首先得到候选区域，然后再分类 R-CNN算法很慢，基本的R-CNN算法是使用某种算法求出候选区域，然后对每个候选区域运行一下分类器，每个区域会输出一个标签，有没有车子、行人、摩托车？并输出一个边界框，就能在确实存在对象的区域得到一个精确的边界框 R-CNN算法不会直接信任输入的边界框，也会输出一个边界框b_{x}，b_{y}，b_{h}和b_{w}，这样得到的边界框比较精确，比单纯使用图像分割算法给出的色块边界要好 Fast R-CNN算法基本上是R-CNN算法，最初的算法是逐一对区域分类，快速R-CNN用的是滑动窗法的一个卷积实现，和3.4 卷积的滑动窗口实现的相似，显著提升了R-CNN的速度，问题是得到候选区域的聚类步骤仍然非常缓慢 更快的R-CNN算法（Faster R-CNN），使用的是卷积神经网络，而不是更传统的分割算法来获得候选区域色块，比Fast R-CNN算法快得多 不过大多数更快R-CNN的算法实现还是比YOLO算法慢很多]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周 深度卷积网络：实例探究（Deep convolutional models: case studies）(Course 4)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%BA%8C%E5%91%A8-%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%9A%E5%AE%9E%E4%BE%8B%E6%8E%A2%E7%A9%B6%EF%BC%88Deep-convolutional-models-case-studies%EF%BC%89-Course-4%2F</url>
    <content type="text"><![CDATA[2.1 经典网络（Classic networks）LeNet-5LeNet-5可以识别图中的手写数字，是针对灰度图片训练的，所以图片的大小只有32×32×1。该LeNet模型总共包含了大约6万个参数，典型的LeNet-5结构包含CONV layer，POOL layer和FC layer，顺序一般是CONV layer-&gt;POOL layer-&gt;CONV layer-&gt;POOL layer-&gt;FC layer-&gt;FC layer-&gt;OUTPUT layer，即\hat y： 随着网络越来越深，图像的高度和宽度在缩小，从最初的32×32缩小到28×28，再到14×14、10×10，最后只有5×5，通道数量一直在增加，从1增加到6个，再到16个 这个神经网络中还有一种模式就是一个或多个卷积层后面跟着一个池化层，然后又是若干个卷积层再接一个池化层，然后是全连接层，最后是输出 AlexNetAlexNet包含约6000万个参数。当用于训练图像和数据集时，AlexNet能够处理非常相似的基本构造模块，这些模块往往包含着大量的隐藏单元或数据，AlexNet比LeNet表现更为出色的另一个原因是它使用了ReLu激活函数 VGG-16VGG，也叫作VGG-16网络。VGG-16网络没有那么多超参数，是一种只需要专注于构建卷积层的简单网络。首先用3×3，步幅为1的过滤器构建卷积层，padding参数为same卷积中的参数。然后用一个2×2，步幅为2的过滤器构建最大池化层。VGG网络的一大优点是简化了神经网络结构 假设要识别这个图像，在最开始的两层用64个3×3的过滤器对输入图像进行卷积，输出结果是224×224×64，因为使用了same卷积，通道数量也一样 接下来创建一个池化层，池化层将输入图像进行压缩，减少到112×112×64。然后又是若干个卷积层，使用128个过滤器，以及一些same卷积，输出112×112×128。然后进行池化，池化后的结果是56×56×128。再用256个相同的过滤器进行三次卷积操作，然后再池化，然后再卷积三次，再池化。如此进行几轮操作后，将最后得到的7×7×512的特征图进行全连接操作，得到4096个单元，然后进行softmax激活，输出从1000个对象中识别的结果 VGG-16的数字16指在这个网络中有13个卷积层和3个全链接层 总共包含约1.38亿个参数，这种网络结构很规整，都是几个卷积层后面跟着可以压缩图像大小的池化层，池化层缩小图像的高度和宽度。同时，卷积层的过滤器数量变化存在一定的规律，由64翻倍变成128，再到256和512。主要缺点是需要训练的特征数量非常巨大 随着网络的加深，图像的高度和宽度都在以一定的规律不断缩小，每次池化后刚好缩小一半，而通道数量在不断增加，而且刚好也是在每组卷积操作后增加一倍。即图像缩小的比例和通道数增加的比例是有规律的 2.2 残差网络（Residual Networks (ResNets)）人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系，这种神经网络被称为Residual Networks(ResNets) Residual Networks由许多隔层相连的神经元子模块组成，称之为Residual block（残差块）。单个Residual block的结构如下图所示： 紫色线是skip connection（跳跃连接），直接建立a^{[l]}与a^{[l+2]}之间的隔层联系。相应的表达式如下： z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]} a^{[l+1]}=g(z^{[l+1]}) z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]} a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$a^{[l]}$直接隔层与下一层的线性输出相连，a^{[l]}插入的时机是在线性激活之后，ReLU激活之前，与z^{[l+2]}共同通过激活函数（ReLU）输出a^{[l+2]} 这种模型结构对于训练非常深的神经网络效果很好。非Residual Networks称为Plain Network Residual Network的结构 Plain Network 与Plain Network相比，Residual Network能够训练更深层的神经网络，有效避免发生发生梯度消失和梯度爆炸 随着神经网络层数增加，Plain Network实际性能会变差，training error甚至会变大 Residual Network的训练效果却很好，training error一直呈下降趋势 2.3 残差网络为什么有用？（Why ResNets work?） 输入X 经过一个大型神经网络输出激活值a^{[l]}，再给这个网络额外添加两层作为一个ResNets块，输出a^{\left\lbrack l + 2 \right\rbrack}： a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})假设在整个网络中使用ReLU激活函数，所以激活值都大于等于0，包括输入X的非零异常值。因为ReLU激活函数输出的数字要么是0，要么是正数 如果使用L2正则化或权重衰减，会压缩W^{\left\lbrack l + 2\right\rbrack}的值。W是关键项，如果W^{\left\lbrack l + 2 \right\rbrack} = 0，方便起见，假设b^{\left\lbrack l + 2 \right\rbrack} = 0，假定使用ReLU激活函数，并且所有激活值都是非负的，g\left(a^{[l]} \right)是应用于非负数的ReLU函数，所以a^{[l+2]} =a^{[l]} 可以看出，即使发生了梯度消失，W^{[l+2]}\approx0，b^{[l+2]}\approx0，也能直接建立a^{[l+2]}与a^{[l]}的线性关系，且a^{[l+2]}=a^{[l]}，这就是identity function（恒等函数）。a^{[l]}直接连到a^{[l+2]}，相当于直接忽略了a^{[l]}之后的这两层神经层。这样看似很深的神经网络，由于许多Residual blocks的存在，弱化削减了某些神经层之间的联系，实现隔层线性传递，而不是一味追求非线性关系，模型本身也就能“容忍”更深层的神经网络了。从性能上来说，这两层额外的Residual blocks也不会降低Big NN的性能，所以给大型神经网络增加两层，不论是把残差块添加到神经网络的中间还是末端位置，都不会影响网络的表现 如果Residual blocks确实能训练得到非线性关系，那么也会忽略short cut，跟Plain Network起到同样的效果 如果Residual blocks中a^{[l+2]}与a^{[l]}的维度不同，可以引入矩阵W_s与a^{[l]}相乘，使得W_s*a^{[l]}的维度与a^{[l+2]}一致 参数矩阵W_s有来两种方法得到： 将W_s作为学习参数，通过模型训练得到 固定W_s值（类似单位矩阵），不需要训练，W_s与a^{[l]}的乘积仅使得a^{[l]}截断或补零 CNN中ResNets的结构： ResNets同类型层之间： 例如CONV layers，大多使用same类型，这也解释了添加项z^{[l+2]}+a^{[l]}（维度相同所以能够相加） 如果是不同类型层之间的连接，例如CONV layer与POOL layer之间，如果维度不同，则引入矩阵W_s 2.4 网络中的网络以及 1×1 卷积（Network in Network and 1×1 convolutions）如果是一张6×6×32的图片，使用1×1过滤器进行卷积效果更好。1×1卷积所实现的功能是遍历这36个单元格，计算左图中32个数字和过滤器中32个数字的元素积之和，然后应用ReLU非线性函数 1×1×32过滤器中的32个数字可以理解为一个神经元的输入是32个数字，这32个数字具有不同通道，乘以32个权重（将过滤器中的32个数理解为权重），然后应用ReLU非线性函数，输出相应的结果 如果过滤器是多个，就好像有多个输入单元，其输入内容为一个切片上所有数字，输出结果是6×6×#filters 1×1卷积可以从根本上理解为对这32个不同的位置都应用一个全连接层，全连接层的作用是输入32个数字（过滤器数量标记为n_{C}^{[ l + 1]}​，在这36个单元上重复此过程）,输出结果是6×6×#filters（过滤器数量），以便在输入层上实施一个非平凡（non-trivial）计算 这种方法通常称为1×1卷积，也被称为Network in Network 假设一个28×28×192的输入层，如果通道数量很大，可以用32个大小为1×1×192的过滤器，使输出层为28×28×32，这就是压缩通道数（n_{C}​）的方法 如果想保持通道数192不变，也是可行的，1×1卷积只是添加了非线性函数，也可以让网络学习更复杂的函数 1×1卷积层给神经网络添加了一个非线性函数，从而减少或保持输入层中的通道数量不变，也可以增加通道数量 2.5 谷歌 Inception 网络简介（Inception network motivation）Inception网络或Inception层的作用是代替人工来确定卷积层中的过滤器类型，或者确定是否需要创建卷积层或池化层 基本思想是Inception网络在单层网络上可以使用多个不同尺寸的filters，进行same convolutions，把各filter下得到的输出拼接起来。还可以将CONV layer与POOL layer混合，同时实现各种效果，但是要注意使用same pool。Inception Network不需人为决定使用哪个过滤器或者是否需要池化，它使用不同尺寸的filters并将CONV和POOL混合起来，将所有功能输出组合拼接，再由神经网络本身去学习参数并选择最好的模块 Inception Network在提升性能的同时，会带来计算量大的问题： 乘法运算的总次数为每个输出值所需要执行的乘法运算次数（5×5×192）乘以输出值个数（28×28×32），结果等于1.2亿。 为此，引入1x1 Convolutions来减少计算量，对于输入层，使用1×1卷积把输入值从192个通道减少到16个通道。然后对这个较小层运行5×5卷积，得到最终输出 把该1x1 Convolution称为“瓶颈层”（bottleneck layer），瓶颈层是网络中最小的部分，即先缩小网络，然后再扩大 引入bottleneck layer之后，第一个卷积层计算成本：（1×1×192）×（28×28×16），相乘结果约等于240万，第二个卷积层的计算成本是：（28×28×32）×（5×5×16），计算结果为1000万，总次数是1204万，计算成本从1.2亿下降到了原来的十分之一 总结： 如果在构建神经网络层的时候，不想决定池化层是使用1×1，3×3还是5×5的过滤器，Inception模块是最好的选择。可以应用各种类型的过滤器，只需要把输出连接起来 计算成本问题，通过使用1×1卷积来构建瓶颈层，大大降低计算成本 只要合理构建瓶颈层，既可以显著缩小表示层规模，又不会降低网络性能，从而节省了计算 2.6 Inception 网络（Inception network）引入1x1 Convolution后的Inception module如下图所示： Inception模块会将之前层的激活或者输出作为它的输入，为了能在最后将这些输出都连接起来，会使用same类型的padding来池化，使得输出的高和宽依然是28×28，这样才能将它与其他输出连接起来。如果进行了最大池化，即便用了same padding，3×3的过滤器，stride为1，其输出将会是28×28×192，其通道数与输入（通道数）相同。要做的是再加上一个1×1的卷积层，将通道的数量缩小到28×28×32，避免了最后输出时，池化层占据所有的通道 最后把得到的各个层的通道都加起来，得到一个28×28×256的输出。这就是一个Inception模块 Inception网络只是很多在不同的位置重复组成的网络： 中间隐藏层也可以作为输出层Softmax，确保了即便是隐藏单元和中间层也参与了特征计算，也能预测图片的分类，起到一种调整的效果，有利于防止发生过拟合 2.7 迁移学习（Transfer Learning）训练集很小的情况： 建议：从网上下载一些神经网络开源的实现，不仅把代码下载下来，也把权重下载下来。然后去掉Softmax层，创建自己的Softmax单元，用来输出Tigger、Misty和neither三个类别。把所有的层看作是冻结的，冻结网络中所有层的参数，只需要训练和Softmax层有关的参数。这个Softmax层有三种可能的输出，Tigger、Misty或者Neither。 通过使用其他人预训练的权重，很可能得到很好的性能，即使只有一个小的数据集。大多数深度学习框架会有trainableParameter=0的参数，对于前面的层，可以设置这个参数。为了不训练这些权重，会有freeze=1的参数。只需要训练softmax层的权重，把前面这些层的权重都冻结 由于前面的层都冻结了，相当于一个固定的函数，因此不需要改变和训练它，取输入图像X，然后把它映射到softmax前一层的激活函数。能加速训练的技巧是如果先计算这一层（紫色箭头标记），计算特征或者激活值，然后把它们存到硬盘里。所做的就是用这个固定的函数，在这个神经网络的前半部分（softmax层之前的所有层视为一个固定映射），取任意输入图像X，然后计算它的某个特征向量，这样训练的就是一个很浅的softmax模型，用这个特征向量来做预测。对计算有用的一步就是对训练集中所有样本的这一层的激活值进行预计算，然后存储到硬盘里，在此之上训练softmax分类器。存储到硬盘或者说预计算方法的优点是不需要每次遍历训练集再重新计算这个激活值 更大的训练集：应该冻结更少的层，然后训练后面的层。如果输出层的类别不同，那么需要构建自己的输出单元，Tigger、Misty或者Neither三个类别。可以取后面几层的权重，用作初始化，然后从这里开始梯度下降 也可以直接去掉这几层，换成自己的隐藏单元和softmax输出层，如果有越来越多的数据，那么需要冻结的层数就越少，能够训练的层数就越多。如果有一个更大的数据集，那么不要单单训练一个softmax单元，而是考虑训练中等大小的网络，包含最终要用的网络的后面几层 如果有大量数据：应该做的就是用开源的网络和它的权重，把所有的权重当作初始化，然后训练整个网络 如果有越多的标定的数据，可以训练越多的层。极端情况下，可以用下载的权重只作为初始化，用它们来代替随机初始化，接着用梯度下降训练，更新网络所有层的所有权重 2.8 数据扩充（Data augmentation）当下计算机视觉的主要问题是没有办法得到充足的数据 最简单的数据扩充方法就是垂直镜像对称 另一个经常使用的技巧是随机裁剪，给定一个数据集，然后开始随机裁剪，得到不同的图片放在数据集中，随机裁剪并不是一个完美的数据扩充的方法，如果随机裁剪的那一部分（红色方框标记部分，编号4）看起来不像猫。但在实践中，这个方法还是很实用的，随机裁剪构成了很大一部分的真实图片 也可以使用旋转，剪切（仅水平或垂直坐标发生变化）图像，扭曲变形，引入很多形式的局部弯曲等等，但在实践中太复杂所以使用的很少 彩色转换：给R、G和B三个通道上加上不同的失真值 实践中对R、G和B的变化是基于某些分布，改变可能很小，R、G和B的值是根据某种概率分布来决定，这样会使得学习算法对照片的颜色更改更具鲁棒性 对R、G和B有不同的采样方式，其中一种影响颜色失真的算法是PCA，即主成分分析，PCA颜色增强的大概含义是，如果图片呈现紫色，即主要含有红色和蓝色，绿色很少，然后PCA颜色增强算法就会对红色和蓝色增减很多，绿色变化相对少一点，所以使总体的颜色保持一致 如果有特别大的训练数据，可以使用CPU线程，不停的从硬盘中读取数据，用CPU线程来实现失真变形，可以是随机裁剪、颜色变化，或者是镜像 同时CPU线程持续加载数据，然后实现任意失真变形，从而构成批数据或者最小批数据，这些数据持续的传输给其他线程或者其他的进程，然后开始训练，可以在CPU或者GPU上实现一个大型网络的训练 常用的实现数据扩充的方法是使用一个线程或者是多线程来加载数据，实现变形失真，然后传给其他的线程或者其他进程，来训练编号2和这个编号1，可以并行实现 在数据扩充过程中也有一些超参数，比如说颜色变化了多少，以及随机裁剪的时候使用的参数 2.9 计算机视觉现状（The state of computer vision） 大部分机器学习问题是介于少量数据和大量数据范围之间 语音识别有很大数量的数据 虽然现在图像识别或图像分类方面有相当大的数据集，但因为图像识别是一个复杂的问题，通过分析像素并识别出它是什么，即使在线数据集非常大，如超过一百万张图片，仍然希望能有更多的数据 物体检测拥有的数据更少 图像识别是如何看图片的问题，并且告诉你这张图是不是猫，而对象检测则是看一幅图，画一个框，告诉你图片里的物体，比如汽车等等。因为获取边框的成本比标记对象的成本更高，所以进行对象检测的数据往往比图像识别数据要少 当有很多数据时，倾向于使用更简单的算法和更少的手工工程，只要有一个大型的神经网络，甚至一个更简单的架构，就可以去学习它想学习的东西 当没有那么多的数据时，更多的是手工工程 对机器学习应用时，通常学习算法有两种知识来源： 一个来源是被标记的数据，像(x,y)应用在监督学习 第二个来源是手工工程，有很多方法去建立一个手工工程系统，它可以是源于精心设计的特征，手工精心设计的网络体系结构或者是系统的其他组件。当没有太多标签数据时，只需要更多地考虑手工工程 在基准研究和比赛中，下面的tips可能会有较好的表现： 集成，意味着想好了要的神经网络之后，可以独立训练几个神经网络，并平均它们的输出。比如说随机初始化三个、五个或者七个神经网络，然后训练所有这些网络，对输出\hat y进行平均计算，而不要平均权重，可能会在基准上提高1%，2%或者更好。但因为集成意味着要对每张图片进行测试，可能需要从3到15个不同的网络中运行一个图像，会让运行时间变慢 Multi-crop at test time，Multi-crop是一种将数据扩充应用到测试图像中的一种形式，在测试图片的多种版本上运行分类器，输出平均结果 如把猫的图片复制四遍，包括两个镜像版本。如取中心的crop，然后取四个角落的crop，通过分类器来运行它 编号1和编号3是中心crop，编号2和编号4是四个角落的crop。把这些加起来会有10种不同的图像的crop，命名为10-crop。通过分类器来运行这十张图片，然后对结果进行平均 集成的一个大问题是需要保持所有这些不同的神经网络，占用了更多的计算机内存 multi-crop，只保留一个网络，不会占用太多的内存，但仍然会让运行时间变慢]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周 卷积神经网络（Foundations of Convolutional Neural Networks）(Course 4)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%B8%80%E5%91%A8-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Foundations-of-Convolutional-Neural-Networks%EF%BC%89-Course-4%2F</url>
    <content type="text"><![CDATA[1.1 计算机视觉（Computer vision）图片分类，或图片识别： 目标检测： 神经网络实现图片风格迁移： 使用传统神经网络处理机器视觉的一个主要问题是输入层维度很大。例如一张64x64x3的图片，神经网络输入层的维度为12288。如果图片尺寸较大，例如一张1000x1000x3的图片，神经网络输入层的维度将达到3百万，使得网络权重W非常庞大。这样会造成两个后果，一是神经网络结构复杂，数据量相对不够，容易出现过拟合；二是所需内存、计算量较大。解决这一问题的方法就是使用卷积神经网络（CNN）。 1.2边缘检测示例（Edge detection example）对于CV问题，神经网络由浅层到深层，分别可以检测出图片的边缘特征 、局部特征（例如眼睛、鼻子等）、整体面部轮廓 图片的边缘检测最常检测的图片边缘有两类：一是垂直边缘（vertical edges），二是水平边缘（horizontal edges） 图片的边缘检测可以通过与相应滤波器进行卷积来实现。以垂直边缘检测为例，原始图片尺寸为6x6，滤波器filter尺寸为3x3，卷积后的图片尺寸为4x4，得到结果如下： ∗表示卷积操作。python中，卷积用conv_forward()表示；tensorflow中，卷积用tf.nn.conv2d()表示；keras中，卷积用Conv2D()表示 垂直边缘是一个3×3的区域，左边是明亮的像素，中间的并不需要考虑，右边是深色像素。在这个6×6图像的中间部分，明亮的像素在左边，深色的像素在右边，就被视为一个垂直边缘 ] 1.3 更多边缘检测内容（More edge detection）图片边缘有两种渐变方式，一种是由明变暗，另一种是由暗变明。实际应用中，这两种渐变方式并不影响边缘检测结果，可以对输出图片取绝对值操作，得到同样的结果 由亮向暗 由暗向亮 下图的垂直边缘过滤器是一个3×3的区域，左边相对较亮，右边相对较暗。右图的水平边缘过滤器也是一个3×3的区域，上边相对较亮，而下方相对较暗 30（右边矩阵中绿色方框标记元素）代表了左边这块3×3的区域（左边矩阵绿色方框标记部分），这块区域是上边比较亮，下边比较暗，所以它在这里发现了一条正边缘。而-30（右边矩阵中紫色方框标记元素）代表了左边另一块区域（左边矩阵紫色方框标记部分），这块区域是底部比较亮，而上边则比较暗，所以在这里它是一条负边 10（右边矩阵中黄色方框标记元素）代表的是左边这块区域（左边6×6矩阵中黄色方框标记的部分）。这块区域左边两列是正边，右边一列是负边，正边和负边的值加在一起得到了一个中间值。但假如这是一个非常大的1000×1000大图，就不会出现亮度为10的过渡带了，因为图片尺寸很大，这些中间值就会变得非常小 对于这个3×3的过滤器来说，使用了其中的一种数字组合： 还可以使用这种： \begin{bmatrix}1 & 0 & - 1 \\ 2 & 0 & - 2 \\ 1 & 0 & - 1 \end{bmatrix}叫做Sobel过滤器，优点在于增加了中间一行元素的权重，使得结果的鲁棒性会更高一些 或者： \begin{bmatrix} 3& 0 & - 3 \\ 10 & 0 & - 10 \\ 3 & 0 & - 3 \end{bmatrix}叫做Scharr过滤器，也是一种垂直边缘检测，如果将其翻转90度，就能得到对应水平边缘检测 随着深度学习的发展，如果想检测图片的各种边缘特征，而不仅限于垂直边缘和水平边缘，那么filter的数值一般需要通过模型训练得到，将矩阵的所有数字都设置为参数，通过数据反馈，让神经网络自动去学习它们，类似于标准神经网络中的权重W一样由梯度下降算法反复迭代求得，会发现神经网络可以学习一些低级的特征，例如这些边缘的特征。CNN的主要目的就是计算出这些filter的数值，确定得到了这些filter后，CNN浅层网络也就实现了对图片所有边缘特征的检测 1.4 Padding 如果有一个n\times n的图像，用f\times f的过滤器做卷积，输出的维度就是(n-f+1)\times (n-f+1) 这样的话会有两个缺点: 每次做卷积操作，输出图片尺寸缩小 原始图片边缘信息对输出贡献得少，输出图片丢失边缘信息 角落边缘的像素（绿色阴影标记）只被一个输出所触碰或者使用，中间的像素点（红色方框标记）会有许多3×3的区域与之重叠。角落或者边缘区域的像素点在输出中采用较少，丢掉了图像边缘位置的许多信息 可以在卷积操作之前填充这幅图像。沿着图像边缘再填充一层像素,6×6的图像填充成8×8的图像。就得到了一个尺寸和原始图像6×6的图像。习惯上，可以用0去填充，如果p是填充的数量，输出也就变成了(n+2p-f+1)\times (n+2p-f+1)。涂绿的像素点（左边矩阵）影响了输出中的这些格子（右边矩阵）。这样角落或图像边缘的信息发挥的作用较小的这一缺点就被削弱了 选择填充多少像素，通常有两个选择，分别叫做Valid卷积和Same卷积 Valid卷积意味着不填充，如果有一个n\times n的图像，用一个f\times f的过滤器卷积，会给一个(n-f+1)\times (n-f+1)维的输出 另一个叫做Same卷积，填充后输出大小和输入大小是一样的。由n-f+1，当填充p个像素点，n就变成了n+2p，公式变为： n+2p-f+1即： p=\frac{f-1}{2}当f是一个奇数，只要选择相应的填充尺寸就能确保得到和输入相同尺寸的输出 计算机视觉中，f通常是奇数，有两个原因： 如果f是偶数，只能使用一些不对称填充 当有一个奇数维过滤器，比如3×3或者5×5的，它就有一个中心点，便于指出过滤器的位置 1.5 卷积步长（Strided convolutions）Stride表示filter在原图片中水平方向和垂直方向每次的步进长度。之前默认stride=1。若stride=2，则表示filter每次步进长度为2，即隔一点移动一次 用s表示stride长度，p表示padding长度，如果原始图片尺寸为n x n，filter尺寸为f x f，则卷积后的图片尺寸为： \lfloor\frac{n+2p-f}{s}+1\rfloor\ \times\ \lfloor\frac{n+2p-f}{s}+1\rfloor真正的卷积运算会先将filter绕其中心旋转180度，然后再将旋转后的filter在原始图片上进行滑动计算。filter旋转如下所示： 相关系数的计算过程则不会对filter进行旋转，而是直接在原始图片上进行滑动计算 目前为止介绍的CNN卷积实际上计算的是相关系数，而不是数学意义上的卷积。为了简化计算，一般把CNN中的这种“相关系数”就称作卷积运算。之所以可以这么等效，是因为滤波器算子一般是水平或垂直对称的，180度旋转影响不大；而且最终滤波器算子需要通过CNN网络梯度下降算法计算得到，旋转部分可以看作是包含在CNN模型算法中。忽略旋转运算可以大大提高CNN网络运算速度，而且不影响模型性能。 卷积运算服从分配律： (A*B)*C=A*(B*C)1.6三维卷积（Convolutions over volumes）3通道的RGB图片对应的滤波器算子也是3通道的。例如一个图片是6 x 6 x 3，分别表示图片的高度（height）、宽度（weight）和通道（#channel） 3通道图片的卷积运算与单通道图片的卷积运算基本一致。过程是将每个单通道（R，G，B）与对应的filter进行卷积运算求和，然后再将3通道的和相加，得到输出图片的一个像素值 不同通道的滤波算子可以不相同。例如R通道filter实现垂直边缘检测，G和B通道不进行边缘检测，全部置零，或者将R，G，B三通道filter全部设置为水平边缘检测 为了进行多个卷积运算，实现更多边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。做完卷积，然后把这两个4×4的输出堆叠在一起，第一个放到前面，第二个放到后面，就得到一个4×4×2的输出立方体 不同滤波器组卷积得到不同的输出，个数由滤波器组决定 若输入图片的尺寸为n x n xn_c，filter尺寸为f x f x n_c，则卷积后的图片尺寸为(n-f+1) x (n-f+1) x {n}'_c(默认padding为1）。n_c为图片通道数目，{n}'_c为滤波器组个数 1.7单层卷积网络（One layer of a convolutional network）卷积神经网络的单层结构如下所示： 相比之前的卷积过程，CNN的单层结构多了激活函数ReLU和偏移量b。整个过程与标准的神经网络单层结构非常类似： Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} A^{[l]}=g^{[l]}(Z^{[l]})卷积运算对应着上式中的乘积运算，滤波器组数值对应着权重W^{[l]}，所选的激活函数为ReLU 每个滤波器组有3x3x3=27个参数，还有1个偏移量b，则每个滤波器组有27+1=28个参数，两个滤波器组总共包含28x2=56个参数。选定滤波器组后，参数数目与输入图片尺寸无关。所以不存在由于图片尺寸过大，造成参数过多的情况，这就是卷积神经网络的一个特征，叫作“避免过拟合”。例如一张1000x1000x3的图片，标准神经网络输入层的维度将达到3百万，而在CNN中，参数数目只由滤波器组决定，数目相对来说要少得多，这是CNN的优势之一 设层数为l，CNN单层结构的所有标记符号： f^{[l]}= filter size p^{[l]}= padding s^{[l]}= stride n_c^{[l]}= number of filters 输入维度为：n_H^{[l-1]}\times n_W^{[l-1]}\times n_c^{[l-1]}，因为是上一层的激活值每个滤波器组维度为：f^{[l]}\times f^{[l]}\times n_c^{[l-1]} 权重维度为：f^{[l]}\times f^{[l]}\times n_c^{[l-1]}\times n_c^{[l]} 偏置维度为：1 \times 1\times 1 \times n_c^{[l]} 输出维度为：n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]} 其中： n_H^{[l]}=\lfloor \frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \rfloor n_W^{[l]}=\lfloor \frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \rfloor如果有m个样本，进行向量化运算，相应的输出维度为： m \times n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}1.8 简单卷积网络示例（A simple convolution network example）简单的CNN网络模型： a^{[3]}$$的维度是7 x 7 x 40，将$$a^{[3]}$$排列成1列，维度为1960 x 1，然后连接最后一级输出层。输出层可以是一个神经元，即二元分类（logistic）；也可以是多个神经元，即多元分类（softmax）。最后得到预测输出$$\hat y随着CNN层数增加，n_H^{[l]}和n_W^{[l]}一般逐渐减小，而n_c^{[l]}一般逐渐增大 CNN有三种类型的layer： Convolution层（CONV） Pooling层（POOL） Fully connected层（FC） CONV最为常见也最重要 1.9 池化层（Pooling layers）Pooling layers是CNN中用来减小尺寸，提高运算速度的，同样能减小noise影响，让各特征更具有健壮性 Pooling layers没有卷积运算，仅在滤波器算子滑动区域内取最大值，即max pooling，这是最常用的做法。超参数p很少在pooling layers中使用 Max pooling的好处是只保留区域内的最大值（特征），数字大意味着可能探测到了某些特定的特征，忽略了其它值，降低了noise影响，提高了模型健壮性。max pooling需要的超参数仅为滤波器尺寸f和滤波器步进长度s，没有其他参数需要模型训练得到，计算量很小 如果是多个通道，每个通道单独进行max pooling操作： average pooling是在滤波器算子滑动区域计算平均值： 实际应用中，max pooling比average pooling更为常用，也有例外，深度很深的神经网络可以用平均池化来分解规模为7×7×1000的网络的表示层，在整个空间内求平均值，得到1×1×1000 总结： 池化的超级参数包括过滤器大小f和步幅s，常用的参数值为f=2，s=2，应用频率非常高，其效果相当于高度和宽度缩减一半。最大池化时，往往很少用到超参数padding，p最常用的值是0，即p=0。最大池化的输入就是： n_{H} \times n_{W} \times n_{c}假设没有padding，则输出： \lfloor\frac{n_{H} - f}{s} +1\rfloor \times \lfloor\frac{n_{w} - f}{s} + 1\rfloor \times n_{c}输入通道与输出通道个数相同，因为对每个通道都做了池化。最大池化只是计算神经网络某一层的静态属性，池化过程中没有需要学习的参数。执行反向传播时，反向传播没有参数适用于最大池化 1.10 卷积神经网络示例（Convolutional neural network example）简单的数字识别CNN例子： CONV层后面紧接一个POOL层，CONV1和POOL1构成第一层，CONV2和POOL2构成第二层。FC3和FC4为全连接层FC，跟标准的神经网络结构一致。最后的输出层（softmax）由10个神经元构成 整个网络各层的尺寸和参数如下表格所示： 池化层和最大池化层没有参数；卷积层的参数相对较少，许多参数都存在于神经网络的全连接层。随着神经网络的加深，激活值尺寸会逐渐变小，如果激活值尺寸下降太快，也会影响神经网络性能 尽量不要自己设置超参数，而是查看文献中别人采用了哪些超参数，选一个在别人任务中效果很好的架构，也可能适用于自己的应用程序 在神经网络中，另一种常见模式就是一个或多个卷积后面跟随一个池化层，然后一个或多个卷积层后面再跟一个池化层，然后是几个全连接层，最后是一个softmax 1.11 为什么使用卷积？（Why convolutions?）和只用全连接层相比，卷积层的两个主要优势在于参数共享和稀疏连接 如果这是一张1000×1000的图片，权重矩阵会变得非常大。而卷积层的参数数量：每个过滤器都是5×5，一个过滤器有25个参数，再加上偏差参数，那么每个过滤器就有26个参数，一共有6个过滤器，所以参数共计156个，参数数量很少 卷积网络映射这么少参数有两个原因： 参数共享：一个特征检测器（例如垂直边缘检测）对图片某块区域有用，同时也可能作用在图片其它区域。 特征检测如垂直边缘检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。如果用一个3×3的过滤器检测垂直边缘，那么图片的左上角区域，以及旁边的各个区域（左边矩阵中蓝色方框标记的部分）都可以使用这个3×3的过滤器。每个特征检测器以及输出都可以在输入图片的不同区域中使用同样的参数，以便提取垂直边缘或其它特征。它不仅适用于边缘特征这样的低阶特征，同样适用于高阶特征，例如提取脸上的眼睛，猫或者其他特征对象。即使减少参数个数，这9个参数同样能计算出16个输出。直观感觉是，一个特征检测器，如垂直边缘检测器用于检测图片左上角区域的特征，这个特征很可能也适用于图片的右下角区域。因此在计算图片左上角和右下角区域时，不需要添加其它特征检测器 连接的稀疏性：因为滤波器算子尺寸限制，每一层的每个输出只与输入部分区域内有关 右边输出单元（元素0）仅与36个输入特征中9个相连接。其它像素值都不会对输出产生任何影响，输出（右边矩阵中红色标记的元素 30）仅仅依赖于这9个特征（左边矩阵红色方框标记的区域），只有这9个输入特征与输出相连接，其它像素对输出没有任何影响 神经网络可以通过这两种机制减少参数，以便用更小的训练集来训练它，从而预防过拟合。CNN比较擅长捕捉区域位置偏移，也就是说CNN进行物体检测时，不太受物体所处图片位置的影响，增加检测的准确性和系统的健壮性。通过观察可以发现，向右移动两个像素，图片中的猫依然清晰可见，因为神经网络的卷积结构使得即使移动几个像素，这张图片依然具有非常相似的特征，应该属于同样的输出标记 最后，把这些层整合起来，比如要构建一个猫咪检测器，x表示一张图片，\hat{y}是二进制标记或某个重要标记。选定一个卷积神经网络，输入图片，增加卷积层和池化层，然后添加全连接层，并随机初始化参数w和b，最后输出一个softmax，即\hat{y}，代价函数J等于神经网络对整个训练集的预测的损失总和再除以m（即\text{Cost} J = \frac{1}{m}\sum_{i = 1}^{m}{L(\hat{y}^{(i)},y^{(i)})}）。所以训练神经网络，要做的就是使用梯度下降法，或其它算法，例如Momentum梯度下降法，含RMSProp或其它因子的梯度下降来优化神经网络中所有参数，以减少代价函数J的值]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周：机器学习策略（2）(ML Strategy (2))(Course 3)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%EF%BC%882%EF%BC%89-ML-Strategy-2-Course-2%2F</url>
    <content type="text"><![CDATA[2.1 进行误差分析（Carrying out error analysis）如果希望让学习算法能够胜任人类能做的任务，但学习算法还没有达到人类的表现，那么人工检查一下算法犯的错误可以了解接下来应该做什么，这个过程称为错误分析假设正在调试猫分类器，取得了90%准确率，相当于10%错误，注意到算法将一些狗分类为猫，需要对模型的一些部分做相应调整，才能更好地提升分类的精度 收集错误样例： 在开发集（测试集）中，获取大约100个错误标记的例子，然后手动检查，一次只看一个，看看开发集里有多少错误标记的样本是狗 100个数据中有5个样例是狗，如果对数据集的错误标记做努力去改进模型的精度，可以提升的上限是5%，仅可以达到9.5%的错误率，称为性能上限（ceiling on performance）。这种情况下，这样耗时的努力方向不是很值得的事 100个数据中，有50多个样例是狗，改进数据集的错误标记是一个值得的改进方向，可以将模型的精确度提升至95 并行分析： 修改那些被分类成猫的狗狗图片标签 修改那些被错误分类的大型猫科动物，如：狮子，豹子等 提升模糊图片的质量 为了并行的分析，可以建立表格来进行。在最左边，人工过一遍想分析的图像集，电子表格的每一列对应要评估的想法，如狗的问题，猫科动物的问题，模糊图像的问题，最后一列写评论 在这个步骤做到一半时，可能会发现其他错误类型，比如可能发现有Instagram滤镜，那些花哨的图像滤镜，干扰了分类器。在这种情况下可以在错误分析途中，增加一列多色滤镜 Instagram滤镜和Snapchat滤镜，再过一遍，并确定新的错误类型百分比，这个分析步骤的结果可以给出一个估计，是否值得去处理每个不同的错误类型 可以把团队可以分成两个团队，其中一个改善大猫的识别，另一个改善模糊图片的识别 总结： 进行错误分析，应该找一组错误样本，可能在开发集或者测试集，观察错误标记的样本，看看假阳性（false positives）和假阴性（false negatives），统计属于不同错误类型的错误数量。在这个过程中，可能会得到启发，归纳出新的错误类型，通过统计不同错误标记类型的百分比，可以发现哪些问题需要优先解决 2.2 清楚标注错误的数据（Cleaning up Incorrectly labeled data）监督学习问题的数据由输入x和输出标签y 构成，如果发现有些输出标签 y 是错的，是否值得花时间去修正这些标签？ 倒数第二不是猫，是标记错误的样本。“标记错误的样本”表示学习算法输出了错误的 y 值，如果数据有一些标记错误的样本，该怎么办？ 训练集：深度学习算法对于训练集中的随机错误是相当健壮的（robust）。只要这些错误样本离随机错误不太远，有时可能做标记的人没有注意或者不小心，按错键了，如果错误足够随机，放着这些错误不管可能也没问题，而不要花太多时间修复它们，只要总数据集足够大，实际错误率可能不会太高 深度学习算法对随机误差很健壮，但对系统性的错误没那么健壮。如果做标记的人一直把白色的狗标记成猫，那就成问题。因为分类器学习之后，会把所有白色的狗都分类为猫。但随机错误或近似随机错误，对于大多数深度学习算法来说不成问题 开发集和测试集有标记出错的样本：在错误分析时，添加一个额外的列，统计标签 y=1错误的样本数。统计因为标签错误所占的百分比，解释为什么学习算法做出和数据集的标记不一样的预测1 是否值得修正6%标记出错的样本： 如果标记错误严重影响了在开发集上评估算法的能力，应该去花时间修正错误的标签 如果没有严重影响到用开发集评估成本偏差的能力，不应该花时间去处理 看3个数字来确定是否值得去人工修正标记出错的数据： 看整体的开发集错误率，系统达到了90%整体准确度，10%错误率，应该看错误标记引起的错误的数量或者百分比。6％的错误来自标记出错，10%的6%是0.6%，剩下的占9.4%，是其他原因导致的，比如把狗误认为猫，大猫图片。即有9.4%错误率需要集中精力修正，而标记出错导致的错误是总体错误的一小部分而已，应该看其他原因导致的错误 错误率降到了2％，但总体错误中的0.6%还是标记出错导致的。修正开发集里的错误标签更有价值 开发集的主要目的是从两个分类器A和B中选择一个。当测试两个分类器A和B时，在开发集上一个有2.1%错误率，另一个有1.9%错误率，但是不能再信任开发集，因为它无法告诉你这个分类器是否比这个好，因为0.6%的错误率是标记出错导致的。现在就有很好的理由去修正开发集里的错误标签，因为右边这个样本标记出错对算法错误的整体评估标准有严重的影响，而左边相对较小 如果决定要去修正开发集数据，手动重新检查标签，并尝试修正一些标签，这里还有一些额外的方针和原则需要考虑： 不管用什么修正手段，都要同时作用到开发集和测试集上，开发和测试集必须来自相同的分布。开发集确定了目标，当击中目标后，希望算法能够推广到测试集上，这样能够更高效的在来自同一分布的开发集和测试集上迭代 如果打算修正开发集上的部分数据，最好也对测试集做同样的修正以确保它们继续来自相同的分布。可以让一个人来仔细检查这些标签，但必须同时检查开发集和测试集 要同时检验算法判断正确和判断错误的样本，如果只修正算法出错的样本，算法的偏差估计可能会变大，会让算法有一点不公平的优势 修正训练集中的标签相对没那么重要，如果训练集来自稍微不同的分布，对于这种情况学习算法其实相当健壮，通常是一件很合理的事情 几个建议： 构造实际系统时，需要更多的人工错误分析，更多的人类见解来架构这些系统 搭建机器学习系统时，花时间亲自检查数据非常值得，可以帮你找到需要优先处理的任务，然后确定应该优先尝试哪些想法，或者哪些方向 2.3 快速搭建你的第一个系统，并进行迭代（Build your first system quickly, then iterate）如果正在开发全新的机器学习应用，应该尽快建立第一个系统原型，然后快速迭代 改进语音识别系统特定的技术: 对于几乎所有的机器学习程序可能会有50个不同的方向可以前进，并且每个方向都是相对合理的可以改善系统。但挑战在于如何选择一个方向集中精力处理。如果想搭建全新的机器学习程序，就是快速搭好第一个系统，然后开始迭代。首先快速设立开发集和测试集还有指标，决定目标所在，如果目标定错，之后改也可以。但一定要设立某个目标，然后马上搭好一个机器学习系统原型，找到训练集训练一下，看算法表现如何，在开发集测试集，评估指标表现如何。当建立第一个系统后，就可以马上用到偏差方差分析和错误分析，来确定下一步优先做什么。如果错误分析到大部分的错误来源是说话人远离麦克风，就有很好的理由去集中精力研究这些技术，所谓远场语音识别的技术，就是处理说话人离麦克风很远的情况 建立初始系统所有意义：是一个快速和粗糙的实现（quick and dirty implementation），有一个学习过的系统，有一个训练过的系统，确定偏差方差的范围，知道下一步应该优先做什么，能够进行错误分析，观察一些错误，然后想出所有能走的方向，哪些是实际上最有希望的方向 当这个领域有很多可以借鉴的学术文献，处理的问题和要解决的几乎完全相同，比如人脸识别有很多学术文献，如果搭建一个人脸识别设备，可以从现有大量学术文献为基础出发，一开始就搭建比较复杂的系统。但如果第一次处理某个新问题，还是构建一些快速而粗糙的实现，然后用来找到改善系统要优先处理的方向 2.4 在不同的划分上进行训练并测试（Training and testing on different distributions）猫咪识别假设只收集到10,000张用户上传的照片和超过20万张网上下载的高清猫图： 做法一：将两组数据合并在一起，把这21万张照片随机分配到训练、开发和测试集中。假设已经确定开发集和测试集各包含2500个样本，训练集有205000个样本。 好处：训练集、开发集和测试集都来自同一分布 坏处：开发集的2500个样本中很多图片都来自网页下载的图片，并不是真正关心的数据分布，因为真正要处理的是来自手机的图片 2500个样本有2500\times \frac{200k}{210k} =2381张图来自网页下载，平均只有119张图来自手机上传。设立开发集的目的是告诉团队去瞄准的目标，而瞄准目标的大部分精力却都用在优化来自网页下载的图片 建议：开发集和测试集都是2500张来自应用的图片，训练集包含来自网页的20万张图片还有5000张来自应用的图片，现在瞄准的目标就是想要处理的目标，才是真正关心的图片分布 语音激活后视镜假设有很多不是来自语音激活后视镜的数据 分配： 训练集500k段语音，开发集和测试集各包含10k段语音（从实际的语音激活后视镜收集） 也可以拿一半放训练集里，训练集51万段语音，开发集和测试集各5000 2.5 不匹配数据划分的偏差和方差（Bias and Variance with mismatched data distributions）当训练集和开发集、测试集不同分布时，分析偏差和方差的方式： 分析的问题在于，当看训练误差，再看开发误差，有两件事变了，很难确认这增加的9%误差率有多少是因为： 算法只见过训练集数据，没见过开发集数据（方差） 开发集数据来自不同的分布 为了弄清楚哪个因素影响更大，定义一组新的数据，称之为训练-开发集，是一个新的数据子集。从训练集的分布里分出来，但不会用来训练网络 随机打散训练集，分出一部分训练集作为训练-开发集（training-dev），训练集、训练-开发集来自同一分布 只在训练集训练神经网络，不让神经网络在训练-开发集上跑后向传播。为了进行误差分析，应该看分类器在训练集上的误差、训练-开发集上的误差、开发集上的误差 假设训练误差是1%，训练-开发集上的误差是9%，开发集误差是10%，存在方差，因为训练-开发集的错误率是在和训练集来自同一分布的数据中测得的，尽管神经网络在训练集中表现良好，但无法泛化到来自相同分布的训练-开发集 假设训练误差为1%，训练-开发误差为1.5%，开发集错误率10%。方差很小，当转到开发集时错误率大大上升，是数据不匹配的问题 如果训练集误差是10%，训练-开发误差是11%，开发误差为12%，人类水平对贝叶斯错误率的估计大概是0%，存在可避免偏差问题 如果训练集误差是10%，训练-开发误差是11%，开发误差是20%，有两个问题 可避免偏差问题 数据不匹配问题 如果加入测试集错误率，而开发集表现和测试集表现有很大差距，可能对开发集过拟合，需要一个更大的开发集 如果人类的表现是4%，训练错误率是7%，训练-开发错误率是10%。开发集是6%。可能开发测试集分布比实际处理的数据容易得多，错误率可能会下降 Human level 4%和Training error 7%衡量了可避免偏差大小，Training error 7%和Training-dev error 10%衡量了方差大小，Training-dev error 10%和Dev/Test dev 6%衡量了数据不匹配问题的大小 rearview mirror speech data 6%和Error on examples trained on 6%：获得这个数字的方式是让一些人标记他们的后视镜语音识别数据，看看人类在这个任务里能做多好，然后收集一些后视镜语音识别数据，放在训练集中，让神经网络去学习，测量那个数据子集上的错误率，如果得到rearview mirror speech data 6%和Error on examples trained on 6%，说明在后视镜语音数据上达到人类水平 General speech recognition Human level 4%和rearview mirror speech data 6%：说明后视镜的语音数据比一般语音识别更难，因为人类都有6%的错误，而不是4%的错误 总结： 开发集、测试集不同分布： 可以提供更多训练数据，有助于提高学习算法的性能 潜在问题不只是偏差和方差问题，还有数据不匹配 2.6 定位数据不匹配（Addressing data mismatch）解决train set与dev/test set样本分布不一致的两条建议： 为了让训练数据更接近开发集，可以人工合成数据（artificial data synthesis）。例如说话人识别问题，实际应用场合（dev/test set）是包含背景噪声的，而训练样本train set很可能没有背景噪声。为了让train set与dev/test set分布一致，可以在train set上人工添加背景噪声，合成类似实际场景的声音。这样会让模型训练的效果更准确。但是不能给每段语音都增加同一段背景噪声，会出现对背景噪音过拟合，这就是人工数据合成需要注意的地方 研发无人驾驶汽车，用计算机合成图像 如果只合成这些车中很小的子集，学习算法可能会对合成的这一个小子集过拟合 2.7 迁移学习（Transfer learning）将已经训练好的模型的一部分知识（网络结构）直接应用到另一个类似模型中去。比如已经训练好一个猫类识别的神经网络模型，直接把该模型中的一部分网络结构应用到使用X光片预测疾病的模型中去，这种学习方法被称为迁移学习（Transfer Learning） 如果已经有一个训练好的神经网络用来做图像识别。想要构建另一个X光片进行诊断的模型。迁移学习的做法是无需重新构建新的模型，而是利用之前的神经网络模型，只改变样本输入、输出以及输出层的权重系数W^{[L]},\ b^{[L]}，即对新的样本(X,Y)，重新训练输出层权重系数W^{[L]},\ b^{[L]}，其它层所有的权重系数W^{[L]},\ b^{[L]}保持不变 如果需要构建新模型的样本数量较少，可以只训练输出层的权重系数W^{[L]},\ b^{[L]}，保持其它层所有的权重系数W^{[l]},\ b^{[l]}不变 如果样本数量足够多，可以只保留网络结构，重新训练所有层的权重系数。这种做法使得模型更加精确，因为样本对模型的影响最大 择哪种方法通常由数据量决定 如果重新训练所有权重系数，初始W^{[l]},\ b^{[l]}由之前的模型训练得到，这一过程称为pre-training。之后，不断调试、优化W^{[l]},\ b^{[l]}的过程称为fine-tuning。pre-training和fine-tuning分别对应上图中的黑色箭头和红色箭头 迁移学习能这么做的原因是神经网络浅层部分能够检测出许多图片固有特征，例如图像边缘、曲线等。使用之前训练好的神经网络部分结果有助于更快更准确地提取X光片特征。二者处理的都是图片，而图片处理是有相同的地方，第一个训练好的神经网络已经实现如何提取图片有用特征。即便是即将训练的第二个神经网络样本数目少，仍然可以根据第一个神经网络结构和权重系数得到健壮性好的模型 迁移学习可以保留原神经网络的一部分，再添加新的网络层，可以去掉输出层后再增加额外一些神经层 迁移学习的应用场合主要包括三点： Task A and B have the same input x. You have a lot more data for Task A than Task B. Low level features from A could be helpful for learning B. 2.8 多任务学习（Multi-task learning）在迁移学习中，步骤是串行的，从任务A里学习然后只是迁移到任务B。在多任务学习中是同时开始学习的，试图让单个神经网络同时做几件事情，希望每个任务都能帮到其他所有任务 假设无人驾驶需要同时检测行人、车辆、停车标志，还有交通灯各种其他东西 如果输入图像x^{(i)}，那么 y^{(i)}不再是一个标签，而是有4个标签。在这个例子中，没有行人，有一辆车，有一个停车标志，没有交通灯。所以 y^{(i)}是个4×1向量。将训练集的标签水平堆叠起来，从y^{(1)}一直到y^{(m)}： \begin{bmatrix} \vdots & \vdots & \vdots & \vdots & \vdots\\ y^{(1)} & y^{(2)} & y^{(3)} & \cdots & y^{(m)}\\ \vdots & \vdots & \vdots & \vdots & \vdots \end{bmatrix}矩阵Y变成4\times m矩阵 输出四个节点，第一个节点是预测图中有没有行人，第二个预测有没有车，第三预测有没有停车标志，第四预测有没有交通灯，所以\hat y是四维 整个训练集的平均损失： \frac{1}{m}\sum_{i = 1}^{m}{\sum_{j = 1}^{4}{L(\hat y_{j}^{(i)},y_{j}^{(i)})}}$\sum_{j = 1}^{4}{L(\hat y_{j}^{(i)},y_{j}^{(i)})}$是单个预测的损失，所以这是对四个分量的求和，行人、车、停车标志、交通灯，标志L指的是logistic损失： L(\hat y_{j}^{(i)},y_{j}^{(i)}) = - y_{j}^{(i)}\log\hat y_{j}^{(i)} - (1 - y_{j}^{(i)})log(1 - \hat y_{j}^{(i)})Multi-task learning与Softmax regression的区别在于： Multi-task learning是multiple labels的，即输出向量y可以有多个元素为1 Softmax regression是single label的，即输出向量y只有一个元素为1 神经网络一些早期特征，在识别不同物体时都会用到，训练一个神经网络做四件事情会比训练四个完全独立的神经网络分别做四件事性能要更好 多任务学习也可以处理图像只有部分物体被标记的情况。比如没有标记是否有停车标志，或者是否有交通灯。也许有些样本都有标记，有些样本只标记了有没有车，然后还有一些是问号 即使是这样的数据集，也可以在上面训练算法，同时做四个任务，即使一些图像只有一小部分标签，其他是问号。训练算法的方式是对j从1到4只对带0和1标签的j值求和，当有问号就在求和时忽略那个项 多任务学习当三件事为真时有意义的： 训练的一组任务，可以共用低层次特征。对于无人驾驶的例子，同时识别交通灯、汽车和行人是有道理的，这些物体有相似的特征 如果每个任务的数据量很接近，这个准则没那么绝对，不一定对 想要从多任务学习得到很大性能提升，其他任务加起来必须要有比单个任务大得多的数据量 多任务学习会降低性能的唯一情况是神经网络还不够大。但如果可以训练一个足够大的神经网络，多任务学习肯定不会或者很少会降低性能 在实践中，多任务学习的使用频率要低于迁移学习。因为很难找到那么多相似且数据量对等的任务可以用单一神经网络训练。不过在计算机视觉领域，物体检测这个例子是最显著的例外情况 2.9 什么是端到端的深度学习？（What is end-to-end deep learning?）以前有一些数据处理系统或者学习系统需要多个阶段的处理。端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它 语音识别目标是输入x，比如说一段音频，然后把它映射到一个输出y，就是这段音频的听写文本: 传统上语音识别需要很多阶段的处理。首先提取一些特征，一些手工设计的音频特征，比如MFCC，这种算法是用来从音频中提取一组特定的人工设计的特征。在提取出一些低层次特征之后，应用机器学习算法在音频片段中找到音位（声音的基本单位），比如“Cat”这个词是三个音节构成的，Cu-、Ah-和Tu-，算法把这三个音位提取出来，然后将音位串在一起构成独立的词，然后将词串起来构成音频片段的听写文本 ) 端到端深度学习是训练一个巨大的神经网络，输入一段音频，输出直接是听写文本。只需要把训练集拿过来，直接学到了x和y之间的函数映射，绕过了其中很多步骤 端到端深度学习的挑战之一是需要大量数据才能让系统表现良好，比如只有3000小时数据去训练语音识别系统，那传统的流水线效果很好。但当有非常大的数据集时，比如10,000小时数据或者100,000小时数据，端到端方法突然开始很厉害。所以当数据集较小时，传统流水线方法效果更好。如果数据量适中，也可以用中间件方法，如输入还是音频，然后绕过特征提取，直接尝试从神经网络输出音位 门禁识别系统 最好的方法是一个多步方法，首先运行一个软件来检测人脸，然后放大图像并裁剪图像，使人脸居中显示，然后红线框起来的照片再喂到神经网络里，让网络去学习，或估计那人的身份 比起一步到位，一步学习，把这个问题分解成两个更简单的步骤更好： 首先弄清楚脸在哪里 第二步是看着脸，弄清楚这是谁 这种方法让两个学习算法分别解决两个更简单的任务，并在整体上得到更好的表现 训练第二步的方式：输入两张图片，网络将两张图比较一下，判断是否是同一个人。比如记录了10,000个员工ID，可以把红色框起来的图像快速比较，看看红线内的照片是不是那10000个员工之一，判断是否应该允许其进入 为什么两步法更好： 解决的两个问题，每个问题实际上要简单得多 两个子任务的训练数据都很多 机器翻译 传统上机器翻译系统也有一个很复杂的流水线，比如英语机翻得到文本，然后做文本分析，基本上要从文本中提取一些特征之类的，经过很多步骤，最后会将英文文本翻译成法文。因为对于机器翻译来说有很多(英文,法文)的数据对，端到端深度学习在机器翻译领域非常好用 2.10 是否要使用端到端的深度学习？（Whether to use end-to-end learning?）端到端学习的优点 端到端学习只是让数据说话。如果有足够多的(x,y)数据，不管从x到y最适合的函数映射是什么，如果训练一个足够大的神经网络，希望这个神经网络能自己搞清楚。使用纯机器学习方法，直接从x到y输入去训练神经网络，可能更能够捕获数据中的任何统计信息，而不是被迫引入人类的成见。例如在语音识别领域，早期的识别系统有这个音位概念，如果让学习算法学习它想学习的任意表示方式，而不是强迫使用音位作为表示方式，其整体表现可能会更好 所需手工设计的组件更少，能够简化设计工作流程，不需要花太多时间去手工设计功能，手工设计中间表示方式 端到端学习的缺点 直接学到x到y的映射，需要大量(x,y)数据 排除了可能有用的手工设计组件。当有大量数据时，手工设计不太重要，当没有太多的数据时，构造一个精心设计的系统，可以将人类对这个问题的很多认识直接注入到问题里，对算法很有帮助 端到端深度学习的弊端之一是它把可能有用的人工设计的组件排除在外，精心设计的人工组件可能非常有用，但也可能真的影响算法表现。例如，强制算法以音位为单位思考，也许让算法自己找到更好的表示方法更好。但往往好处更多，手工设计的组件往往在训练集更小的时候帮助更大 决定是否使用端到端深度学习，关键的问题是是否有足够的数据能够直接学到从x映射到y足够复杂的函数。识别图中骨头位置是相对简单的问题，系统不需要那么多数据。但把手的X射线照片直接映射到孩子的年龄，直接去找这种函数，就是更为复杂的问题。如果用纯端到端方法，需要很多数据去学习]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第 三 周 超 参 数 调 试 、 Batch 正 则 化 和 程 序 框 架 （Hyperparameter tuning）(Course 2)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC-%E4%B8%89-%E5%91%A8-%E8%B6%85-%E5%8F%82-%E6%95%B0-%E8%B0%83-%E8%AF%95-%E3%80%81-Batch-%E6%AD%A3-%E5%88%99-%E5%8C%96-%E5%92%8C-%E7%A8%8B-%E5%BA%8F-%E6%A1%86-%E6%9E%B6-%EF%BC%88Hyperparameter-tuning%EF%BC%89-Course-2%2F</url>
    <content type="text"><![CDATA[3.1 调试处理（Tuning process）深度神经网络需要调试的超参数（Hyperparameters）包括： \alpha：学习因子 \beta：动量梯度下降因子 \beta_1,\beta_2,\varepsilon：Adam算法参数 #layers：神经网络层数 #hidden units：各隐藏层神经元个数 learning rate decay：学习因子下降参数 mini-batch size：批量训练样本包含的样本个数 学习因子\alpha是最重要的超参数，也是需要重点调试的超参数。动量梯度下降因子\beta、各隐藏层神经元个数#hidden units和mini-batch size的重要性仅次于,然后就是神经网络层数#layers和学习因子下降参数learning rate decay。最后，Adam算法的三个参数\beta_1,\beta_2,\varepsilon一般常设置为0.9，0.999和10^{-8} 传统的机器学习中，对每个参数等距离选取任意个数的点，分别使用不同点对应的参数组合进行训练，最后根据验证集上的表现好坏来选定最佳的参数。例如有两个待调试的参数，分别在每个参数上选取5个点，这样构成了5x5=25中参数组合： 这种做法在参数比较少的时候效果较好 深度神经网络模型中是使用随机选择。随机选择25个点，作为待调试的超参数： ) 随机化选择参数是为了尽可能地得到更多种参数组合。如果使用均匀采样，每个参数只有5种情况；而使用随机采样的话，每个参数有25种可能的情况，更可能得到最佳的参数组合 另外一个好处是对重要性不同的参数之间的选择效果更好。假设hyperparameter1为\alpha，hyperparameter2为\varepsilon，显然二者的重要性是不一样的。如果使用第一种均匀采样的方法，\varepsilon的影响很小，相当于只选择了5个\alpha值。而如果使用第二种随机采样的方法，\varepsilon和\alpha都有可能选择25种不同值。这大大增加了\alpha调试的个数，更有可能选择到最优值 在实际应用中完全不知道哪个参数更加重要的情况下，随机采样的方式能有效解决这一问题，但是均匀采样做不到这点 随机采样之后，可能得到某些区域模型的表现较好。为了得到更精确的最佳参数，继续对选定的区域进行由粗到细的采样（coarse to fine sampling scheme）。就是放大表现较好的区域，对此区域做更密集的随机采样 如对下图中右下角的方形区域再做25点的随机采样，以获得最佳参数： 3.2 为超参数选择合适的范围（Using an appropriate scale to pick hyperparameters）随机取值并不是在有效范围内的随机均匀取值，而是选择合适的标尺，用于探究这些超参数 对于超参数#layers和#hidden units，都是正整数，是可以进行均匀随机采样的，即超参数每次变化的尺度都是一致 对于某些超参数，可能需要非均匀随机采样（即非均匀刻度尺）。例如超参数\alpha，待调范围是[0.0001, 1]。如果使用均匀随机采样，90%的采样点分布在[0.1, 1]之间，只有10%分布在[0.0001, 0.1]之间。而最佳的\alpha值可能主要分布在[0.0001, 0.1]之间，因此更应在区间[0.0001, 0.1]内细分更多刻度 通常的做法是将linear scale转换为log scale，将均匀尺度转化为非均匀尺度，然后再在log scale下进行均匀采样。这样，[0.0001, 0.001]，[0.001, 0.01]，[0.01, 0.1]，[0.1, 1]各个区间内随机采样的超参数个数基本一致，扩大了之前[0.0001, 0.1]区间内采样值个数 如果线性区间为[a, b]，令m=log(a)，n=log(b)，则对应的log区间为[m,n]。对log区间的[m,n]进行随机均匀采样，得到的采样值r，最后反推到线性区间，即10^r.10^r是最终采样的超参数。代码为： 12345m = np.log10(a)n = np.log10(b)r = np.random.rand()r = m + (n-m)*rr = np.power(10,r) 动量梯度因子\beta在超参数调试也需要进行非均匀采样。一般\beta的取值范围在[0.9, 0.999]之间，1−\beta的取值范围在[0.001, 0.1]。那么直接对1−\beta在[0.001, 0.1]区间内进行log变换 为什么\beta也需要向\alpha那样做非均匀采样： 假设\beta从0.9000变化为0.9005，那么\frac{1}{1-\beta}基本没有变化。但假设β从0.9990变化为0.9995，那么\frac{1}{1-\beta}前后差别1000。\beta越接近1，指数加权平均的个数越多，变化越大。所以对\beta接近1的区间，应该采集得更密集一些 3.3 超参数训练的实践： Pandas VS Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）经过调试选择完最佳的超参数不是一成不变的，一段时间之后（例如一个月），需要根据新的数据和实际情况，再次调试超参数，以获得实时的最佳模型 在训练深度神经网络时，一种情况是有庞大的数据组，但没有许多计算资源或足够的 CPU 和GPU 的前提下，只能对一个模型进行训练，调试不同的超参数，使得这个模型有最佳的表现。称之为Babysitting one model。另外一种情况是可以对多个模型同时进行训练，每个模型上调试不同的超参数，根据表现情况，选择最佳的模型。称之为Training many models in parallel 第一种情况只使用一个模型，类比做Panda approach；第二种情况同时训练多个模型，类比做Caviar approach。使用哪种模型是由计算资源、计算能力所决定的。一般来说，对于非常复杂或者数据量很大的模型，使用Panda approach更多一些 3.4 归一化网络的激活函数（ Normalizing activations in a network）在神经网络中，第l层隐藏层的输入就是第l-1层隐藏层的输出A^{[l-1]}。对A^{[l-1]}进行标准化处理，从原理上来说可以提高W^{[l]}和b^{[l]}的训练速度和准确度。这种对各隐藏层的标准化处理就是Batch Normalization。一般是对Z^{[l-1]}进行标准化处理而不是A^{[l-1]} Batch Normalization对第l层隐藏层的输入Z^{[l-1]}做如下标准化处理，忽略上标[l-1]： \mu=\frac1m\sum_iz^{(i)} \sigma^2=\frac1m\sum_i(z_i-\mu)^2 z^{(i)}_{norm}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\varepsilon}}m是单个mini-batch包含样本个数，ε是为了防止分母为零，可取值10^{-8}。使得该隐藏层的所有输入z^{(i)}均值为0，方差为1 大部分情况下并不希望所有的z^{(i)}均值都为0，方差都为1，也不太合理。通常需要对z^{(i)}进行进一步处理： \tilde z^{(i)}=\gamma\cdot z^{(i)}_{norm}+\beta$\gamma$和\beta是learnable parameters，可以通过梯度下降等算法求得。\gamma和\beta是让\tilde z^{(i)}的均值和方差为任意值，只需调整其值。如： \gamma=\sqrt{\sigma^2+\varepsilon},\ \ \beta=u则\tilde z^{(i)}=z^{(i)}，即identity function。设置\gamma和\beta为不同的值，可以得到任意的均值和方差 通过Batch Normalization，对隐藏层的各个z^{[l](i)}进行标准化处理，得到\tilde z^{[l](i)}，替代z^{[l](i)} 输入的标准化处理Normalizing inputs和隐藏层的标准化处理Batch Normalization是有区别的。Normalizing inputs使所有输入的均值为0，方差为1。而Batch Normalization可使各隐藏层输入的均值和方差为任意值。从激活函数的角度来说，如果各隐藏层的输入均值在靠近0的区域即处于激活函数的线性区域，这样不利于训练好的非线性神经网络，得到的模型效果也不会太好 3.5 将 Batch Norm 拟合进神经网络（Fitting Batch Norm into a neural network）前向传播的计算流程： 实现梯度下降： for t = 1 … num （这里num 为Mini Batch 的数量）： 在每一个X^t 上进行前向传播（forward prop）的计算： 在每个隐藏层都用 Batch Norm 将z^{[l]}替换为 \widetilde{z}^{[l]} 使用反向传播（Back prop）计算各个参数的梯度：dw^{[l]},d\gamma^{[l]},d\beta^{[l]} 更新参数： w^{[l]}:=w^{[l]}-\alpha dw^{[l]} \gamma^{[l]}:=\gamma^{[l]}-\alpha d\gamma^{[l]} \beta^{[l]}:=\beta^{[l]}-\alpha d\beta^{[l]} 经过Batch Norm的作用，整体流程如下： ) Batch Norm对各隐藏层Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}有去均值的操作，Batch Norm 要做的就是将z^{[l]}归一化，结果成为均值为0，标准差为1的分布，再由\beta 和\gamma 进行重新的分布缩放，意味着无论b^{[l]} 值为多少，在这个过程中都会被减去，不会再起作用。所以常数项b^{[l]}可以消去，其数值效果完全可以由\widetilde{z}^{[l]}中的\beta来实现。在使用Batch Norm的时候，可以忽略各隐藏层的常数项b^{[l]}。在使用梯度下降算法时，分别对W^{[l]},\beta^{[l]}和\gamma^{[l]}进行迭代更新 除了传统的梯度下降算法之外，还可以使用动量梯度下降、RMSprop或者Adam等优化算法 3.6 Batch Norm 为什么奏效？（Why does Batch Norm work?）Batch Norm 可以加速神经网络训练的原因： 和输入层的输入特征进行归一化，从而改变Cost function的形状，使得每一次梯度下降都可以更快的接近函数的最小值点，从而加速模型训练过程的原理有相同的道理，只是Batch Norm是将各个隐藏层的激活函数的激活值进行的归一化，并调整到另外的分布 Batch Norm 可以使权重比网络更滞后或者更深层 判别是否是猫的分类问题：假设第一训练样本的集合中的猫均是黑猫，而第二个训练样本集合中的猫是各种颜色的猫。如果将第二个训练样本直接输入到用第一个训练样本集合训练出的模型进行分类判别，在很大程度上无法保证能够得到很好的判别结果 因为训练样本不具有一般性（即不是所有的猫都是黑猫），第一个训练集合中均是黑猫，而第二个训练集合中各色猫均有，虽然都是猫，但是很大程度上样本的分布情况是不同的，无法保证模型可以仅仅通过黑色猫的样本就可以完美的找到完整的决策边界 这种训练样本（黑猫）和测试样本（猫）分布的变化称之为covariate shift。如下图所示： 深度神经网络中，covariate shift会导致模型预测效果变差，重新训练的模型各隐藏层的W^{[l]}和B^{[l]}均产生偏移、变化。而Batch Norm的作用恰恰是减小covariate shift的影响，让模型变得更加健壮，鲁棒性更强 使用深层神经网络，使用Batch Norm，该模型对花猫的识别能力应该也是不错 Batch Norm 解决Covariate shift的问题) 网络的目的是通过不断的训练，最后输出一个更加接近于真实值的\hat y，以第2个隐藏层为输入来看： 对于后面的神经网络，是以第二层隐层的输出值a^{[2]}作为输入特征的，通过前向传播得到最终的\hat y，但是网络还有前面两层，由于训练过程，参数w^{[1]},w^{[2]}是不断变化的，对于后面的网络，a^{[2]}的值也是处于不断变化之中，所以就有了Covariate shift的问题 如果对z^{[2]}使用了Batch Norm，即使其值不断的变化，其均值和方差却会保持。Batch Norm的作用是限制前层的参数更新导致对后面网络数值分布程度的影响，使得输入后层的数值变得更加稳定。Batch Norm减少了各层W^{[l]},B^{[l]}之间的耦合性，让各层更加独立，实现自我训练学习的效果。如果输入发生covariate shift，Batch Norm的作用是对个隐藏层输出Z^{[l]}进行均值和方差的归一化处理，让W^{[l]},B^{[l]}更加稳定，使得原来的模型也有不错的表现 Batch Norm 削弱了前层参数与后层参数之间的联系，使得网络的每层都可以自己进行学习，相对其他层有一定的独立性，有助于加速整个网络的学习 Batch Norm 正则化效果 使用Mini-batch梯度下降，每次计算均值和偏差都是在一个Mini-batch上进行计算，而不是在整个数据样集上。这样在均值和偏差上带来一些比较小的噪声。那么用均值和偏差计算得到的\widetilde{z}^{[l]}也将会加入一定的噪声 和Dropout相似，其在每个隐藏层的激活值上加入了一些噪声，（Dropout以一定的概率给神经元乘上0或者1）。Batch Norm 也有轻微的正则化效果 如果使用Batch Norm ，使用大的Mini-batch如256，相比使用小的Mini-batch如64，会引入更少的噪声，会减少正则化的效果 Batch Norm的正则化效果比较微弱，正则化不是Batch Norm的主要功能 3.7 测试时的 Batch Norm（Batch Norm at test time）训练过程中Batch Norm的主要过程： \mu=\frac1m\sum_iz^{(i)} \sigma^2=\frac1m\sum_i(z^{(i)}-\mu)^2 z_{norm}^{(i)}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\varepsilon}} \tilde z^{(i)}=\gamma\cdot z^{(i)}_{norm}+\beta$\mu$和\sigma^2是对单个mini-batch中所有m个样本求得的。在测试过程中，如果只有一个样本，求其均值和方差是没有意义的，就需要对\mu和\sigma^2进行估计。实际应用是使用指数加权平均（exponentially weighted average）的方法来预测测试过程单个样本的\mu和\sigma^2 对于第l层隐藏层，在训练的过程中, ，对于训练集的Mini-batch，考虑所有mini-batch在该隐藏层下的\mu^{[l]}和{\sigma^{2}}^{[l]}，使用指数加权平均，当训练结束的时候，得到指数加权平均后当前单个样本的\mu^{[l]}和{\sigma^{2}}^{[l]},这些值直接用于Batch Norm公式的计算，用以对测试样本进行预测，再利用训练过程得到的\gamma和\beta计算出各层的\tilde z^{(i)}值 3.8 Softmax 回归（Softmax regression）Softmax 回归，能在识别多种分类中的一个时做出预测，对于多分类问题，用C表示种类个数，神经网络中输出层就有C个神经元，即n^{[L]}=C，每个神经元的输出依次对应属于该类的概率，即P(y=c|x)，处理多分类问题一般使用Softmax回归模型 把猫做类 1，狗为类 2，小鸡 是类 3， 如果不属于以上任何一类， 就分到“其它”或者“以上均不符合”这一类，叫 做类 0 用大写C表示输入会被分入的类别总个数,当有 4 个分类时，指示类别的数字，就是从 0 到C − 1( 0、 1、 2、 3) Softmax回归模型输出层的激活函数： z^{[L]}=W^{[L]}a^{[L-1]}+b^{[L]} a^{[L]}_i=\frac{e^{z^{[L]}_i}}{\sum_{i=1}^Ce^{z^{[L]}_i}}输出层每个神经元的输出a^{[L]}_i对应属于该类的概率，满足： \sum_{i=1}^Ca^{[L]}_i=1所有的a^{[L]}_i，即\hat y，维度为(C, 1) 在没有隐藏隐藏层的时候，直接对Softmax层输入样本的特点，则在不同数量的类别下，Sotfmax层的作用： 图中的颜色显示了 Softmax 分类器的输出的阈值，输入的着色是基于三种输出中概率最高的那种，任何两个分类之间的决策边界都是线性的 如果使用神经网络，特别是深层神经网络，可以得到更复杂、更精确的非线性模型 3.9 训练一个 Softmax 分类器（Training a Softmax classifier）C=4，某个样本的预测输出\hat y和真实输出y： \hat y=\left[ \begin{matrix} 0.3 \\ 0.2 \\ 0.1 \\ 0.4 \end{matrix} \right] y=\left[ \begin{matrix} 0 \\ 1 \\ 0 \\ 0 \end{matrix} \right]从\hat y值来看，P(y=4|x)=0.4，概率最大，而真实样本属于第2类，该预测效果不佳 定义softmax classifier的loss function为： L(\hat y,y)=-\sum_{j=1}^4y_j\cdot log\ \hat y_j$L(\hat y,y)$简化为： L(\hat y,y)=-y_2\cdot log\ \hat y_2=-log\ \hat y_2让L(\hat y,y)更小，就应该让\hat y_2越大越好。\hat y_2反映的是概率 m个样本的cost function为： J=\frac{1}{m}\sum_{i=1}^mL(\hat y,y)预测输出向量A^{[L]}即\hat Y的维度为(4, m) softmax classifier的反向传播过程: 先推导dZ^{[L]}： da^{[L]}=-\frac{1}{a^{[L]}} \frac{\partial a^{[L]}}{\partial z^{[L]}}=\frac{\partial}{\partial z^{[L]}}\cdot (\frac{e^{z^{[L]}_i}}{\sum_{i=1}^Ce^{z^{[L]}_i}})=a^{[L]}\cdot (1-a^{[L]}) 所有m个训练样本： dZ^{[L]}=A^{[L]}-Y]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第四周：深层神经网络(Deep Neural Networks)(Course 1)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E5%9B%9B%E5%91%A8%EF%BC%9A%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Deep-Neural-Networks%2F</url>
    <content type="text"><![CDATA[4.1 深层神经网络（Deep L-layer neural network） $L-layer\quad NN$，则包含了L-1个隐藏层，最后的L层是输出层 $a^{[l]}$和W^{[l]}中的上标l都是从1开始的，l=1,\cdots,L 输入x记为a^{[0]}​​，把输出层\hat y记为a^{[L]} $X$：(12288, 209)(with m=209 examples) Shape of W Shape of b Activation Shape of Activation Layer 1 (n^{[1]},12288) (n^{[1]},1) Z^{[1]} = W^{[1]} X + b^{[1]} (n^{[1]},209) Layer 2 (n^{[2]}, n^{[1]}) (n^{[2]},1) Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} (n^{[2]},209) \vdots \vdots \vdots \vdots \vdots Layer L-1 (n^{[L-1]}, n^{[L-2]}) (n^{[L-1]}, 1) Z^{[L-1]} = W^{[L-1]} A^{[L-2]} + b^{[L-1]} (n^{[L-1]}, 209) Layer L (n^{[L]}, n^{[L-1]}) (n^{[L]}, 1) Z^{[L]} = W^{[L]} A^{[L-1]} + b^{[L]} (n^{[L]}, 209) 4.2 前向传播和反向传播（Forward and backward propagation）正向传播过程 z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]} a^{[l]}=g^{[l]}(z^{[l]})$m$个训练样本，向量化形式为： Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} A^{[l]}=g^{[l]}(Z^{[l]})反向传播过程 dz^{[l]}=da^{[l]}\ast g^{[l]'}(z^{[l]}) dW^{[l]}=dz^{[l]}\cdot {a^{[l-1]}}^T db^{[l]}=dz^{[l]} da^{[l-1]}=W^{[l]T}\cdot dz^{[l]}得到： dz^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}\ast g^{[l]'}(z^{[l]})$m$个训练样本，向量化形式为： dZ^{[l]}=dA^{[l]}\ast g^{[l]'}(Z^{[l]}) dW^{[l]}=\frac1mdZ^{[l]}\cdot A^{[l-1]T} db^{[l]}=\frac1mnp.sum(dZ^{[l]},axis=1,keepdim=True) dA^{[l-1]}=W^{[l]T}\cdot dZ^{[l]} dZ^{[l]}=W^{[l+1]T}\cdot dZ^{[l+1]}\ast g^{[l]'}(Z^{[l]}) 4.3 深层网络中的前向传播（Forward propagation in a Deep Network ）对于第l层，其正向传播过程的Z^{[l]}和A^{[l]}可以表示为： Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} A^{[l]}=g^{[l]}(Z^{[l]})其中l=1,\cdots,L 4.4 为什么使用深层表示？（Why deep representations?）人脸识别经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。 随着层数由浅到深，神经网络提取的特征也是从边缘到局部特征到整体，由简单到复杂。如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。 语音识别模型浅层的神经元能够检测一些简单的音调，较深的神经元能够检测出基本的音素，更深的神经元就能够检测出单词信息。如果网络够深，还能对短语、句子进行检测。 神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大 深层网络另外一个优点:减少神经元个数，从而减少计算量 使用电路理论，计算逻辑输出： y=x_1\oplus x_2\oplus x_3\oplus\cdots\oplus x_n对于这个逻辑运算，深度网络的结构是每层将前一层的两两单元进行异或，最后得到一个输出 整个深度网络的层数是log_2(n)，不包含输入层。总共使用的神经元个数为： 1+2+\cdots+2^{log_2(n)-1}=1\cdot\frac{1-2^{log_2(n)}}{1-2}=2^{log_2(n)}-1=n-1输入个数是n，这种深层网络所需的神经元个数仅仅是n-1个 如果不用深层网络，使用单个隐藏层，需要的神经元个数将是指数级别那么大。由于包含了所有的逻辑位（0和1），则需要2^{n-1}个神经元 处理同一逻辑问题，深层网络所需的神经元个数比浅层网络要少很多 4.5 搭建神经网络块（Building blocks of deep neural networks）第l层的流程块图 对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示： 4.6 参数 VS 超参数（Parameters vs Hyperparameters）神经网络中的参数是W^{[l]}和b^{[l]} 超参数则是例如学习速率\alpha，训练迭代次数N，神经网络层数L，各层神经元个数n^{[l]}，激活函数g(z)等 叫做超参数的原因是它们决定了参数W^{[l]}和b^{[l]}的值 如何设置最优的超参数： 通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周 机器学习策略（1）（ML strategy（1））(Course 3)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%B8%80%E5%91%A8-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5%EF%BC%881%EF%BC%89%EF%BC%88ML-strategy%EF%BC%881%EF%BC%89%EF%BC%89-Course-3%2F</url>
    <content type="text"><![CDATA[1.1 为什么是 ML 策略？（Why ML Strategy?）当最初得到一个深度神经网络模型时，希望从很多方面来对它进行优化，例如： Collect more data Collect more diverse training set Train algorithm longer with gradient descent Try Adam instead of gradient descent Try bigger network Try smaller network Try dropout Add L2 regularization Network architecture: Activation functions, #hidden units… 可选择的方法很多、很复杂、繁琐。盲目选择、尝试不仅耗费时间而且可能收效甚微。因此，使用快速、有效的策略来优化机器学习模型是非常必要的。 1.2 正交化（Orthogonalization）每次只调试一个参数，保持其它参数不变，使得到的模型某一性能改变是一种最常用的调参策略，称之为正交化方法（Orthogonalization） Orthogonalization的核心在于每次调试一个参数只会影响模型的某一个性能 机器学习监督式学习模型大致分成四个独立的“功能”： Fit training set well on cost function ，优化训练集可以通过使用更复杂NN，使用Adam等优化算法来实现 Fit dev set well on cost function，优化验证集可以通过正则化，采用更多训练样本来实现 Fit test set well on cost function，优化测试集可以通过使用更多的验证集样本来实现 Performs well in real world，提升实际应用模型可以通过更换验证集，使用新的cost function来实现 每一种“功能”对应不同的调节方法，是正交的 early stopping在模型功能调试中并不推荐使用。因为early stopping在提升验证集性能的同时降低了训练集的性能。即early stopping同时影响两个“功能”，不具有独立性、正交性 1.3 单一数字评估指标（Single number evaluation metric）A和B模型的准确率（Precision）和召回率（Recall）分别如下： 使用单值评价指标F1 Score来评价模型的好坏。F1 Score综合了Precision和Recall的大小： F1=\frac{2\cdot P\cdot R}{P+R} 还可以使用平均值作为单值评价指标： 不同国家样本的错误率，计算平均性能，选择平均错误率最小的模型（C模型） 1.4 满足和优化指标（Satisficing and optimizing metrics）当把所有的性能指标都综合在一起，构成单值评价指标比较困难时：可以把某些性能作为优化指标（Optimizing metic），寻求最优化值；而某些性能作为满意指标（Satisficing metic），只要满足阈值就行 Accuracy和Running time这两个性能不太合适综合成单值评价指标。可以将Accuracy作为优化指标（Optimizing metic），Running time作为满意指标（Satisficing metic）。给Running time设定一个阈值，在其满足阈值的情况下，选择Accuracy最大的模型。如果设定Running time必须在100ms以内，模型C不满足阈值条件，剔除；模型B相比较模型A而言，Accuracy更高，性能更好 如果要考虑N个指标，则选择一个指标为优化指标，其他N-1个指标都是满足指标： N_{metric}:\left\{ \begin{array}{l} 1\qquad \qquad \qquad Optimizing\ metric\\ N_{metric}-1\qquad Satisificing\ metric \end{array} \right.性能指标（Optimizing metic）需要优化，越优越好；满意指标（Satisficing metic）只要满足设定的阈值 1.5 训练/开发/测试集划分（Train/dev/test distributions）训练、开发、测试集选择设置的一些规则和意见： 训练、开发、测试集的设置会对产品带来非常大的影响； 在选择开发集和测试集时要使二者来自同一分布，且从所有数据中随机选取； 所选择的开发集和测试集中的数据，要与未来想要或者能够得到的数据类似，即模型数据和未来数据要具有相似性； 设置的测试集只要足够大，使其能够在过拟合的系统中给出高方差的结果就可以，也许10000左右的数目足够； 设置开发集只要足够使其能够检测不同算法、不同模型之间的优劣差异就可以，百万大数据中1%的大小就足够； 尽量保证dev sets和test sets来源于同一分布且都反映了实际样本的情况。如果dev sets和test sets不来自同一分布，从dev sets上选择的“最佳”模型往往不能够在test sets上表现得很好。好比在dev sets上找到最接近一个靶的靶心的箭，但是test sets提供的靶心却远远偏离dev sets上的靶心，结果肯定无法射中test sets上的靶心位置 1.6 开发集和测试集的大小（Size of dev and test sets） 样本数量不多（小于一万）的时候，通常将Train/dev/test sets的比例设为60%/20%/20% 没有dev sets的情况下，Train/test sets的比例设为70%/30% 样本数量很大（百万级别）的时候，通常将相应的比例设为98%/1%/1%或者99%/1% dev sets数量的设置，遵循的准则是通过dev sets能够检测不同算法或模型的区别，以便选择出更好的模型 test sets数量的设置，遵循的准则是通过test sets能够反映出模型在实际中的表现 实际应用中，可能只有train/dev sets，而没有test sets。这种情况也是允许的，只要算法模型没有对dev sets过拟合。但条件允许的话，最好有test sets，实现无偏估计 1.7 什么时候该改变开发/测试集和指标？（When to change dev/test sets and metrics）算法模型的评价标准有时候需要根据实际情况进行动态调整，目的是让算法模型在实际应用中有更好的效果 example1假设有两个猫的图片的分类器： 评估指标：分类错误率 算法A：3%错误率 算法B：5%错误率 初始的评价标准是错误率，A更好一些。实际使用时发现算法A会通过一些色情图片，但是B没有。从用户的角度来说，更倾向选择B模型，虽然B的错误率高一些。这时候需要改变之前只使用错误率作为评价标准，考虑新的情况进行改变。如增加色情图片的权重，增加其代价 假设开始的评估指标如下： Error = \dfrac{1}{m_{dev}}\sum\limits_{i=1}^{m_{dev}}I\{y^{(i)}_{pred}\neq y^{(i)}\}该评估指标对色情图片和非色情图片一视同仁 修改的方法，在其中加入权重w^{(i)}： Error = \dfrac{1}{\sum w^{(i)}}\sum\limits_{i=1}^{m_{dev}} w^{(i)}I\{y^{(i)}_{pred}\neq y^{(i)}\} w^{(i)}=\begin{cases} 1, & x^{(i)}\ is\ non-porn\\ 10 \ or\ 100, & x^{(i)}\ is\ porn \end{cases}通过设置权重，当算法将色情图片分类为猫时，误差项会快速变大 概括来说，机器学习可分为两个过程： Define a metric to evaluate classifiers How to do well on this metric 第一步是找靶心，第二步是通过训练，射中靶心。但是在训练的过程中可能会根据实际情况改变算法模型的评价标准，进行动态调整,如果评估指标无法正确评估算法的排名，则需要重新定义一个新的评估指标 example2对example1中的两个不同的猫图片的分类器A和B： 实际情况是一直使用网上下载的高质量的图片进行训练；当部署到手机上时，由于图片的清晰度及拍照水平的原因，当实际测试算法时，会发现算法B的表现其实更好 如果在训练开发测试的过程中得到的模型效果比较好，但是在实际应用中所真正关心的问题效果却不好的时候，就需要改变开发、测试集或者评估指标 Guideline： 定义正确的评估指标来更好的给分类器的好坏进行排序 优化评估指标 1.8 为什么是人的表现？（ Why human-level performance?）机器学习模型的表现通常会跟人类水平表现作比较： 当开始往人类水平努力时，进展很快，机器学习模型经过训练会不断接近human-level performance甚至超过它。超过之后，准确性会上升得比较缓慢，当继续训练算法时，可能模型越来越大，数据越来越多，但是性能无法超过某个理论上限，这就是所谓的贝叶斯最优错误率（Bayes optimal error）。理论上任何模型都不能超过它，即没有任何办法设计出一个x到y的函数，让它能够超过一定的准确度，bayes optimal error代表了最佳表现 对于语音识别来说，如果x是音频片段，有些音频很嘈杂，基本不可能知道说的是什么，所以完美的准确率可能不是100%。对于猫图识别来说，也许一些图像非常模糊，不管是人类还是机器，都无法判断该图片中是否有猫。所以完美的准确度可能不是100 贝叶斯最优错误率有时写作Bayesian，即省略optimal，就是从x到y映射的理论最优函数，永远不会被超越。，无论在一个问题上工作多少年，紫色线永远不会超越贝叶斯错误率，贝叶斯最佳错误率 机器学习的进展直到超越人类的表现之前一直很快，当超越时，有时进展会变慢。有两个原因： 人类水平在很多任务中离贝叶斯最优错误率已经不远 只要表现比人类的表现更差，可以使用某些工具来提高性能。一旦超越了人类的表现，这些工具就没那么好用 只要人类的表现比任何其他算法都要好，就可以让人类看看算法处理的例子，知道错误出在哪里，并尝试了解为什么人能做对，算法做错 1.9 可避免偏差（Avoidable bias）猫分类器: 人类具有近乎完美的准确度，人类水平的错误是1%,如果学习算法达到8%的训练错误率和10%的开发错误率，算法在训练集上的表现和人类水平的表现有很大差距，说明算法对训练集的拟合并不好。从减少偏差和方差这个角度看，把重点放在减少偏差上。比如训练更大的神经网络，跑久一点梯度下降，试试能不能在训练集上做得更好 同样的训练错误率和开发错误率，假设人类水平错误实际上是7.5%，系统在训练集上的表现还好，只比人类的表现差一点。在第二个例子中，应专注减少学习算法的方差，可以试试正则化，让开发错误率更接近训练错误率 用人类水平的错误率估计或代替贝叶斯错误率或贝叶斯最优错误率，对于计算机视觉任务而言，这样替代相当合理，因为人类非常擅长计算机视觉任务，人类能做到的水平和贝叶斯错误率相差不远 左边的例子8%的训练错误率真的很高，可以把它降到1%，减少偏差的手段可能有效。右边的例子中，如果认为贝叶斯错误率是7.5%，这里使用人类水平错误率来替代贝叶斯错误率，就知道没有太多改善的空间了，不能继续减少训练错误率，训练误差和开发误差之间有更多的改进空间，可以将这个2%的差距缩小一点，使用减少方差的手段，比如正则化，或者收集更多的训练数据 贝叶斯错误率或者对贝叶斯错误率的估计和训练错误率之间的差值称为可避免偏差 理论上是不可能超过贝叶斯错误率的，除非过拟合 训练错误率和开发错误率之前的差值，说明算法在方差问题上还有多少改善空间 1.10 理解人的表现（Understanding human-level performance）医学图像识别的例子： 在减小误诊率的背景下，人类水平误差在这种情形下应定义为：0.5% error。但是实际应用中，不同人可能选择的human-level performance基准是不同的，这会带来一些影响 如果在为了部署系统或者做研究分析的背景下，也许超过一名普通医生即可，即人类水平误差在这种情形下应定义为：1% error 假如该模型training error为0.7%，dev error为0.8。如果选择Team of experienced doctors，即human-level error为0.5%，则bias比variance更加突出。如果选择Experienced doctor，即human-level error为0.7%，则variance更加突出。选择什么样的human-level error，有时候会影响bias和variance值的相对变化。当然这种情况一般只会在模型表现很好，接近bayes optimal error的时候出现。越接近bayes optimal error，模型越难继续优化，因为这时候的human-level performance可能是比较模糊难以准确定义的 1.11 超过人的表现（Surpassing human- level performance）对于自然感知类问题，例如视觉、听觉等，机器学习的表现不及人类。但是在很多其它方面，机器学习模型的表现已经超过人类了，包括： Online advertising Product recommendations Logistics(predicting transit time) Loan approvals 机器学习模型超过human-level performance是比较困难的。但是只要提供足够多的样本数据，训练复杂的神经网络，模型预测准确性会大大提高，很有可能接近甚至超过human-level performance。值得一提的是当算法模型的表现超过human-level performance时，很难再通过人的直觉来解决如何继续提高算法模型性能的问题 1.12 改善你的模型的表现（Improving your model performance）提高机器学习模型性能主要要解决两个问题：avoidable bias和variance。training error与human-level error之间的差值反映的是avoidable bias，dev error与training error之间的差值反映的是variance 基本假设： 模型在训练集上有很好的表现； 模型推广到开发和测试集啥也有很好的表现 减少可避免偏差 训练更大的模型 训练更长时间、训练更好的优化算法（Momentum、RMSprop、Adam） 寻找更好的网络架构（RNN、CNN）、寻找更好的超参数 减少方差 收集更多的数据 正则化（L2、dropout、数据增强） 寻找更好的网络架构（RNN、CNN）、寻找更好的超参数]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周：深度学习的实用层面(Practical aspects of Deep Learning)(Course 2)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%B8%80%E5%91%A8%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2-Practical-aspects-of-Deep-Learning-Course-2%2F</url>
    <content type="text"><![CDATA[1.1 训练，验证，测试集（Train / Dev / Test sets）在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助创建高效的神经网络。训练神经网络时，需要做出很多决策，例如： 神经网络分多少层 每层含有多少个隐藏单元 学习速率是多少 各层采用哪些激活函数 循环迭代的过程是这样的： 先有个想法Idea，先选择初始的参数值，构建神经网络模型结构 然后通过代码Code的形式，实现这个神经网络； 通过实验Experiment验证这些参数对应的神经网络的表现性能。 根据验证结果，对参数进行适当的调整优化，再进行下一次的Idea-&gt;Code-&gt;Experiment循环。通过很多次的循环，不断调整参数，选定最佳的参数值，从而让神经网络性能最优化 在机器学习发展的小数据量时代，常见做法是将所有数据三七分，就是的70%训练集，30%测试集，如果没有明确设置验证集，也可以按照60%训练，20%验证和20%测试集来划分 在大数据时代，数据量可能是百万级别，验证集和测试集占数据总量的比例会趋向于变得更小。因为验证集的目的就是验证不同的算法，检验哪种算法更有效，因此，验证集要足够大才能评估，比如2个甚至10个不同算法，并迅速判断出哪种算法更有效。可能不需要拿出20%的数据作为验证集 数据量过百万的应用，训练集可以占到99.5%，验证和测试集各占0.25%，或者验证集占0.4%，测试集占0.1% 确保验证集和测试集的数据来自同一分布 没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。所以如果只有验证集，没有测试集，要做的就是在训练集上训练，尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。因为验证集中已经涵盖测试集数据，其不再提供无偏性能评估 搭建训练验证集和测试集能够加速神经网络的集成，也可以更有效地衡量算法的偏差和方差，从而更高效地选择合适方法来优化算法 1.2 偏差，方差（Bias /Variance）在一个只有x_1和x_2两个特征的二维数据集中，可以绘制数据，将偏差和方差可视化。 在多维空间数据中，绘制数据和可视化分割边界无法实现，但可以通过几个指标，来研究偏差和方差 理解偏差和方差的两个关键数据是训练集误差（Train set error）和验证集误差（Dev set error） 假定训练集误差是1%，验证集误差是11%，可以看出训练集设置得非常好，而验证集设置相对较差，可能过度拟合了训练集，验证集并没有充分利用交叉验证集的作用，这种情况称之为“高方差”。 假设训练集误差是15%，验证集误差是16%，该案例中人的错误率几乎为0%，算法并没有在训练集中得到很好训练，如果训练数据的拟合度不高，就是数据欠拟合，这种算法偏差比较高。对于验证集产生的结果却是合理的，验证集中的错误率只比训练集的多了1%，这种算法偏差高，因为它甚至不能拟合训练集 训练集误差是15%，偏差相当高，验证集的评估结果更糟糕，错误率达到30%，这种算法偏差高，因为它在训练集上结果不理想，而且方差也很高，这是方差偏差都很糟糕的情况 训练集误差是0.5%，验证集误差是1%，猫咪分类器只有1%的错误率，偏差和方差都很低 以上分析都是基于假设预测的，训练集和验证集数据来自相同分布，假设人眼辨别的错误率接近0%，一般来说，最优误差也被称为贝叶斯误差，最优误差接近0%，如果最优误差或贝叶斯误差非常高，比如15%。再看看这个分类器（训练误差15%，验证误差16%），15%的错误率对训练集来说也是非常合理的，偏差不高，方差也非常低 偏差和方差都高： 这条曲线中间部分灵活性非常高，却过度拟合了这两个样本，这类分类器偏差很高，因为它几乎是线性的 采用曲线函数或二次元函数会产生高方差，因为曲线灵活性太高以致拟合了这两个错误样本和中间这些活跃数据。但对于高维数据，有些数据区域偏差高，有些数据区域方差高，所以在高维数据中采用这种分类器看起来就不会那么牵强 1.3 机器学习基础（Basic Recipe for Machine Learning）初始模型训练完成后，首先要知道算法的偏差高不高，如果偏差较高，试着评估训练集或训练数据的性能。如果偏差的确很高，甚至无法拟合训练集，要做的就是增加神经网络的隐藏层个数、神经元个数，训练时间延长，选择其它更复杂的NN模型等 如果网络足够大，通常可以很好的拟合训练集，一旦偏差降低到可以接受的数值，检查一下方差有没有问题，为了评估方差，要查看验证集性能，从一个性能理想的训练集推断出验证集的性能是否也理想，如果方差高，最好的解决办法就是增加训练样本数据，进行正则化Regularization，选择其他更复杂的NN模型 两点需要注意： 第一点，高偏差和高方差是两种不同的情况，通常用训练验证集来诊断算法是否存在偏差或方差问题，然后根据结果选择尝试部分方法。如果算法存在高偏差问题，准备更多训练数据没什么用处 第二点，在当前的深度学习和大数据时代，只要持续训练一个更大的网络，只要正则适度，通常构建一个更大的网络便可以在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。 这两步实际要做的工作是：使用更复杂的神经网络和海量的训练样本，一般能够同时有效减小Bias和Variance 1.4 正则化（Regularization）深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据 $\frac{\lambda}{2m}$乘以w范数的平方，欧几里德范数的平方等于w_j（ j值从1到n_x）平方的和，也可表示为ww^T，也就是向量参数w的欧几里德范数（2范数）的平方，此方法称为L2正则化。因为这里用了欧几里德法线，被称为向量参数w的L2范数。 J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||_2^2 ||w||_2^2=\sum_{j=1}^{n_x}w_j^2=w^Tw为什么不再加上参数b呢？因为通常w是一个高维参数矢量，几乎涵盖所有参数，已经可以表达高偏差问题，所以参数很大程度上由w决定，而b只是众多参数中的一个，改变b值对整体模型影响较小,所以通常省略不计,如果加了参数b，也没太大影响 $L2$正则化是最常见的正则化类型，L1正则化是正则项\frac{\lambda}{m}乘以\sum_{j=1}^{n^x}|w|，\sum_{j=1}^{n^x}|w|也被称为参数向量w的L1范数无论分母是，m还是2m，它都是一个比例常量 J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||_1 ||w||_1=\sum_{j=1}^{n_x}|w_j|如果用的是L1正则化，w最终会是稀疏的，也就是说w向量中有很多0，虽然L1正则化使模型变得稀疏，却没有降低太多存储内存,实际上L1 regularization在解决high variance方面比L2 regularization并不更具优势。而且，L1的在微分求导方面比较复杂 $\lambda$是正则化参数，可以设置\lambda为不同的值，在Dev set中进行验证，选择最佳的\lambda,通常使用验证集或交叉验证集来配置这个参数 在深度学习模型中，L2 regularization的表达式为： J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L||w^{[l]}||^2 ||w^{[l]}||^2=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^2$||w^{[l]}||^2$称为Frobenius范数，记为||w^{[l]}||_F^2。一个矩阵的Frobenius范数就是计算所有元素平方和再开方，如下所示： ||A||_F=\sqrt {\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2}由于加入了正则化项，梯度下降算法中的dw^{[l]}计算表达式需要做如下修改： dw^{[l]}=dw^{[l]}_{before}+\frac{\lambda}{m}w^{[l]} w^{[l]}:=w^{[l]}-\alpha\cdot dw^{[l]}L2 regularization也被称做weight decay。这是因为，由于加上了正则项，dw^{[l]}有个增量，在更新w^{[l]}的时候，会多减去这个增量，使得w^{[l]}比没有正则项的值要小一些。不断迭代更新，不断地减小 \begin{aligned}w^{[l]} &:=w^{[l]}-\alpha\cdot dw^{[l]}\\ &=w^{[l]}-\alpha\cdot(dw^{[l]}_{before}+\frac{\lambda}{m}w^{[l]})\\ &=(1-\alpha\frac{\lambda}{m})w^{[l]}-\alpha\cdot dw^{[l]}_{before} \end{aligned}其中，(1-\alpha\frac{\lambda}{m})]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周：优化算法 (Optimization algorithms)(Course 2)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%9A%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-Optimization-algorithms-Course-2%2F</url>
    <content type="text"><![CDATA[2.1 Mini-batch 梯度下降（Mini-batch gradient descent）神经网络训练过程是同时对所有m个样本（称为batch）通过向量化计算方式进行的。如果m很大，训练速度会很慢，因为每次迭代都要对所有样本进进行求和运算和矩阵运算。这种梯度下降算法称为Batch Gradient Descent解决： 把m个训练样本分成若干个子集，称为mini-batches，然后每次在单一子集上进行神经网络训练，这种梯度下降算法叫做Mini-batch Gradient Descent 假设总的训练样本个数m=5000000，其维度为(n_x,m)。将其分成5000个子集，每个mini-batch含有1000个样本。将每个mini-batch记为X^{\{t\}}，其维度为(n_x,1000)。相应的每个mini-batch的输出记为Y^{\{t\}}，其维度为(1,1000)，且t=1,2,\cdots,5000 X^{(i)}：第i个样本 Z^{[l]}：神经网络第l层网络的线性输出 X^{\{t\}},Y^{\{t\}}：第t组mini-batch Mini-batches Gradient Descent是先将总的训练样本分成T个子集（mini-batches），然后对每个mini-batch进行神经网络训练，包括Forward Propagation，Compute Cost Function，Backward Propagation，循环至T个mini-batch都训练完毕 for\ \ t=1,\cdots,T\ \ \{ \ \ \ \ Forward\ Propagation \ \ \ \ Compute\ Cost\ Function \ \ \ \ Backward\ Propagation \ \ \ \ W:=W-\alpha\cdot dW \ \ \ \ b:=b-\alpha\cdot db \}经过T次循环之后，所有m个训练样本都进行了梯度下降计算。这个过程称之为经历了一个epoch。对于Batch Gradient Descent而言，一个epoch只进行一次梯度下降算法；而Mini-Batches Gradient Descent，一个epoch会进行T次梯度下降算法 对于Mini-Batches Gradient Descent，可以进行多次epoch训练。每次epoch，最好是将总体训练数据打乱、重新分成T组mini-batches，这样有利于训练出最佳的神经网络模型 2.2 理解 mini-batch 梯度下降法（Understanding mini-batch gradient descent）Batch gradient descent和Mini-batch gradient descent的cost曲线： 对于一般的神经网络模型，使用Batch gradient descent，随着迭代次数增加，cost是不断减小的。而使用Mini-batch gradient descent，随着在不同的mini-batch上迭代训练，其cost不是单调下降，而是受类似noise的影响，出现振荡。但整体的趋势是下降的，最终也能得到较低的cost值 出现细微振荡的原因是不同的mini-batch之间是有差异的。可能第一个子集(X^{\{1\}},Y^{\{1\}})是好的子集，而第二个子集(X^{\{2\}},Y^{\{2\}})包含了一些噪声noise。出现细微振荡是正常的 如果mini-batch size=m，即为Batch gradient descent，只包含一个子集为(X^{\{1\}},Y^{\{1\}})=(X,Y)； 如果mini-batch size=1，即为Stachastic gradient descent，每个样本就是一个子集(X^{\{1\}},Y^{\{1\}})=(x^{(i)},y^{(i)})，共有m个子集 蓝色的线代表Batch gradient descent，紫色的线代表Stachastic gradient descent。Batch gradient descent会比较平稳地接近全局最小值，但因为使用了所有m个样本，每次前进的速度有些慢。Stachastic gradient descent每次前进速度很快，但路线曲折，有较大的振荡，最终会在最小值附近来回波动，难达到最小值。而且在数值处理上不能使用向量化的方法来提高运算速度 mini-batch size不能设置得太大（Batch gradient descent），也不能设置得太小（Stachastic gradient descent）。相当于结合了Batch gradient descent和Stachastic gradient descent各自的优点，既能使用向量化优化算法，又能较快速地找到最小值。mini-batch gradient descent的梯度下降曲线如下图绿色所示，每次前进速度较快，且振荡较小，基本能接近全局最小值。 总体样本数量m不太大时，例如m\leq2000，建议直接使用Batch gradient descent 总体样本数量m很大时，建议将样本分成许多mini-batches。推荐常用的mini-batch size为64,128,256,512。都是2的幂。原因是计算机存储数据一般是2的幂，这样设置可以提高运算速度 mini-batch 中确保 X{\{t\}} 和Y{\{t\}}要符合 CPU/GPU 内存，取决于应用方向以及训练集的大小。如果处理的 mini-batch 和 CPU/GPU 内存不相符，不管用什么方法处理数据，算法的表现都急转直下变得惨不忍睹 从训练集（X，Y）中构建小批量 随机洗牌（Shuffle）：创建训练集（X，Y）的混洗版本，X和Y的每一列代表一个训练示例。随机混洗是在X和Y之间同步完成的。这样在混洗之后第i列的X对应的例子就是Y第i列中的标签。混洗步骤可确保将示例随机分成不同的小批次 分区（Partition）：将混洗（X，Y）分区为小批量mini_batch_size（此处为64）。训练示例的数量并非总是可以被mini_batch_size整除。最后一个小批量可能会更小 2.3 指数加权平均数（Exponentially weighted averages）半年内伦敦市的气温变化： 温度数据有noise，抖动较大 如果希望看到半年内气温的整体变化趋势，可以通过移动平均（moving average）的方法来对每天气温进行平滑处理 设V_0=0，当成第0天的气温值 第一天的气温与第0天的气温有关： V_1=0.9V_0+0.1\theta_1第二天的气温与第一天的气温有关： \begin{aligned}V_2 =&0.9V_1+0.1\theta_2\\ =&0.9(0.9V_0+0.1\theta_1)+0.1\theta_2\\ =&0.9^2V_0+0.9\cdot0.1\theta_1+0.1\theta_2 \end{aligned}第三天的气温与第二天的气温有关： \begin{aligned}V_3 =&0.9V_2+0.1\theta_3\\ =&0.9(0.9^2V_0+0.9\cdot0.1\theta_1+0.1\theta_2)+0.1\theta_3\\ =&0.9^3V_0+0.9^2\cdot 0.1\theta_1+0.9\cdot 0.1\theta_2+0.1\theta_3 \end{aligned}第t天与第t-1天的气温迭代关系为： \begin{aligned}V_t =&0.9V_{t-1}+0.1\theta_t\\ =&0.9^tV_0+0.9^{t-1}\cdot0.1\theta_1+0.9^{t-2}\cdot 0.1\theta_2+\cdots+0.9\cdot0.1\theta_{t-1}+0.1\theta_t \end{aligned}经过移动平均处理得到的气温如下图红色曲线所示： 这种滑动平均算法称为指数加权平均（exponentially weighted average）。一般形式为： V_t=\beta V_{t-1}+(1-\beta)\theta_t$\beta$值决定了指数加权平均的天数，近似表示为： \frac{1}{1-\beta}当\beta=0.9，则\frac{1}{1-\beta}=10，表示将前10天进行指数加权平均。当\beta=0.98，则\frac{1}{1-\beta}=50，表示将前50天进行指数加权平均。\beta值越大，则指数加权平均的天数越多，平均后的趋势线就越平缓，但是同时也会向右平移 绿色曲线和黄色曲线分别表示了\beta=0.98和\beta=0.5时，指数加权平均的结果 2.4 理解指数加权平均数（Understanding exponentially weighted averages ）指数加权平均公式的一般形式： \begin{aligned}V_t =&\beta V_{t-1}+(1-\beta)\theta_t\\ =&(1-\beta)\theta_t+(1-\beta)\cdot\beta\cdot\theta_{t-1}+(1-\beta)\cdot \beta^2\cdot\theta_{t-2}+\cdots +(1-\beta)\cdot \beta^{t-1}\cdot \theta_1+\beta^t\cdot V_0 \end{aligned}$\theta_t,\theta_{t-1},\theta_{t-2},\cdots,\theta_1$是原始数据值，(1-\beta),(1-\beta)\beta,(1-\beta)\beta^2,\cdots,(1-\beta)\beta^{t-1}是类似指数曲线，从右向左，呈指数下降的。V_t 的值是这两个子式的点乘，将原始数据值与衰减指数点乘，相当于做了指数衰减，离得越近，影响越大，离得越远，影响越小，衰减越厉害 为了减少内存的使用，使用这样的语句来实现指数加权平均算法： V_{\theta}=0 Repeat\ \{ \ \ \ \ Get\ next\ \theta_t \ \ \ \ V_{\theta}:=\beta V_{\theta}+(1-\beta)\theta_t \}2.5 指 数 加 权 平 均 的 偏 差 修 正 （ Bias correction inexponentially weighted averages ）当\beta=0.98时，指数加权平均结果如绿色曲线。但实际上真实曲线如紫色曲线 紫色曲线与绿色曲线的区别是，紫色曲线开始的时候相对较低一些。因为开始时设置V_0=0，所以初始值会相对小一些，直到后面受前面的影响渐渐变小，趋于正常 修正这种问题的方法是进行偏移校正（bias correction），即在每次计算完V_t后，对V_t进行下式处理： \frac{V_t}{1-\beta^t}刚开始的时候，t比较小，(1-\beta^t)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第三周：浅层神经网络(Shallow neural networks)(Course 1)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%B8%89%E5%91%A8%EF%BC%9A%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Shallow-neural-networks%2F</url>
    <content type="text"><![CDATA[3.1 神经网络概述（Neural Network Overview） 3.2 神经网络的表示（Neural Network Representation ）单隐藏层神经网络就是典型的浅层（shallow）神经网络 单隐藏层神经网络也被称为两层神经网络（2 layer NN） 第l层的权重W^{[l]}维度的行等于l层神经元的个数，列等于l-1层神经元的个数；第i层常数项b^{[l]}维度的行等于l层神经元的个数，列始终为1 3.3 计算一个神经网络的输出（Computing a Neural Network’s output ）两层神经网络可以看成是逻辑回归再重复计算一次 逻辑回归的正向计算可以分解成计算z和a的两部分： z=w^Tx+b a=\sigma(z) 两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算 z^{[1]}=W^{[1]}x+b^{[1]} a^{[1]}=\sigma(z^{[1]}) z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} a^{[2]}=\sigma(z^{[2]}) 3.4 多样本向量化（Vectorizing across multiple examples ）for循环来求解其正向输出： for i = 1 to m: \begin{aligned}&z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]}\\&a^{[1](i)}=\sigma(z^{[1](i)})\\&z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]} \\&a^{[2](i)}=\sigma(z^{[2](i)})\end{aligned}矩阵运算的形式： Z^{[1]}=W^{[1]}X+b^{[1]} A^{[1]}=\sigma(Z^{[1]}) Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]} A^{[2]}=\sigma(Z^{[2]})行表示神经元个数，列表示样本数目m 3.5 激活函数（Activation functions） sigmoid函数 tanh函数 ReLU函数 Leaky ReLU函数 对于隐藏层的激活函数，tanh函数要比sigmoid函数表现更好一些。因为tanh函数的取值范围在[-1,+1]之间，隐藏层的输出被限定在[-1,+1]之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化（均值为0）的效果 对于输出层的激活函数，因为二分类问题的输出取值为\{0,+1\}，所以一般会选择sigmoid作为激活函数 选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点 Leaky$$ $$ReLU$$激活函数，能够保证$$z$$小于零时梯度不为$$03.6 为什么需要（ 非线性激活函数？（why need a nonlinear activation function?）假设所有的激活函数都是线性的，直接令激活函数g(z)=z，即a=z z^{[1]}=W^{[1]}x+b^{[1]} a^{[1]}=z^{[1]} z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} a^{[2]}=z^{[2]} a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}=(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})=W'x+b'多层隐藏层的神经网络，如果使用线性函数作为激活函数，最终的输出仍然是输入x的线性模型。这样的话神经网络就没有任何作用了。因此，隐藏层的激活函数必须要是非线性的 如果是预测问题而不是分类问题，输出y是连续的情况下，输出层的激活函数可以使用线性函数。如果输出y恒为正值，则也可以使用ReLU激活函数 3.7 激活函数的导数（Derivatives of activation functions ）$sigmoid$函数的导数： g(z)=\frac{1}{1+e^{(-z)}} g'(z)=\frac{d}{dz}g(z)=g(z)(1-g(z))=a(1-a)$tanh$函数的导数： g(z)=\frac{e^{(z)}-e^{(-z)}}{e^{(z)}+e^{(-z)}} g'(z)=\frac{d}{dz}g(z)=1-(g(z))^2=1-a^2$ReLU$函数的导数： g(z)=max(0,z) x = \begin{cases} 0 &\text{if } z < 0 \\ 1 &\text{if } z \geq 0 \end{cases}$Leaky ReLU$函数： g(z)=max(0.01z,z) g'(z) = \begin{cases} 0.01 &\text{if } z < 0 \\ 1 &\text{if } z \geq 0 \end{cases}3.8 神经网络的梯度下降（Gradient descent for neural networks） dZ^{[2]}=A^{[2]}-Y dW^{[2]}=\frac1mdZ^{[2]}A^{[1]T} db^{[2]}=\frac1mnp.sum(dZ^{[2]},axis=1,keepdim=True) dZ^{[1]}=W^{[2]T}dZ^{[2]}\ast g'(Z^{[1]}) dW^{[1]}=\frac1mdZ^{[1]}X^T db^{[1]}=\frac1mnp.sum(dZ^{[1]},axis=1,keepdim=True)3.9 （选修）直观理解反向传播（Backpropagation intuition ）单个训练样本反向过程可以根据梯度计算方法逐一推导： dz^{[2]}=a^{[2]}-y dW^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial W^{[2]}}=dz^{[2]}a^{[1]T} db^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial b^{[2]}}=dz^{[2]}\cdot 1=dz^{[2]} dz^{[1]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial a^{[1]}}\cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}=W^{[2]T}dz^{[2]}\ast g^{[1]'}(z^{[1]}) dW^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial W^{[1]}}=dz^{[1]}x^T db^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}=dz^{[1]}\cdot 1=dz^{[1]} 浅层神经网络（包含一个隐藏层），m个训练样本的正向传播过程和反向传播过程分别包含了6个表达式，其向量化矩阵形式如下图所示： 3.10 随机初始化（Random Initialization）神经网络模型中的参数权重W不能全部初始化为零 如果权重W^{[1]}和W^{[2]}都初始化为零，即： W^{[1]}= \left[ \begin{matrix} 0 & 0 \\ 0 & 0 \end{matrix} \right] W^{[2]}= \left[ \begin{matrix} 0 & 0 \end{matrix} \right]这样使得隐藏层第一个神经元的输出等于第二个神经元的输出，即a_1^{[1]}=a_2^{[1]}。经过推导得到dz_1^{[1]}=dz_2^{[1]}，dW_1^{[1]}=dW_2^{[1]}，这样的结果是隐藏层两个神经元对应的权重行向量W_1^{[1]}和W_2^{[1]}每次迭代更新都会得到完全相同的结果，W_1^{[1]}始终等于W_2^{[1]}，完全对称。这样隐藏层设置多个神经元就没有任何意义 权重W全部初始化为零带来的问题称为symmetry breaking problem 随机初始化： 1234W_1 = np.random.randn((2,2))*0.01b_1 = np.zero((2,1))W_2 = np.random.randn((1,2))*0.01b_2 = 0 让W比较小，是因为如果使用sigmoid函数或者tanh函数作为激活函数的话，W比较小，得到的|z|也比较小（靠近零点），而零点区域的梯度比较大，这样能大大提高梯度下降算法的更新速度，尽快找到全局最优解 如果W较大，得到的|z|也比较大，附近曲线平缓，梯度较小，训练过程会慢很多 如果激活函数是ReLU或者Leaky ReLU函数，则不需要考虑这个问题 如果输出层是sigmoid函数，则对应的权重W最好初始化到比较小的值]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning.ai深度学习课程笔记]]></title>
    <url>%2F2019%2F02%2F27%2FDeepLearning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[笔记中没有涵盖所有的视频内容，主要是我不懂或者觉得比较重要的内容，学生我水平有限，如笔记中有知识点、公式、代码错误，还烦请指出 Andrew Ng（吴恩达）的公开信： 朋友们， 我在做三个全新的AI项目。现在，我十分兴奋地宣布其中的第一个：deeplearning.ai，一个立志于扩散AI知识的项目。该项目在Coursera上发布了一系列深度学习课程，这些课程将帮助你掌握深度学习、对它高效地应用，并打造属于你自己的AI事业。 AI是新一轮电力革命 就像一百年前电力改造了每个主流行业，当今的AI技术在做着相同的事。好几个大型科技公司都设立了AI部门，用AI革新他们的业务。接下来的几年里，各个行业、规模大小各不相同的公司也都会意识到——-在由AI驱动的未来，他们必须成为其中的一份子。 创建由AI驱动的社会 我希望，我们可以建立一个由AI驱动的社会：让每个人看得起病，给每个孩子个性化的教育，让所有人都能坐上价格亲民的自动驾驶汽车，并向男人和女人提供有意义的工作。总而言之，是一个让每个人的生活变得更好的社会。 但是，任何一个公司都不可能单独完成这些任务。就像现在每一个计算机专业的毕业生都知道怎么用云，将来，每个程序员也必须懂得怎么用AI。用深度学习改善人类生活的方法有数百万种，社会也需要数百万个人——即来自世界各国的你们，来创造出了不起的AI系统。不管你是加州的一个软件工程师，一名中国的研究员，还是印度的ML工程师，我希望都能用深度学习来解决世界上的各种挑战。 你会学到什么 任何一个掌握了机器学习基础知识的人，都可以学习这五门系列课程，它们组成了Coursera的全新深度学习专业。 你会学到深度学习的基础，理解如何创建神经网络，学习怎么成功地领导机器学习项目。你会学习卷积神经网络、RNNs、LSTM、Adam、Dropout、BatchNorm、Xavier/He initialization以及更多。学习过程中，你会接触到医疗、自动驾驶、读手语、音乐生成、自然语言处理的案例。 你不仅会掌握深度学习理论，还会看到它是怎样在行业应用落地的。你会在Python和TensorFlow里试验这些想法，你还会听到各位深度学习领袖人物的意见，他们会分享各自的学习经历，并提供职业规划建议。 当你拿到Coursera的深度学习专业证书，就可以自信得把“深度学习”四个字写进你的简历。 加入我，建立一个由AI驱动的社会 从2011年到现在，已经有180万人加入了我的机器学习课程。当时，我和四名斯坦福的学生发布了这门课程，它随即成为了Coursera的第一门公开课。那之后，我受到你们之中许多人的启发——当我看到你们是如何努力地理解机器学习，开发优秀的AI系统，并开启令人惊艳的事业。 我希望深度学习专业能帮助你们实现更了不起的事，让你们为社会贡献更多，在职业道路上走得更远。 我希望大家和我一道，建立一个由AI驱动的社会。 我会通知大家另外两个项目的进展，并不断探索，为全世界AI社区的每一个人提供更多支持的途径。 Sincerely， 吴恩达 吴恩达与deeplearning.ai团队]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周：神经网络的编程基础(Basics of Neural Network programming)(Course 1)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-Basics-of-Neural-Network-programming%2F</url>
    <content type="text"><![CDATA[2.1 二分类(Binary Classification)逻辑回归模型一般用来解决二分类（Binary Classification）问题 二分类就是输出y只有{0,1}两个离散值（也有{-1,1}的情况） 彩色图片包含RGB三个通道。例如该cat图片的尺寸为（64，64，3） 在神经网络模型中，首先要将图片输入x（维度是（64，64，3））转化为一维的特征向量（featurevector）。方法是每个通道一行一行取，再连接起来。则转化后的输入特征向量维度为（12288，1）。此特征向量x是列向量，维度一般记为n_x 如果训练样本共有m张图片，那么整个训练样本X组成了矩阵，维度是（n_x,m), n_x代表了每个样本X^{(i)}特征个数，列m代表了样本个数,输出Y组成了一维的行向量，维度是（1，m） ) 2.2 逻辑回归(Logistic Regression)逻辑回归中，预测值\hat y=P(y=1 | x)表示为1的概率，取值范围在[0,1]之间 使用线性模型，引入参数w和b。权重w的维度是（n_x，1），b是一个常数项 逻辑回归的预测输出可以完整写成： \hat y = Sigmoid(w^Tx+b)=\sigma(w^Tx+b)Sigmoid函数的一阶导数可以用其自身表示： \sigma'(z)=\sigma(z)(1-\sigma(z)) 2.3 逻辑回归的代价函数（Logistic Regression Cost Function）单个样本的cost function用Loss function来表示，使用平方误差（squared error）： L(\hat y,y)=\frac12(\hat y-y)^2逻辑回归一般不使用平方误差来作为Loss function。原因是这种Loss function一般是non-convex的。 non-convex函数在使用梯度下降算法时，容易得到局部最小值（localminimum），即局部最优化。而最优化的目标是计算得到全局最优化（Global optimization），因此一般选择的Loss function应该是convex的 构建另外一种Loss function(针对单个样本)，且是convex的： L(\hat y,y)=-(ylog\ \hat y+(1-y)log\ (1-\hat y))当y=1时，L(\hat y,y)=-\log \hat y，如果\hat y越接近1，L(\hat y,y)\approx 0，表示预测效果越好；如果\hat y越接近0，L(\hat y,y)\approx +\infty，表示预测效果越差 当y=0时，L(\hat y,y)=-\log(1- \hat y)，如果\hat y越接近0，L(\hat y,y)\approx 0，表示预测效果越好；如果\hat y越接近1，L(\hat y,y)\approx +\infty，表示预测效果越差 Cost function是m个样本的Loss function的平均值，反映了m个样本的预测输出\hat y与真实样本输出y的平均接近程度: J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac1m\sum_{i=1}^m[y^{(i)}log\ \hat y^{(i)}+(1-y^{(i)})log\ (1-\hat y^{(i)})] 2.4 逻辑回归的梯度下降（Logistic Regression Gradient Descent）对单个样本而言，逻辑回归Loss function表达式如下： z=w^Tx+b \hat y=a=\sigma(z) L(a,y)=-(y\log(a)+(1-y)\log(1-a)) 计算该逻辑回归的反向传播过程: da=\frac{\partial L}{\partial a}=-\frac ya+\frac{1-y}{1-a} dz=\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a}\cdot \frac{\partial a}{\partial z}=(-\frac ya+\frac{1-y}{1-a})\cdot a(1-a)=a-y dw_1=\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w_1}=x_1\cdot dz=x_1(a-y) dw_2=\frac{\partial L}{\partial w_2}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w_2}=x_2\cdot dz=x_2(a-y) db=\frac{\partial L}{\partial b}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial b}=1\cdot dz=a-y则梯度下降算法可表示为： w_1:=w_1-\alpha\ dw_1 w_2:=w_2-\alpha\ dw_2 b:=b-\alpha\ db 2.5 梯度下降的例子(Gradient Descent on m Examples)m个样本的Cost function表达式如下： z^{(i)}=w^Tx^{(i)}+b \hat y^{(i)}=a^{(i)}=\sigma(z^{(i)}) J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac1m\sum_{i=1}^m[y^{(i)}log\ \hat y^{(i)}+(1-y^{(i)})log\ (1-\hat y^{(i)})]Cost function关于w和b的偏导数可以写成和平均的形式： dw_1=\frac1m\sum_{i=1}^mx_1^{(i)}(a^{(i)}-y^{(i)}) dw_2=\frac1m\sum_{i=1}^mx_2^{(i)}(a^{(i)}-y^{(i)}) dw_m=\frac1m\sum_{i=1}^mx_m^{(i)}(a^{(i)}-y^{(i)}) db=\frac1m\sum_{i=1}^m(a^{(i)}-y^{(i)})算法流程如下所示： 12345678910111213J=0; dw1=0; dw2=0; db=0;for i = 1 to mz(i) = wx(i)+b;a(i) = sigmoid(z(i));J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i));dz(i) = a(i)-y(i);dw1 += x1(i)dz(i);dw2 += x2(i)dz(i);db += dz(i);J /= m;dw1 /= m;dw2 /= m;db /= m; 经过每次迭代后，根据梯度下降算法，w和b都进行更新： w_1:=w_1-\alpha\ dw_1 w_2:=w_2-\alpha\ dw_2 w_m:=w_m-\alpha\ dw_m b:=b-\alpha\ db在深度学习中，样本数量m通常很大，使用for循环会让神经网络程序运行得很慢。应该尽量避免使用for循环操作，而使用矩阵运算，能够大大提高程序运行速度 2.6 向量化 logistic 回归的梯度输出（Vectorizing Logistic Regression’s Gradient Output）db可表示为： db=\frac1m \sum_{i=1}^mdz^{(i)}dw可表示为： dw=\frac1m X\cdot dZ^T单次迭代，梯度下降算法流程如下所示： 12345678Z = np.dot(w.T,X) + bA = sigmoid(Z)dZ = A-Ydw = 1/m*np.dot(X,dZ.T)db = 1/m*np.sum(dZ)w = w - alpha*dwb = b - alpha*db 2.7 （选修）logistic 损失函数的解释（Explanation of logistic regression cost function ）$\hat y$可以看成是预测输出为正类（+1）的概率： \hat y=P(y=1|x)当y=1时： p(y|x)=\hat y当y=0时： p(y|x)=1-\hat y整合到一个式子: P(y|x)=\hat y^y(1-\hat y)^{(1-y)}进行log处理： log\ P(y|x)=log\ \hat y^y(1-\hat y)^{(1-y)}=y\ log\ \hat y+(1-y)log(1-\hat y)上述概率P(y|x)越大越好，加上负号，则转化成了单个样本的Loss function，越小越好: L=-(y\ log\ \hat y+(1-y)log(1-\hat y))对于所有m个训练样本，假设样本之间是独立同分布的，总的概率越大越好： max\ \prod_{i=1}^m\ P(y^{(i)}|x^{(i)})引入log函数，加上负号，将上式转化为Cost function： J(w,b)=-\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac 1m\sum_{i=1}^m[y^{(i)}\ log\ \hat y^{(i)}+(1-y^{(i)})log(1-\hat y^{(i)})]]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周：深度学习引言(Introduction to Deep Learning)(Course 1)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%B8%80%E9%97%A8%E8%AF%BE-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Neural-Networks-and-Deep-Learning%2F</url>
    <content type="text"><![CDATA[1.1 神经网络的监督学习(Supervised Learning with Neural Networks) 一般的监督式学习（房价预测和线上广告问题），只要使用标准的神经网络模型就可以图像识别处理问题，则要使用卷积神经网络（Convolution Neural Network），即CNN 处理类似语音这样的序列信号时，则要使用循环神经网络（Recurrent Neural Network），即RNN 自动驾驶这样的复杂问题则需要更加复杂的混合神经网络模型 CNN一般处理图像问题，RNN一般处理语音信号 数据类型一般分为两种：Structured Data和Unstructured Data Structured Data通常指的是有实际意义的数据，例如房价预测中的size，#bedrooms，price等；例如在线广告中的User Age，Ad ID等 Unstructured Data通常指的是比较抽象的数据，例如Audio，Image或者Text 1.2Why is Deep Learning taking off？ 红色曲线代表了传统机器学习算法的表现，例如是SVM，logistic regression，decision tree等。当数据量比较小的时候，传统学习模型的表现是比较好的。当数据量很大的时候，其性能基本趋于水平 构建一个深度学习的流程是首先产生Idea，然后将Idea转化为Code，最后进行Experiment。接着根据结果修改Idea，继续这种Idea-&gt;Code-&gt;Experiment的循环，直到最终训练得到表现不错的深度学习网络模型 ![]]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
</search>

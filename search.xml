<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[第三周 序列模型和注意力机制（Sequence models & Attention mechanism）(Course 5)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%B8%89%E5%91%A8-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Sequence-models-Attention-mechanism%EF%BC%89-Course-5%2F</url>
    <content type="text"><![CDATA[3.1 基础模型（Basic Models）机器翻译用x^{} 到x^{< 5>}表示输入句子的单词，用y^{}到y^{}表示输出句子的单词： 首先，建立一个RNN编码网络（encoder network）（编号1），单元可以是GRU或LSTM。每次只向该网络中输入一个法语单词，将输入序列接收完毕后，这个RNN网络会输出一个向量来代表这个输入序列 之后建立一个解码网络（编号2），以编码网络的输出作为输入，之后它可以被训练为每次输出一个翻译后的单词，一直到它输出序列的结尾或者句子结尾标记 在给出足够的法语和英语文本的情况下，训练模型，通过输入一个法语句子来输出对应的英语翻译，这个模型将会非常有效。这个模型简单地用一个编码网络来对输入的法语句子进行编码，然后用一个解码网络来生成对应的英语翻译 图像描述 给出猫的图片，能自动地输出该图片的描述：一只猫坐在椅子上 通过输入图像来输出描述： 将图片输入到卷积神经网络中（一个预训练的AlexNet结构）（编号2），然后让其学习图片的编码，或者学习图片的一系列特征。去掉最后的softmax单元（编号3），这个预训练的AlexNet结构会输出4096维的特征向量，向量表示的就是这只猫的图片，这个预训练网络可以是图像的编码网络 接着把这个向量输入到RNN中（编号4），RNN要做的就是生成图像的描述，每次生成一个单词：输入一个描述输入的特征向量，然后让网络一个一个地输出单词序列 3.2 选择最可能的句子（Picking the most likely sentence）seq2seq机器翻译模型和第一周所用的语言模型之间有很多相似的地方，但也有许多重要的区别： 可以把机器翻译想成是建立一个条件语言模型，能够估计句子的可能性 绿色（编号2）表示encoder网络，紫色（编号3）表示decoder网络。不同在于语言模型总是以零向量（编号4）开始，而encoder网络会计算出一系列向量（编号2）来表示输入的句子，以这个向量作为输入，这叫做条件语言模型（conditional language model） 相比语言模型输出任意句子的概率，翻译模型会输出句子的英文翻译（编号5），这取决于输入的法语句子（编号6）。即估计一个英文翻译的概率，比如估计”Jane is visiting Africa in September.“翻译的概率，这句翻译是取决于法语句子，”Jane visite I’Afrique en septembre.“，这就是英语句子相对于输入的法语句子的可能性，是一个条件语言模型 模型将法语翻译成英文，通过输入的法语句子模型将会告诉你各种英文翻译所对应的可能性 $x$是法语句子”Jane visite l’Afrique en septembre.“，它将告诉你不同的英语翻译所对应的概率：从这个分布中进行取样得到P(y|x)，但不是从得到的分布中进行随机取样，而是要找到一个英语句子y（编号1），使得条件概率最大化： max\ P(y^{},y^{},\cdots,y^{}|x^{},x^{},\cdots,x^{})而解决这种问题最通用的算法就是束搜索(Beam Search) 为什么不用贪心搜索(Greedy Search)： 贪心搜索生成第一个词的分布以后，将会根据条件语言模型挑选出最有可能的第一个词进入机器翻译模型中，在挑选出第一个词之后将会继续挑选出最有可能的第二个词……这种算法就叫做贪心搜索，但是真正需要的是一次性挑选出整个单词序列，从y^{}、y^{}到y^{}来使得整体的概率最大化。所以贪心算法先挑出最好的第一个词，在这之后再挑最好的第二词，然后再挑第三个，这种方法并不管用 第一串（编号1）翻译明显比第二个（编号2）好，但如果贪心算法挑选出了”Jane is“作为前两个词，因为在英语中going更加常见，于是对于法语句子来说”Jane is going“相比”Jane is visiting“会有更高的概率作为法语的翻译，所以如果仅仅根据前两个词来估计第三个词的可能性，得到的更可能是going，最终得到一个欠佳的句子 当想得到单词序列y^{}、y^{}一直到最后一个词总体的概率时，一次仅仅挑选一个词并不是最佳的选择。如果字典中有10,000个单词，翻译有10个词，可能的组合就有10,000的10次方这么多，从这样大一个字典中来挑选单词，句子数量非常巨大，大大增加了运算成本，降低运算速度，不可能去计算每一种组合的可能性 所以最常用的办法就是用一个近似的搜索算法，它会尽力地将挑选出句子y使得条件概率最大化，尽管不能保证找到的y值一定可以使概率最大化 机器翻译模型和之前的语言模型一个主要的区别就是：相比之前的模型随机地生成句子，该模型是找到最有可能的翻译 3.3 集束搜索（Beam Search）“Jane visite l’Afrique en Septembre.”翻译成英语”Jane is visiting Africa in September“.，集束搜索算法首先做的就是挑选要输出的英语翻译中的第一个单词。这里列出了10,000个词的词汇表，忽略大小写，在集束搜索的第一步中用这个网络来评估第一个单词的概率值，给定输入序列x，即法语作为输入，第一个输出y的概率值是多少 集束搜索会考虑多个选择，集束搜索算法会有一个参数B，叫做集束宽（beam width）。这个例子中集束宽设成3，意味着集束搜索一次会考虑3个可能结果，比如对第一个单词有不同选择的可能性，最后找到in、jane、september，是英语输出的第一个单词的最可能的三个选项，然后集束搜索算法会把结果存到计算机内存里以便后面尝试用这三个词。为了执行集束搜索的第一步，需要输入法语句子到编码网络，然后解码这个网络，softmax层会输出10,000个概率值，然后取前三个存起来，概率表示为： P(\hat y^{} | x)集束搜索算法的第二步：针对每个第一个单词考虑第二个单词是什么 为了评估第二个词的概率值，把y^{}设为单词in（编号3），输出就是y^{}（编号5），有了这个连接（编号6），这个网络就可以用来评估：在给定法语句子和翻译结果的第一个单词in的情况下第二个单词的概率 在第二步更关心的是要找到最可能的第一个和第二个单词对，所以不仅仅是第二个单词有最大的概率，而是第一个、第二个单词对有最大的概率（编号7）。可以表示成第一个单词的概率（编号8）乘以第二个单词的概率（编号9）： P(\hat y^{},\hat y^{}|x)=P(\hat y^{} | x)\cdot P(\hat y^{}|x,\hat y^{})jane、september跟上面一样 注意，如果集束搜索找到了第一个和第二个单词对最可能的三个选择是“in September”或者“jane is”或者“jane visits”，就意味着去掉了september作为英语翻译结果的第一个单词的选择，第一个单词现在减少到了两个可能结果，但是集束宽是3，还是有y^{}，y^{}对的三个选择 接着，再预测第三个单词。分别以in september，jane is，jane visits为条件，计算每个词汇表单词作为预测第三个单词的概率。从中选择概率最大的3个作为第三个单词的预测值，得到：in september jane，jane is visiting，jane visits africa 概率表示为： P(\hat y^{}|x,\hat y^{},\hat y^{})此时，得到的前三个单词的3种情况的概率为： P(\hat y^{},\hat y^{},\hat y^{}|x)=P(\hat y^{} | x)\cdot P(\hat y^{}|x,\hat y^{})\cdot P(\hat y^{}|x,\hat y^{},\hat y^{})以此类推，每次都取概率最大的三种预测。最后，选择概率最大的那一组作为最终的翻译语句： Jane is visiting Africa in September. 如果参数B=1，则就等同于greedy search。实际应用中，可以根据不同的需要设置B为不同的值。一般B越大，机器翻译越准确，但同时也会增加计算复杂度 3.4 改进集束搜索（Refinements to Beam Search）长度归一化（Length normalization）是对束搜索算法稍作调整的一种方式，使之得到更好的结果 束搜索就是最大化概率P(y^{< 1 >}\ldots y^{< T_{y}>}|X)，表示成: P(y^{}|X)P(y^{< 2 >}|X,y^{< 1 >})P(y^{< 3 >}|X,y^{< 1 >},y^{< 2>})\cdots P(y^{< T_{y} >}|X,y^{},y^{}\ldots y^{< T_{y} - 1 >})即乘积概率（the product probabilities）： arg\ max\prod_{t=1}^{T_y} P(\hat y^{< t>}|x,\hat y^{},\cdots,\hat y^{})这些概率值通常都远小于1，会造成数值下溢（numerical underflow），即数值太小了，导致电脑的浮点表示不能精确地储存 因此在实践中,不会最大化这个乘积，而是取log值： arg\ max \sum_{t=1}^{T_y}\log P(\hat y^{}|x,\hat y^{},\cdots,\hat y^{})会得到一个数值上更稳定的算法，不容易出现数值的舍入误差（rounding errors）或者说数值下溢（numerical underflow） 参照原来的目标函数（this original objective），如果有一个很长的句子，那么这个句子的概率会很低，因为乘了很多项小于1的数字来估计句子的概率，就会得到一个更小的概率值，所以可能不自然地倾向于简短的翻译结果，因为短句子的概率是由更少数量的小于1的数字乘积得到的，所以乘积不会那么小。概率的log值也有同样的问题 解决：可以把它归一化，通过除以翻译结果的单词数量。即取每个单词的概率对数值的平均，这样很明显地减少了对输出长的结果的惩罚： arg\ max\ \frac{1}{T_y}\sum_{t=1}^{T_y}\log P(\hat y^{< t>}|x,\hat y^{},\cdots,\hat y^{})在实践中，会用一个更柔和的方法（a softer approach），在T_{y}上加上指数\alpha： arg\ max\ \frac{1}{T_y^{\alpha}}\sum_{t=1}^{T_y}\log P(\hat y^{< t>}|x,\hat y^{},\cdots,\hat y^{})$\alpha$可以等于0.7。如果\alpha等于1，就相当于完全用长度来归一化，如果\alpha等于0，T_{y}的0次幂就是1，就相当于完全没有归一化，\alpha就是算法另一个超参数（hyper parameter） 总结一下如何运行束搜索算法： 当运行束搜索时，会看到很多长度分别等于1、2、3…的句子等等，针对这些所有的可能的输出句子，取概率最大的几个句子，然后对这些句子计算目标函数arg\ max\ \frac{1}{T_y^{\alpha}}\sum_{t=1}^{T_y}\log P(\hat y^{}|x,\hat y^{},\cdots,\hat y^{})，最后从经过评估的这些句子中挑选出在归一化的log 概率目标函数上得分最高的一个，也叫作归一化的对数似然目标函数（a normalized log likelihood objective） 如何选择束宽B： B越大，考虑的选择越多，找到的句子可能越好，但是算法的计算代价越大，因为要把很多的可能选择保存起来，内存占用增大 如果用小的束宽B，结果会没那么好，因为在算法运行中，保存的选择更少，但是算法运行的更快，内存占用也小 在产品中，经常可以看到把束宽设到10，当B很大的时候，性能提高会越来越少。对于很多应用来说，从束宽1，也就是贪心算法，到束宽为3、到10，会看到一个很大的改善。但是当束宽从1000增加到3000时，效果就没那么明显 相对广度优先搜索（BFS, Breadth First Search algorithms），深度优先搜索（DFS, Depth First Search）这些精确的搜索算法（exact search algorithms），束搜索运行的更快，但是不能保证一定能找到argmax的准确的最大值 3.5 集束搜索的误差分析（Error analysis in beam search）束搜索算法是一种近似搜索算法（an approximate search algorithm），也被称作启发式搜索算法（a heuristic search algorithm），它不总是输出可能性最大的句子，它仅记录着B为前3或者10或是100种可能 人工标记为y^*。束搜索算法翻译结果标记为\hat y，是一个十分糟糕的翻译，改变了句子的原意： 模型有两个主要部分，一个是神经网络模型，或说是序列到序列模型（sequence to sequence model），称作是RNN模型，另一部分是束搜索算法，以某个集束宽度B运行 RNN(循环神经网络)实际上是个编码器和解码器（the encoder and the decoder），它会计算P(y|x)。如对于句子：Jane visits Africa in September，将Jane visits Africa填入这里（上图编号1），忽略字母的大小写，后面也是一样，计算得到P(y^*|x)，P(\hat y|x) 同样如此，然后比较一下这两个值哪个更大 若P(y^*|x) 大于P(\hat y|x)，可束搜索算法却选择了\hat y ， 因此能够得出束搜索算法实际上不能给出使P(y|x)最大化的y值，因为束搜索算法的任务就是寻找一个y的值来使这项更大，但是它却选择了\hat y，而y^*实际上能得到更大的值。因此这种情况下束搜索算法出错 若P(y^*|x)小于或等于P(\hat y|x)，y^* 是比 \hat y更好的翻译结果，不过根据RNN模型的结果，P(y^*) 是小于P(\hat y)的，即相比于\hat y，y^*成为输出的可能更小。因此在这种情况下是RNN模型出了问题 以上都忽略了长度归一化（length normalizations）的细节，如果用了某种长度归一化，那么要比较长度归一化后的最优化目标函数值 误差分析过程： 先遍历开发集，找出算法产生的错误 假如P(y^*|x)的值为2 x 10^{-10}，而P(\hat y|x)的值为 1 x10^{-10}，得知束搜索算法实际上选择了比y^*可能性更低的\hat y，则束搜索算法出错，缩写为B 接着继续遍历第二个错误，若对于第二个例子是RNN模型出现了问题，用缩写R来代表RNN 接着遍历更多的例子，有时是束搜索算法出现了问题，有时是模型出现了问题，等等 执行误差分析，得出束搜索算法和RNN模型出错的比例是多少。对开发集中每一个错误例子，即算法输出了比人工翻译更差的结果的情况，尝试确定是搜索算法出了问题，还是生成目标函数(束搜索算法使之最大化)的RNN模型出了问题。找到这两个部分中哪个是产生更多错误的原因 只有当发现是束搜索算法造成了大部分错误时，才值得花费努力增大集束宽度B；如果发现是RNN模型出了更多错，那么可以进行更深层次的分析，来决定是需要增加正则化还是获取更多的训练数据，抑或是尝试一个不同的网络结构 3.6 Bleu 得分（选修）（Bleu Score (optional)）机器翻译（machine translation）的一大难题是一个法语句子可以有多种英文翻译而且都同样很好，常见的解决办法是通过一个BLEU得分（the BLEU score）的东西来解决，BLEU得分是一个有用的单一实数评估指标，用于评估生成文本的算法，判断输出的结果是否与人工写出的参考文本的含义相似 一般有多个人工翻译： BLEU得分做的就是给定一个机器生成的翻译，它能够自动地计算一个分数来衡量机器翻译的好坏。只要机器生成的翻译与任何一个人工翻译的结果足够接近，那么它就会得到一个高的BLEU分数。BLEU代表bilingual evaluation understudy(双语评估替补)。且这些人工翻译的参考会包含在开发集或是测试集中 假设机器翻译 (MT)的输出是：the the the the the the the，是一个十分糟糕的翻译。衡量机器翻译输出质量的方法之一是观察输出结果的每一个词，看其是否出现在参考中，这被称做是机器翻译的精确度（a precision of the machine translation output）。这个情况下，机器翻译输出了七个单词并且这七个词中的每一个都出现在了参考1或是参考2，因此输出的精确度就是7/7，分母为机器翻译单词数目，分子为相应单词是否出现在参考翻译中。但是，这种方法很不科学，并不可取 改良后的精确度评估方法（the modified precision measure）：把每一个单词的记分上限定为它在参考句子中出现的最多次数。在参考1中，单词the出现了两次，在参考2中，单词the只出现了一次。单词the的得分上限为2。输出句子的得分为2/7，分母是7个词中单词the总共出现的次数，分子是单词the出现的计数，在达到上限时截断计数 上述都只是关注单独的单词，在BLEU得分中，另外一种更科学的打分方法是bleu score on bigrams（二元词组），bigram的意思就是相邻的两个单词 定义截取计数（the clipped count），也就是Count_clip：给算法设置得分上限，上限值为二元词组出现在参考1或2中的最大次数 假定机器翻译输出了稍微好一点的翻译，对MT output进行分解，得到的bigrams及其出现在MT output中的次数count为： 相应的bigrams precision为4/6也就是2/3，为二元词组改良后的精确度 将改良后的一元词组精确度定义为P_1，P代表的是精确度。下标1的意思是一元词组，即考虑单独的词，P_n 定义为n元词组精确度，用n-gram替代掉一元词组。即机器翻译输出中的n元词组的countclip之和除以n元词组的出现次数之和 如果机器翻译输出与参考1或是参考2完全一致，那么所有的P_1、P_2等等的值，都会等于1.0 最终的BLEU得分： 将得到的P_1，P_2， P_3…P_n 相加再取平均值 BLEU得分被定义为： p=exp (\frac{1}{n}\sum\limits_{i=1}^{n}{P_i}) 然后用BP（“简短惩罚”brevity penalty） 的惩罚因子（the BP penalty）来调整。它能够惩罚输出了太短翻译结果的翻译系统： p=BP\cdot exp(\frac1n\sum_{i=1}^np_i)BLEU得分被用来评估许多生成文本的系统（systems that generate text），比如说机器翻译系统（machine translation systems），图像描述系统（image captioning systems）。不过它并没有用于语音识别（speech recognition）。因为在语音识别当中，通常只有一个答案 3.7 注意力模型直观理解（Attention Model Intuition）给定一个很长的法语句子，在神经网络中，绿色的编码器要做的就是读整个句子，然后记忆整个句子，再在感知机中传递。紫色的神经网络，即解码网络（the decoder network）将生成英文翻译 对于短句子效果非常好，会有一个相对高的Bleu分（Bleu score），但是对于长句子而言，比如说大于30或者40词的句子，它的表现就会变差。Bleu评分随着单词数量变化，短的句子会难以翻译，因为很难得到所有词。对于长的句子，效果也不好，因为在神经网络中，记忆非常长句子是非常困难的。 而注意力模型翻译得很像人类，一次翻译句子的一部分。且机器翻译系统只会翻译句子的一部分，不会有一个巨大的下倾（huge dip），这个下倾衡量了神经网络记忆一个长句子的能力 对于句子里的每五个单词，使用双向的RNN（a bidirectional RNN），使用另一个RNN生成英文翻译： S^{}$由原语句附近单元共同决定，注意力权重（attention weights）\alpha^{< t,t'>} 表示尝试生成第t个英文词时应该花多少注意力在第t'个法语词上面。直到最终生成< EOS>。离得越近，注意力权重越大，相当于当前的注意力区域有个滑动窗。c表示编码器激活函数在注意力权重加权后的结果，将c输入到解码器用来生成翻译语句， 同时上一个时间步输出的翻译结果也加入 3.8 注意力模型（Attention Model）注意力模型让一个神经网络只注意到一部分的输入句子。当它在生成句子的时候，更像人类翻译 假定有一个输入句子，并使用双向的RNN，或者双向的GRU或者双向的LSTM，去计算每个词的特征： 用a^{}\cdot a^{}是输出\hat y^{}在t{'}时对RNN单元花在a^{< t{ '}>}上的注意力权重因子。即在t处生成输出词应该花多少注意力在第t{'}个输入词上面 为了让\alpha^{}，使得： a^{})}{\sum^{T_x}_{t'=1} \text{exp}(e^{}，就能得到\alpha^{}： 建立一个简单的神经网络 输入s^{}，即神经网络在上个时间步的状态和a^{} 和\alpha^{]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周 循环序列模型（Recurrent Neural Networks）(Course 5)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%B8%80%E5%91%A8-%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%88Recurrent-Neural-Networks%EF%BC%89-Course-5%2F</url>
    <content type="text"><![CDATA[1.1 为什么选择序列模型？（Why Sequence Models?）语音识别：给定一个输入音频片段 X，要求输出对应的文字记录 Y。输入和输出数据都是序列模型，因为 X是一个按时播放的音频片段，输出 Y是一系列单词音乐生成问题：只有输出数据 Y是序列，而输入数据可以是空集，也可以是个单一的整数，这个数可能指代想要生成的音乐风格，或者想要生成的那首曲子的头几个音符 处理情感分类：输入数据 X是序列，会得到类似这样的输入：“There is nothing to like in this movie.”，你认为这句评论对应几星？ DNA序列分析：DNA用A、C、G、T四个字母来表示。给定一段DNA序列，能够标记出哪部分是匹配某种蛋白质？ 机器翻译：输入句：“Voulez-vou chante avecmoi?”（法语：要和我一起唱么？），要求输出另一种语言的翻译结果 视频行为识别：得到一系列视频帧，要求识别其中的行为 命名实体识别：给定一个句子，要求识别出句中的人名 这些问题都可以被称作使用标签数据 (X,Y)作为训练集的监督学习。但序列问题有很多不同类型。有些问题里，输入数据 X和输出数据Y都是序列，但就算在那种情况下，X和Y有时也会不一样长。或者像上图编号1和编号2所示的X和Y有相同的数据长度。在另一些问题里，只有 X或者只有Y是序列 1.2 数学符号（Notation）序列模型的输入语句是：“Harry Potter and Herminoe Granger invented a new spell.”。假如想要建立一个能够自动识别句中人名位置的序列模型，那么这就是一个命名实体识别问题 该句话包含9个单词，输出y即为1 x 9向量，每位表征对应单词是否为人名的一部分，对应的输出y表示为： y=[1\ \ 1\ \ 0\ \ 1\ \ 1\ \ 0\ \ 0\ \ 0\ \ 0]^T$y^{&lt; t&gt;}$表示序列对应位置的输出，T_y表示输出序列长度，1\leq t\leq T_y 对于输入x，表示为： [x^{}\ \ x^{}\ \ x^{}\ \ x^{}\ \ x^{}\ \ x^{}\ \ x^{}\ \ x^{}\ \ x^{}]$x^{&lt; t&gt;}$表示序列对应位置的输入，T_x表示输入序列长度。此例中T_x=T_y，但是也存在T_x\neq T_y 如何表示每个x^{}： 建立一个词汇库vocabulary，尽可能包含更多的词汇。例如一个包含10000个词汇的词汇库为： \left[ \begin{matrix} a \\ and \\ \cdot \\ \cdot \\ \cdot \\ harry \\ \cdot \\ \cdot \\ \cdot \\ potter \\ \cdot \\ \cdot \\ \cdot \\ zulu \end{matrix} \right]然后使用one-hot编码，词汇表中与x^{}对应的位置为1，其它位置为0。如果出现词汇表之外的单词，可以使用UNK或其他字符串来表示 对于多样本：对应的命名规则可表示为：X^{(i)}，Y^{(i)}，T_x^{(i)}，T_y^{(i)}，i表示第i个样本。不同样本的T_x^{(i)}或T_y^{(i)}都有可能不同 1.3 循环神经网络模型（Recurrent Neural Network Model）序列模型从左到右，依次传递，此例中，T_x=T_y。x^{}到\hat y^{}之间是隐藏神经元。a^{}会传入到第t+1元素中，作为输入。其中，a^{}一般为零向量 循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的，用W_{\text{ax}}来表示管理着从x^{}到隐藏层的连接的一系列参数，每个时间步使用的都是相同的参数W_{\text{ax}}。而激活值是由参数W_{aa}决定，同时每一个时间步都使用相同的参数W_{aa}，同样的输出结果由W_{\text{ya}}决定： 预测{\hat{y}}^{< 3 >}时，不仅要使用x^{}的信息，还要使用来自x^{}和x^{}的信息，而这个循环神经网络的一个缺点是只使用了这个序列中之前的信息来做出预测，因为如果给定了这个句子，“Teddy Roosevelt was a great President.”，为了判断Teddy是否是人名的一部分，仅仅知道句中前两个词是完全不够的，还需要知道句中后部分的信息，因为句子也可能是这样的，“Teddy bears are on sale!”。因此如果只给定前三个单词，是不可能确切地知道Teddy是否是人名的一部分，第一个例子是人名，第二个例子就不是，所以不可能只看前三个单词就能分辨出其中的区别 所以这样特定的神经网络结构的一个限制是它在某一时刻的预测仅使用了从序列之前的输入信息，并没有使用序列中后部分的信息 RNN的正向传播（Forward Propagation）过程为： a^{}=g(W_{aa}\cdot a^{}+W_{ax}\cdot x^{}+b_a) \hat y^{}=g(W_{ya}\cdot a^{}+b_y) 为了简化表达式，可以对a^{}项进行整合： W_{aa}\cdot a^{}+W_{ax}\cdot x^{}=[W_{aa}\ \ W_{ax}]\left[ \begin{matrix} a^{} \\ x^{} \end{matrix} \right]\rightarrow W_a[a^{},x^{}]则正向传播可表示为： a^{}=g(W_a[a^{},x^{}]+b_a) \hat y^{}=g(W_{y}\cdot a^{}+b_y)) RNN前向传播示意图： 1.4 通过时间的反向传播（Backpropagation through time）反向传播计算方向与前向传播基本上是相反： 识别人名的例子，经过RNN正向传播，单个元素的Loss function为： L^{}(\hat y^{},y^{})=-y^{}log\ \hat y^{}-(1-y^{})log\ (1-\hat y^{}) 这是 binary classification 的 Loss Function，注意与1.6 的softmax Loss Function区别 该样本所有元素的Loss function为： L(\hat y,y)=\sum_{t=1}^{T_y}L^{}(\hat y^{},y^{})反向传播（Backpropagation）过程就是从右到左分别计算L(\hat y,y)对参数W_{a}，W_{y}，b_a，b_y的偏导数，这种从右到左的求导过程被称为Backpropagation through time RNN反向传播示意图： 1.5 不同类型的循环神经网络（Different types of RNNs）RNN模型包含以下几个类型： 一对一，当去掉a^{}时它就是一种标准类型的神经网络 一对多，比如音乐生成或者序列生成 多对一，如是情感分类的例子，首先读取输入，一个电影评论的文本，然后判断他们是否喜欢电影还是不喜欢 多对多，如命名实体识别，T_{x}=T_{y} 多对多，如机器翻译，T_x\neq T_y 1.6 语言模型和序列生成（Language model and sequence generation）在语音识别中，某句语音有两种翻译： the apple and pair salad the apple and pear salad 语言模型会计算出这两句话各自的出现概率： P( \text{The apple and pair salad}) = 3.2 \times 10^{-13} P\left(\text{The apple and pear salad} \right) = 5.7 \times 10^{-10} 选择概率最大的语句作为正确的翻译 概率计算的表达式为： P(y^{},y^{},\cdots,y^{})如何使用RNN构建语言模型： 首先需要一个足够大的训练集，训练集由大量的单词语句语料库（corpus）构成。然后，对corpus的每句话进行切分词（tokenize），建立vocabulary，对每个单词进行one-hot编码。例如下面这句话： The Egyptian Mau is a bread of cat. 每句话结束末尾，需要加上&lt; EOS &gt;作为语句结束符。若语句中有词汇表中没有的单词，用&lt; UNK &gt;表示。假设单词“Mau”不在词汇表中，则上面这句话可表示为： The Egyptian &lt; UNK &gt; is a bread of cat. &lt; EOS &gt; 准备好训练集并对语料库进行切分词等处理之后，接下来构建相应的RNN模型： $x^{}$和a^{}均为零向量。Softmax输出层\hat y^{}表示出现该语句第一个单词的概率，softmax输出层\hat y^{}表示在第一个单词基础上出现第二个单词的概率，即条件概率，以此类推，最后是出现&lt; EOS &gt;的条件概率 单个元素的softmax loss function为： L^{}(\hat y^{},y^{})=-\sum_iy_i^{}log\ \hat y_i^{} 这是softmax Loss Function ，注意与1.4 binary classification 的 Loss Function区别 该样本所有元素的Loss function为： L(\hat y,y)=\sum_tL^{}(\hat y^{},y^{})对语料库的每条语句进行RNN模型训练，最终得到的模型可以根据给出语句的前几个单词预测其余部分，将语句补充完整。例如给出“Cats average 15”，RNN模型可能预测完整的语句是“Cats average 15 hours of sleep a day.” 整个语句出现的概率等于语句中所有元素出现的条件概率乘积。例如某个语句包含y^{},y^{},y^{}，则整个语句出现的概率为： P(y^{},y^{},y^{})=P(y^{})\cdot P(y^{}|y^{})\cdot P(y^{}|y^{},y^{})1.7 对新序列采样（Sampling novel sequences）基于词汇的RNN模型序列模型模拟了任意特定单词序列的概率，要做的是对这些概率分布进行采样来生成一个新的单词序列。编号1所示的网络已经被上方所展示的结构训练过，编号2是进行采样 第一步要做的是对想要模型生成的第一个词进行采样，输入x^{} =0，a^{} =0，第一个时间步得到的是所有可能的输出，是经过softmax层后得到的概率，然后根据这个softmax的分布进行随机采样。对这个向量使用np.random.choice，来根据向量中这些概率的分布进行采样，就能对第一个词进行采样 然后继续下一个时间步，\hat y^{}作为输入（编号4），然后softmax层就会预测\hat y^{}是什么。假如第一个词进行抽样后得到的是The，现在要计算出在第一词是The的情况下，第二个词应该是什么（编号5），然后再用采样函数来对\hat y^{}进行采样 即无论得到什么样的用one-hot码表示的选择结果，都把它传递到下一个时间步，然后进行采样，直到最后一个时间步 怎样知道一个句子结束： 如果代表句子结尾的标识在字典中，可以一直进行采样直到得到EOS标识（编号6），代表着已经抵达结尾，可以停止采样 如果字典中没有这个词，可以决定从20个或100个或其他个单词进行采样，然后一直将采样进行下去直到达到所设定的时间步。不过这种过程有时候会产生一些未知标识（编号7），如果要确保算法不会输出这种标识，要做的是拒绝采样过程中产生任何未知的标识，一旦出现就继续在剩下的词中进行重采样，直到得到一个不是未知标识的词 这就是如何从RNN语言模型中生成一个随机选择的句子。以上所建立的是基于词汇的RNN模型，意思就是字典中的词都是英语单词（下图编号1） 基于字符的RNN结构用以上字符组成字典（上图编号2所示） 序列\hat y^{}，\hat y^{}，\hat y^{}在训练数据中是单独的字符，对于“Cats average 15 hours of sleep a day.”，C是\hat y^{}，a就是\hat y^{}，t就是\hat y^{}等等 优点： 不必担心会出现未知的标识，基于字符的语言模型会将Mau这样的序列也视为可能性非零的序列。而基于词汇的语言模型，如果Mau不在字典中，只能当作未知标识UNK 缺点： 最后会得到太多太长的序列，基于字符的语言模型在捕捉句子中的依赖关系也就是句子较前部分如何影响较后部分不如基于词汇的语言模型那样可以捕捉长范围的关系，并且基于字符的语言模型训练起来计算成本比较高 1.8 循环神经网络的梯度消失（Vanishing gradients withRNNs） 编号1cat是单数，应该用was，编号2 cats是复数，用were 这个例子中的句子有长期的依赖，最前面的单词对句子后面的单词有影响。但基本的RNN模型（编号3）不擅长捕获长期依赖效应 RNN反向传播很困难，会有梯度消失的问题，后面层的输出误差（编号6）很难影响前面层（编号7）的计算。即很难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式的was或者were。且在英语里面中间的内容（编号8）可以任意长，所以基本的RNN模型会有很多局部影响，输出\hat y^{}主要受附近的值（编号10）的影响，编号6所示的输出很难受到序列靠前的输入（编号10）的影响，因为不管输出是什么，对的还是错的，这个区域都很难反向传播到序列的前面部分，也因此网络很难调整序列前面的计算 在反向传播的时候，随着层数的增多，梯度不仅可能指数型的下降，也可能指数型的上升。梯度消失在训练RNN时是首要的问题，不过梯度爆炸也会出现，但是梯度爆炸很明显，因为指数级大的梯度会让参数变得极其大，以至于网络参数崩溃。参数大到崩溃会看到很多NaN，或者不是数字的情况，这意味着网络计算出现了数值溢出 解决方法：用梯度修剪。梯度向量如果大于某个阈值，缩放梯度向量，保证不会太大 1.9 GRU单元（Gated Recurrent Unit（GRU））门控循环单元：改变了RNN的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题 简化的GRU模型RNN隐藏层的单元的可视化： $a^{&lt; t&gt;}$表达式为： a^{}=tanh(W_a[a^{},x^{}]+b_a)为了解决梯度消失问题，对上述单元进行修改，添加了记忆单元，构建GRU，如下图所示： 表达式为： \tilde c^{}=tanh(W_c[c^{},x^{}]+b_c) \Gamma_u=\sigma(W_u[c^{},x^{}]+b_u) c^{}=\Gamma_u*\tilde c^{}+(1-\Gamma_u)*c^{}$c^{}=a^{}$，c^{}=a^{}。符号c表示记忆细胞的值，a表示输出的激活值，\tilde c^{}是个候选值，替代了c的值，\Gamma_u（0到1）意为gate，u表示“update”，当\Gamma_u=1时，代表更新；当\Gamma_u=0时，代表记忆，保留之前的模块输出。\Gamma_u能够保证RNN模型中跨度很大的依赖关系不受影响，消除梯度消失问题 完整的GRU完整的GRU添加了另外一个gate，即\Gamma_r，表达式如下： \tilde c^{}=tanh(W_c[\Gamma_r*c^{},x^{}]+b_c) \Gamma_u=\sigma(W_u[c^{},x^{}]+b_u) \Gamma_r=\sigma(W_r[c^{},x^{}]+b_r) c^{}=\Gamma_u*\tilde c^{}+(1-\Gamma_u)*c^{} a^{}=c^{}$\Gamma_{r}$门：计算出的下一个c^{}的候选值{\tilde{c}}^{}跟c^{}有多大的相关性 $c^{&lt; t&gt;}$可以是一个向量（编号1），如果有100维的隐藏的激活值，那么c^{}、{\tilde{c}}^{}、\Gamma_{u}还有画在框中的其他值也是100维 $*$实际上就是元素对应的乘积（c^{< t>}=\Gamma_u*\tilde c^{< t>}+(1-\Gamma_u)*c^{}），若\Gamma_{u}（\Gamma_u=\sigma(W_u[c^{},x^{< t>}]+b_u)）是一个100维的向量，而里面的值几乎都是0或者1，则这100维的记忆细胞c^{< t>}（c^{< t>}=a^{< t>}，编号1）就是要更新的比特 1.10 长短期记忆（LSTM（long short term memory）unit）LSTM是另一种更强大的解决梯度消失问题的方法。它对应的RNN隐藏层单元结构如下图所示： 相应的表达式为： \tilde c^{}=tanh(W_c[a^{},x^{}]+b_c) \Gamma_u=\sigma(W_u[a^{},x^{< t>}]+b_u) \Gamma_f=\sigma(W_f[a^{},x^{< t>}]+b_f) \Gamma_o=\sigma(W_o[a^{},x^{< t>}]+b_o) c^{< t>}=\Gamma_u*\tilde c^{< t>}+\Gamma_f*c^{} a^{< t>}=\Gamma_o*c^{< t>}LSTM包含三个gates：\Gamma_u,\Gamma_f,\Gamma_o，分别对应update gate，forget gate和output gate 在LSTM中不再有a^{< t>} = c^{< t>}的情况 红线显示了只要正确地设置了遗忘门和更新门，LSTM很容易把c^{}的值一直往下传递到右边，比如c^{} = c^{}。这就是为什么LSTM和GRU非常擅长于长时间记忆某个值 “窥视孔连接”（peephole connection）:门值不仅取决于a^{}和x^{< t>}，也取决于上一个记忆细胞的值（c^{}），即c^{}也能影响门值 如果考虑c^{}对\Gamma_u,\Gamma_f,\Gamma_o的影响，可加入“窥视孔连接”，对LSTM的表达式进行修改： \tilde c^{< t>}=tanh(W_c[a^{},x^{< t>}]+b_c) \Gamma_u=\sigma(W_u[a^{},x^{< t>},c^{}]+b_u) \Gamma_f=\sigma(W_f[a^{},x^{< t>},c^{}]+b_f) \Gamma_o=\sigma(W_o[a^{},x^{< t>},c^{}]+b_o) c^{< t>}=\Gamma_u*\tilde c^{< t>}+\Gamma_f*c^{} a^{< t>}=\Gamma_o*c^{}LSTM主要的区别：比如（上图编号13）有一个100维的隐藏记忆细胞单元，第i个c^{}的元素只会影响第i个元素对应的那个门，所以关系是一对一的，并不是任意这100维的c^{}可以影响所有的门元素 LSTM前向传播图： GRU：模型简单，更容易创建一个更大的网络，只有两个门，在计算上运行得更快，且可以扩大模型的规模 LSTM：更加强大和灵活，因为它有三个门而不是两个 1.11 双向循环神经网络（Bidirectional RNN）双向RNN模型在序列的某点处不仅可以获取之前的信息，还可以获取未来的信息 用只有4个单词的句子，x^{}到x^{}。这个网络有一个前向的循环单元a^{\rightarrow }，a^{\rightarrow }，a^{\rightarrow }，a^{\rightarrow }，这四个循环单元都有一个当前输入x输入进去，得到预测的\hat y^{}，\hat y^{}，\hat y^{}和\hat y^{} 再增加一个反向循环层：a^{\leftarrow }，a^{\leftarrow }，a^{\leftarrow }，a^{\leftarrow } 给定一个输入序列x^{}到x^{}，这个序列先后计算前向的a^{\rightarrow }，a^{\rightarrow }，a^{\rightarrow }，a^{\rightarrow }，而反向序列从a^{\leftarrow }开始，计算完了反向的a^{\leftarrow }，可以用这些激活值计算反向a^{\leftarrow },a^{\leftarrow },a^{\leftarrow } 值得注意的是计算的是网络激活值，这不是反向传播而是前向的传播，图中前向传播一部分计算是从左到右，一部分计算是从右到左。把所有激活值都计算完了就可以计算预测结果 预测结果： \hat y^{} =g(W_{g}\left\lbrack a^{\rightarrow },a^{\leftarrow } \right\rbrack +b_{y})这些基本单元不仅仅是标准RNN单元，也可以是GRU单元或者LSTM单元 双向RNN网络模型的缺点是需要完整的数据的序列才能预测任意位置。比如要构建一个语音识别系统，双向RNN模型需要等待整个语音说完，获取整个语音表达才能处理这段语音，并进一步做语音识别 1.12 深层循环神经网络（Deep RNNs） $a^{\lbrack l\rbrack }$表示第l层的激活值，&lt;t&gt;表示第t个时间点 激活值a^{[l]< t>}有两个输入: a^{[l]< t>}=g(W_a^{[l]}[a^{[l]},a^{[l-1]< t>}]+b_a^{[l]})对于RNN来说，有三层就已经不少了。由于时间的维度，RNN网络会变得相当大，即使只有很少的几层 另外一种Deep RNNs结构是每个输出层上还有一些垂直单元： 即把输出去掉（编号1），在每一个上面堆叠循环层，然后换成一些深的层，这些层并不水平连接，只是一个深层的网络，然后用来预测y^{< t>} 这些单元（编号3）没必要是标准的RNN，也可以是GRU单元或者LSTM单元，也可以构建深层的双向RNN网络，但深层的RNN训练需要很多计算资源，需要很长的时间]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周 自然语言处理与词嵌入（Natural Language Processing and Word Embeddings）(Course 5)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%BA%8C%E5%91%A8-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%EF%BC%88Natural-Language-Processing-and-Word-Embeddings%EF%BC%89-Course-5%2F</url>
    <content type="text"><![CDATA[2.1 词汇表征（Word Representation）one-hot向量表示：单词Man，Woman，King，Queen，Apple，Orange分别出现在词汇表的第5391，9853，4914，7157，456，6257的位置，则它们分别用O_{5391},O_{9853} 等表示，O代表one-hot： 缺点是把每个词孤立起来，使得算法对相关词的泛化能力不强 因为任何两个one-hot向量的内积都是0，例如king和queen，词性相近，但是单从one-hot编码上来看，内积为零，无法知道二者的相似性 因此用特征表征（Featurized representation）的方法对每个单词进行编码。也就是使用一个特征向量表征单词，特征向量的每个元素都是对该单词某一特征的量化描述，量化范围可以是[-1,1]之间，而单词使用这种高维特征表示时，就叫做词嵌入（word embedding）， 词嵌入可以让算法自动的理解一些类似的词，比如男人对女人，国王对王后： 以上举例的特征实际上并不是手工设计的，而是算法（word embedding）学习而来；而且这些学习的特征，可能并不具有良好的解释性，但无论如何，算法都可以快速找到哪些单词是类似的 特征向量的长度依情况而定，特征元素越多则对单词表征得越全面。这里的特征向量长度设定为300。使用特征表征之后，词汇表中的每个单词都可以使用对应的300 x 1的向量来表示，该向量的每个元素表示该单词对应的某个特征值。每个单词用e+词汇表索引的方式标记，例如e_{5391}，e_{9853}，e_{4914}，e_{7157}，e_{456}，e_{6257} 用这种表示方法来表示apple和orange这些词，那么apple和orange的这种表示肯定会非常相似，可能有些特征不太一样，如颜色口味，但总的来说apple和orange的大部分特征实际上都一样，或者说都有相似的值。这样对于已经知道orange juice的算法很大几率上也会明白apple juice这个东西，这样对于不同的单词算法会泛化的更好 如果能够学习到一个300维的特征向量，或者说300维的词嵌入，把这300维的数据嵌入到一个二维空间里，就可以可视化了。常用的可视化算法是t-SNE算法，会发现man和woman这些词聚集在一块，king和queen聚集在一块等等 在对这些概念可视化的时候，词嵌入算法对于相近的概念，学到的特征也比较类似，最终把它们映射为相似的特征向量 2.2 使用词嵌入（Using Word Embeddings）之前Named entity识别的例子（即找出语句中的人名），每个单词采用的是one-hot编码。RNN模型能确定Sally Johnson是一个人名而不是一个公司名，是因为“orange farmer”是份职业，很明显“Sally Johnson”是一个人名（输出1） 如果用特征化表示方法，即用词嵌入作为输入训练好的模型，如果一个新的输入：“Robert Lin is an apple farmer.”，因为知道orange和apple很相近，那么算法很容易就知道Robert Lin也是一个人的名字 featurized representation的优点是可以减少训练样本的数目，前提是对海量单词建立特征向量表述。即使训练样本不够多，测试时遇到陌生单词，例如“durian cultivator”，根据之前海量词汇特征向量就判断出“durian”也是一种水果，与“apple”类似，而“cultivator”与“farmer”也很相似。从而得到与“durian cultivator”对应的应该也是一个人名。这种做法将单词用不同的特征来表示，即使是训练样本中没有的单词，也可以根据word embedding的结果得到与其词性相近的单词，从而得到与该单词相近的结果，有效减少了训练样本的数量 词嵌入能够达到这种效果，原因是学习词嵌入的算法会考察非常大的文本集 词嵌入做迁移学习的步骤： 先从大量的文本集中学习词嵌入，或者下载网上预训练好的词嵌入模型 用这些词嵌入模型迁移到新的只有少量标注训练集的任务中，比如用300维的词嵌入来表示单词。好处就是可以用更低维度的特征向量代替原来的10000维的one-hot向量。尽管one-hot向量很快计算，但学到的用于词嵌入的300维的向量会更加紧凑 当在新的任务上训练模型，而在命名实体识别任务上只有少量的标记数据集，可以选择要不要继续微调，用新的数据调整词嵌入。但实际中只有第二步中有很大的数据集才会这样做，如果标记的数据集不是很大，通常不会在微调词嵌入上费力气 当任务的训练集相对较小时，词嵌入的作用最明显，所以它广泛用于NLP领域 词嵌入和人脸编码有很多相似性，训练了一个Siamese网络结构，这个网络会学习不同人脸的一个128维表示，然后通过比较编码结果来判断两个图片是否是同一个人脸，在人脸识别领域用编码指代向量f(x^{\left(i \right)})，f(x^{\left( j\right)})，词嵌入的意思和这个差不多 人脸识别领域和词嵌入不同就是： 在人脸识别中训练一个网络，任给一个人脸照片，甚至是没有见过的照片，神经网络都会计算出相应的一个编码结果 学习词嵌入则是有一个固定的词汇表，比如10000个单词，学习向量e_{1}到e_{10000}，学习一个固定的编码，即每一个词汇表的单词的固定嵌入 人脸识别中的算法未来可能涉及到海量的人脸照片，而自然语言处理有一个固定的词汇表，没有出现过的单词就记为未知单词 2.3 词嵌入的特性（Properties of Word Embeddings） 该例中，假设用的是四维的嵌入向量，假如向量e_{\text{man}}和e_{\text{woman}}、e_{\text{king}}和e_{\text{queen}} 分别进行减法运算，相减结果表明，“Man”与“Woman”的主要区别是性别，“King”与“Queen”也是一样 所以当算法被问及man对woman相当于king对什么时，算法所做的就是计算e_{\text{man}}-e_{\text{woman}}，然后找出一个向量也就是找出一个词，使得： e_{\text{man}}-e_{\text{woman}}\approx e_{\text{king}} - e_{?}即当这个新词是queen时，式子的左边会近似地等于右边 ]在图中，词嵌入向量在一个可能有300维的空间里，箭头代表的是向量在gender（性别）这一维的差，为了得出类比推理，计算当man对于woman，king对于什么，要做的就是找到单词w来使得 e_{\text{man}}-e_{\text{woman}}\approx e_{\text{king}} - e_{w}等式成立，即找到单词w来最大化e_{w}与e_{\text{king}} - e_{\text{man}} + e_{\text{woman}}的相似度，即 Find\ word\ w:argmax\ Sim(e_{w},e_{\text{king}} - e_{\text{man}} + e_{\text{woman}})即把e_{w}全部放到等式的一边，另一边是e_{\text{king}}- e_{\text{man}} + e_{\text{woman}}。应用相似度函数，通过方程找到一个使得相似度最大的单词，如果结果理想的话会得到单词queen t-SNE算法所做的就是把这些300维的数据用一种非线性的方式映射到2维平面上，可以得知t-SNE中这种映射很复杂而且很非线性。在大多数情况下，由于t-SNE的非线性映射，不能总是期望使等式成立的关系会像左边那样成一个平行四边形 关于相似函数，比较常用的是余弦相似度，假如在向量u和v之间定义相似度： Sim(u,v)=\frac{u^Tv}{||u||\cdot ||v||}分子是u和v的内积。如果u和v非常相似，那么它们的内积将会很大，把整个式子叫做余弦相似度，是因为该式是u和v的夹角的余弦值 参考资料： 给定两个向量u和v，余弦相似度定义如下： {CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta)$u.v$ 是两个向量的点积（或内积），||u||_2是向量u的范数（或长度）， \theta 是向量u和v之间的角度。这种相似性取决于角度在向量u和v之间。如果向量u和v非常相似，它们的余弦相似性将接近1; 如果它们不相似，则余弦相似性将取较小的值 两个向量之间角度的余弦是衡量它们有多相似的指标，角度越小，两个向量越相似 还可以计算Euclidian distance来比较相似性，即||u-v||^2。距离越大，相似性越小 2.4 嵌入矩阵（Embedding Matrix）当应用算法来学习词嵌入时，实际上是学习一个嵌入矩阵 假设某个词汇库包含了10000个单词，每个单词包含的特征维度为300，那么表征所有单词的embedding matrix维度为300 x 10000，用E来表示。某单词w的one-hot向量表示为O_w，维度为10000 x 1 则该单词的嵌入向量(embedding vector)表达式为： e_w=E\cdot O_w只要知道了embedding matrixE，就能计算出所有单词的embedding vector e_w 不过上述这种矩阵乘积运算E\cdot O_w效率并不高，矩阵维度很大，且O_w大部分元素为零。通常做法是直接从E中选取第w列作为e_w 2.5 学习词嵌入（Learning Word Embeddings）embedding matrix E可以通过构建自然语言模型，运用梯度下降算法得到。若输入样本是： I want a glass of orange (juice). 通过这句话的前6个单词，预测最后的单词“juice”。E未知待求，每个单词可用embedding vector e_w表示。构建的神经网络模型结构如下图所示： 神经网络输入层包含6个embedding vectors，每个embedding vector维度是300，则输入层总共有1800个输入。Softmax层有10000个概率输出，与词汇表包含的单词数目一致。正确的输出label是“juice”。其中$E,W^{[1]},b^{[1]},W^{[2]},b^{[2]}$为待求值。对足够的训练例句样本，运用梯度下降算法，迭代优化，最终求出embedding matrixE 这种算法的效果还不错，能够保证具有相似属性单词的embedding vector相近 为了让神经网络输入层数目固定，可以选择只取预测单词的前4个单词作为输入，例如该句中只选择“a glass of orange”四个单词作为输入。这里的4是超参数，可调 把输入叫做context，输出叫做target。对应到上面这句话里： context: a glass of orange target: juice 关于context的选择有多种方法： target前n个单词或后n个单词，n可调 target前1个单词 target附近某1个单词（Skip-Gram）E 事实证明，不同的context选择方法都能计算出较准确的embedding matrix E 2.6 Word2Vec选择context和target的方法中，比较流行的是采用Skip-Gram模型 Skip-Gram模型的做法是：首先随机选择一个单词作为context，例如“orange”；然后使用一个宽度为5或10（自定义）的滑动窗，在context附近选择一个单词作为target，可以是“juice”、“glass”、“my”等等。最终得到了多个context—target对作为监督式学习样本： 训练的过程是构建自然语言模型，经过softmax单元的输出为： \hat y=\frac{e^{\theta_t^T\cdot e_c}}{\sum_{j=1}^{10000}e^{\theta_j^T\cdot e_c}}$\theta_t$为target对应的参数，e_c为context的embedding vector，且e_c=E\cdot O_c 相应的loss function为： L(\hat y,y)=-\sum_{i=1}^{10000}y_ilog\ \hat y_i 由于 y是一个one-hot向量，所以上式实际上10000个项里面只有一项是非0的 然后，运用梯度下降算法，迭代优化，最终得到embedding matrix E 然而，这种算法计算量大，影响运算速度。主要因为softmax输出单元为10000个，\hat y计算公式中包含了大量的求和运算 解决的办法之一是使用hierarchical softmax classifier，即树形分类器： [] 这种树形分类器是一种二分类。它在每个数节点上对目标单词进行区间判断，最终定位到目标单词。最多需要\log_2 N步就能找到目标单词，N为单词总数 实际应用中，对树形分类器做了一些改进。改进后的树形分类器是非对称的，通常选择把比较常用的单词放在树的顶层，而把不常用的单词放在树的底层。这样更能提高搜索速度 关于context的采样：如果使用均匀采样，那么一些常用的介词、冠词，例如the, of, a, and, to等出现的概率更大一些。但是这些单词的embedding vectors通常不是最关心的，更关心的例如orange, apple， juice等这些名词。所以实际应用中一般不选择随机均匀采样的方式来选择context，而是使用其它算法来处理这类问题 2.7 负采样（Negative Sampling）算法要做的是构造一个新的监督学习问题：给定一对单词，比如orange和juice，去预测这是否是一对上下文词-目标词（context-target） 在这个例子中orange和juice就是个正样本，用1作为标记，orange和king就是个负样本，标为0。要做的就是采样得到一个上下文词和一个目标词，中间列叫做词（word）。然后： 生成一个正样本，先抽取一个context，在一定词距内比如说正负10个词距内选一个target，生成这个表的第一行，即orange– juice -1的过程 生成一个负样本，用相同的context，再在字典中随机选一个词，如king、book、the、of，标记为0。因为如果随机选一个词，它很可能跟orange没关联 如果从字典中随机选到的词，正好出现在了词距内，比如说在上下文词orange正负10个词之内，也没关系，如of被标记为0，即使of的确出现在orange词的前面 接下来将构造一个监督学习问题，学习算法输入x，即输入这对词（编号7），要去预测目标的标签（编号8），即预测输出y 如何选取K： 小数据集的话，K从5到20，数据集越小K就越大 如果数据集很大，K就选的小一点。对于更大的数据集K就从2到5 学习从x映射到y的监督学习模型： 编号2是新的输入x，编号3是要预测的值y。记号c表示context，记号t表示可能的target，y表示0和1，即是否是一对context-target。要做的是定义一个逻辑回归模型，给定输入的c，t对的条件下，y=1的概率，即： P\left( y = 1 \middle| c,t \right) = \sigma(\theta_{t}^{T}e_{c})如果输入词是orange，即词6257，要做的就是输入one-hot向量，和E相乘获得嵌入向量e_{6257}，最后得到10,000个可能的逻辑回归分类问题，其中一个（编号4）将会是用来判断目标词是否是juice的分类器，其他的词比如下面的某个分类器（编号5）是用来预测king是否是目标词 negative sampling中某个固定的正样本对应k个负样本，即模型总共包含了k+1个binary classification。对比之前10000个输出单元的softmax分类，negative sampling转化为k+1个二分类问题，每次迭代并不是训练10000个，而仅训练其中k+1个，计算量要小很多，大大提高了模型运算速度 这种方法就叫做负采样（Negative Sampling）: 选择一个正样本，随机采样k个负样本 选取了context orange之后，如何选取负样本： 通过单词出现的频率进行采样：导致一些类似a、the、of等词的频率较高 均匀随机地抽取负样本：没有很好的代表性 （推荐）： P(w_{i}) = \frac{f( w_{i})^{\frac{3}{4}}}{\sum_{j = 1}^{10,000}{f( w_{j} )^{\frac{3}{4}}}}这种方法处于上面两种极端采样方法之间，即不用频率分布，也不用均匀分布，而采用的是对词频的\frac{3}{4}除以词频\frac{3}{4}整体的和进行采样的。其中，f(w_j)是语料库中观察到的某个词的词频 2.8 GloVe 词向量（GloVe Word Vectors） GloVe代表用词表示的全局变量（global vectors for word representation） 假定X_{ij}是单词i在单词j上下文中出现的次数，i和j 与t和c的功能一样，可以认为X_{ij}等同于X_{tc}。根据context和target的定义，会得出X_{ij}等于X_{ji} 如果将context和target的范围定义为出现于左右各10词以内的话，就有对称关系X_{ij}=X_{ji} 如果对context的选择是context总是目target前一个单词，那么X_{ij}\neq X_{ji} 对于GloVe算法，可以定义context和target为任意两个位置相近的单词，假设是左右各10词的距离，那么X_{ij}就是一个能够获取单词i和单词j彼此接近的频率计数器 GloVe模型做的就是进行优化，将差距进行最小化处理： \text{mini}\text{mize}\sum_{i = 1}^{10,000}{\sum_{j = 1}^{10,000}}{f\left( X_{ij} \right)\left( \theta_{i}^{T}e_{j} + b_{i} + b_{j}^{'} - \log X_{ij} \right)^{2}}$\theta_{i}^{T}e_{j}$即\theta_{t}^{T}e_{c}。对于\theta_{t}^{T}e_{c}，这两个单词同时出现的频率是多少受X_{ij}影响，若两个词的embedding vector越相近，同时出现的次数越多，则对应的loss越小 当X_{ij}=0时，权重因子f(X_{ij})=0。这种做法直接忽略了无任何相关性的context和target，只考虑X_{ij}>0的情况 出现频率较大的单词相应的权重因子f(X_{ij})较大，出现频率较小的单词相应的权重因子f(X_{ij})较小一些 因为\theta和e是完全对称的，所以\theta_{i}和e_{j}是对称的。因此训练算法的方法是一致地初始化\theta和e，然后使用梯度下降来最小化输出，当每个词都处理完之后取平均值： e_{w}^{(final)}= \frac{e_{w} +\theta_{w}}{2}GloVe算法不能保证嵌入向量的独立组成部分： 通过上面的很多算法得到的词嵌入向量，无法保证词嵌入向量的每个独立分量是能够理解的。但能够确定是每个分量和所想的一些特征是有关联的，可能是一些我们能够理解的特征的组合而构成的一个组合分量 使用上面的GloVe模型，从线性代数的角度解释如下： \Theta_{i}^{T}e_{j} = \Theta_{i}^{T}A^{T}A^{-T}e_{j}=(A\Theta_{i})^{T}(A^{-T}e_{j})加入的A项，可能构成任意的分量组合 2.9 情感分类（Sentiment Classification）情感分类任务就是看一段文本，然后分辨这个人是否喜欢他们在讨论的这个东西，最大的挑战就是可能标记的训练集没有那么多，但是有了词嵌入，即使只有中等大小标记的训练集，也能构建一个不错的情感分类器 输入x是一段文本，输出y是要预测的相应情感。比如一个餐馆评价的星级 情感分类一个最大的挑战就是可能标记的训练集没有那么多。对于情感分类任务来说，训练集大小从10,000到100,000个单词都很常见，甚至有时会小于10,000个单词，采用了词嵌入能够带来更好的效果，尤其是只有很小的训练集 给定四个词（”dessert is excellent“），通常用10,000个词的词汇表，找到相应的one-hot向量，再乘以嵌入矩阵E，E可以从一个很大的文本集里学习到，比如它可以从一亿个词或者一百亿个词里学习嵌入，然后用来提取单词the的嵌入向量e_{8928}，对dessert、is、excellent做同样的步骤 然后取这些向量（编号2），如300维度的向量，通过平均值计算单元（编号3），求和并平均，再送进softmax分类器，然后输出\hat y。这个softmax能够输出5个可能结果的概率值，从一星到五星 这个算法适用于任何长短的评论，因为即使评论是100个词长，也可以对这一百个词的特征向量求和取平均，得到一个300维的特征向量，然后送进softmax分类器 但问题是没考虑词序，如负面的评价，”Completely lacking in good taste, good service, and good ambiance.“，good这个词出现了很多次，但算法忽略词序，仅仅把所有单词的词嵌入加起来或者平均下来，最后的特征向量会有很多good的表示，分类器很可能认为这是一个好的评论，尽管事实上这是一个差评，只有一星的评价 为了解决这一问题，情感分类的另一种模型是RNN： 首先取这条评论，”Completely lacking in good taste, good service, and good ambiance.“，找出每一个one-hot向量，乘以词嵌入矩阵E，得到词嵌入表达e，然后把它们送进RNN RNN的工作就是在最后一步（编号1）计算一个特征表示，用来预测\hat y。这样的算法考虑词的顺序效果更好，能意识到”things are lacking in good taste“是个负面的评价，“not good”也是一个负面的评价。而不像原来的算法一样，只是把所有的加在一起得到一个大的向量，根本意识不到“not good”和 “good”不是一个意思，”lacking in good taste“也是如此，等等 如果训练一个这样的算法，最后会得到一个很合适的情感分类的算法。由于词嵌入是在一个更大的数据集里训练的，这样会更好的泛化一些没有见过的新的单词。比如”Completely absent of good taste, good service, and good ambiance.“，即使absent这个词不在标记的训练集里 如果是在一亿或者一百亿单词集里训练词嵌入，它仍然可以正确判断，并且泛化的很好，甚至这些词是在训练集中用于训练词嵌入，但不在专门做情感分类问题标记的训练集 2.10 词嵌入除偏（Debiasing Word Embeddings）根据训练模型所使用的文本，词嵌入能够反映出性别、种族、年龄、性取向等其他方面的偏见： 假设已经完成一个词嵌入的学习，各个词的位置如图： 首先做的事就是辨别出想要减少或想要消除的特定偏见的趋势 怎样辨别出偏见相似的趋势： 一、对于性别歧视，对所有性别对立的单词求差值，再平均： bias\ direction=\frac1N ((e_{he}-e_{she})+(e_{male}-e_{female})+\cdots)二、中和步骤，对于定义不确切的词可以将其处理一下，避免偏见。像doctor和babysitter使之在性别方面中立。将它们在这个轴（编号1）上进行处理，减少或是消除他们的性别歧视趋势的成分，即减少在水平方向上的距离（编号2方框内所示的投影） 三、均衡步，babysitter和grandmother之间的距离或者说是相似度实际上是小于babysitter和grandfather之间的（编号1），因此这可能会加重不良状态，或者非预期的偏见，也就是说grandmothers相比于grandfathers最终更有可能输出babysitting。所以在最后的均衡步中，想要确保的是像grandmother和grandfather这样的词都能够有一致的相似度，或者说是相等的距离，做法是将grandmother和grandfather移至与中间轴线等距的一对点上（编号2），现在性别歧视的影响也就是这两个词与babysitter的距离就完全相同了（编号3） 最后，掌握哪些单词需要中立化非常重要。一般来说，大部分英文单词，例如职业、身份等都需要中立化，消除embedding vector中性别这一维度的影响]]></content>
      <categories>
        <category>深度学习 </category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第四周 特殊应用：人脸识别和神经风格转换（Special applications: Face recognition & Neural style transfer)(Course 4)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E5%9B%9B%E5%91%A8-%E7%89%B9%E6%AE%8A%E5%BA%94%E7%94%A8%EF%BC%9A%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E5%92%8C%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E6%8D%A2%EF%BC%88Special-applications-Face-recognition-Neural-style-transfer%EF%BC%89-Course-4%2F</url>
    <content type="text"><![CDATA[4.1 什么是人脸识别？（What is face recognition?） 人脸验证（face verification）问题：如果有一张输入图片以及某人的ID或者是名字，系统要做的是验证输入图片是否是这个人，也被称作1对1问题，只需要弄明白这个人是否和他声称的身份相符 人脸识别（face recognition）问题：（1对多问题（1:K））输入一张人脸图片，验证输出是否为K个模板中的某一个，即一对多问题 一般人脸识别比人脸验证更难。因为假设人脸验证系统的错误率是1%，那么在人脸识别中，输出分别与K个模板都进行比较，则相应的错误率就会增加，约K%。模板个数越多，错误率越大一些 什么是人脸识别？（What is face recognition?）人脸验证（face verification）问题：如果有一张输入图片以及某人的ID或者是名字，系统要做的是验证输入图片是否是这个人，也被称作1对1问题，只需要弄明白这个人是否和他声称的身份相符人脸识别（face recognition）问题：（1对多问题（1:K））输入一张人脸图片，验证输出是否为K个模板中的某一个，即一对多问题一般人脸识别比人脸验证更难。因为假设人脸验证系统的错误率是1%，那么在人脸识别中，输出分别与K个模板都进行比较，则相应的错误率就会增加，约K%。模板个数越多，错误率越大一些 4.2 One-Shot学习（One-shot learning） 要让人脸识别能够做到一次学习，要做的是学习Similarity函数 让神经网络学习用d表示的函数： d(img1,img2) = degree\ of\ difference\ between\ images以两张图片作为输入，然后输出这两张图片的差异值 如果这两张图片的差异值小于某个阈值\tau，就能预测这两张图片是同一个人 如果差异值大于τ，就能预测这是不同的两个人 对于人脸识别问题，只需计算测试图片与数据库中K个目标的相似函数，取其中d(img1,img2)最小的目标为匹配对象。若所有的d(img1,img2)都很大，则表示数据库没有这个人 如果之后有新人加入了团队（编号5），只需将他的照片加入数据库，系统依然能照常工作 4.3 Siamese 网络（Siamese network）函数d的作用是输入两张人脸，然后输出相似度。实现这个功能的一个方式是用Siamese网络 向量（编号1）是由网络深层的全连接层计算出来的，叫做f(x^{(1)})。可以把f(x^{(1)})看作是输入图像x^{(1)}的编码，即取输入图像（编号2），然后表示成128维的向量 如果要比较两个图片，要做的是把第二张图片喂给有同样参数的同样的神经网络，得到一个不同的128维的向量（编号3），第二张图片的编码叫做f(x^{(2)}) 然后定义d，将x^{(1)}和x^{(2)}的距离定义为两幅图片的编码之差的范数： d( x^{( 1)},x^{( 2)}) =|| f( x^{( 1)}) - f( x^{( 2)})||_{2}^{2}对于两个不同的输入，运行相同的卷积神经网络，然后比较它们，就叫做Siamese网络架构 训练Siamese神经网络：不同图片的CNN网络所有结构和参数都是一样的。所以要做的是训练一个网络，利用梯度下降算法不断调整网络参数，使得属于同一人的图片之间d(x^{(1)},x^{(2)}) 很小，而不同人的图片之间d(x^{(1)},x^{(2)})很大 即神经网络的参数定义了一个编码函数f(x^{(i)})，如果给定输入图像x^{(i)}，这个网络会输出x^{(i)}的128维的编码。然后要做的就是学习参数 使得如果两个图片x^{( i)}和x^{( j)}是同一个人，那么得到的两个编码的距离就小 如果x^{(i)}和x^{(j)}是不同的人，那么编码距离就大 如果改变这个网络所有层的参数，会得到不同的编码结果，要做的是用反向传播来改变这些所有的参数，以确保满足这些条件 4.4 Triplet 损失（Triplet 损失）要想通过学习神经网络的参数来得到优质的人脸图片编码，方法之一就是定义三元组损失函数然后应用梯度下降 三元组损失每个样本包含三张图片：靶目标（Anchor）、正例（Positive）、反例（Negative），简写成A、P、N 网络的参数或者编码应满足： 让|| f(A) - f(P) ||^{2}很小，即： || f(A) - f(P)||^{2} \leq ||f(A) - f(N)||^{2} ||f(A)-f(P)||^2-||f(A)-F(N)||^2\leq 0$|| f(A) - f(P) ||^{2}$是d(A,P)，|| f(A) - f(N) ||^{2}是d(A,N) 如果所有的图片都是零向量，即f(A)=0,f(P)=0,f(N)=0那么上述不等式也满足。但是对进行人脸识别没有任何作用，所以添加一个超参数\alpha，且\alpha>0，对上述不等式做出如下修改： ||f(A)-f(P)||^2-||f(A)-F(N)||^2\leq -\alpha ||f(A)-f(P)||^2-||f(A)-F(N)||^2+\alpha \leq 0 间隔参数\alpha也被称为边界margin，类似于支持向量机中的margin，拉大了Anchor和Positive图片对和Anchor与Negative图片对之间的差距。若d(A,P)=0.5，\alpha=0.2，则d(A,N)\geq0.7 损失函数的定义基于三元图片组，即取这个和0的最大值： L( A,P,N) = max(|| f( A) - f( P)||^{2} -|| f( A) - f( N)||^{2} + \alpha,0)$max$函数的作用是只要|| f( A) - f( P)||^{2} -|| f( A) - f( N)||^{2} + \alpha\leq0，损失函数就是0 如果|| f( A) - f( P)||^{2} -|| f( A) - f( N)||^{2} + \alpha\leq0，最终会得到|| f(A) - f( P)||^{2} -|| f( A) - f( N)||^{2} +\alpha，即正的损失值。通过最小化这个损失函数达到的效果就是使这部分|| f( A) - f( P)||^{2} -||f( A) - f( N)||^{2} +\alpha成为0，或者小于等于0 整个网络的代价函数是训练集中单个三元组损失的总和 如何选择三元组来形成训练集：如果从训练集中随机地选择A​、P​和N​，遵守A​和P​是同一个人，而A​和N​是不同的人这一原则。那么约束条件（d(A,P) + \alpha \leq d(A,N)​）很容易达到，因为随机选择的图片，A​和N​比A​和P​差别很大的概率很大，而且差距远大于\alpha​，这样网络并不能从中学到什么 所以为了构建一个数据集，要做的就是尽可能选择难训练的三元组A、P和N： 想要所有的三元组都满足条件（d(A,P) + a \leq d(A,N)），A、P和N的选择应使得d(A,P)很接近d(A,N)，即d(A,P) \approx d(A,N)，这样学习算法会竭尽全力使右边式子变大（d(A,N)），或者使左边式子（d(A,P)）变小，这样左右两边至少有一个\alpha的间隔。并且选择这样的三元组还可以增加学习算法的计算效率 总结： 训练三元组损失需要把训练集做成很多三元组，这就是一个三元组（编号1），有一个Anchor图片和Positive图片，这两个（Anchor和Positive）是同一个人，还有一张另一个人的Negative图片。这是另一组（编号2），其中Anchor和Positive图片是同一个人，但是Anchor和Negative不是同一个人，等等。 定义了这些包括A、P和N图片的数据集之后，还需要用梯度下降最小化代价函数J，这样做的效果就是反向传播到网络中的所有参数来学习到一种编码，使得如果两个图片是同一个人，那么它们的d就会很小，如果两个图片不是同一个人，它们的d 就会很大 4.5 面部验证与二分类（Face verification and binary classification）另一个训练神经网络的方法是选取一对神经网络，选取Siamese网络，使其同时计算这些嵌入，比如说128维的嵌入（编号1），或者更高维，然后将其输入到逻辑回归单元进行预测，如果是相同的人，那么输出是1，若是不同的人，输出是0。这就把人脸识别问题转换为一个二分类问题，训练这种系统时可以替换Triplet loss的方法 最后的逻辑回归单元怎么处理： 比如说sigmoid函数应用到某些特征上，输出\hat y会变成： \hat y = \sigma(\sum_{k = 1}^{128}{w_{i}\| f( x^{( i)})_{k} - f( x^{( j)})_{k}\| + b})把这128个元素当作特征，然后把他们放入逻辑回归中，最后的逻辑回归可以增加参数w_{i}和b，就像普通的逻辑回归一样。然后在这128个单元上训练合适的权重，用来预测两张图片是否是一个人 $\hat y$的另外一种表达式为： \hat y=\sigma(\sum_{k=1}^Kw_k\frac{(f(x^{(i)})_k-f(x^{(j)})_k)^2}{f(x^{(i)})_k+f(x^{(j)})_k}+b)这个公式也被叫做\chi^{2}公式，也被称为\chi平方相似度 上面神经网络拥有的参数和下面神经网络的相同（编号3和4所示的网络），两组参数是绑定的，这样的系统效果很好 如果这是一张新图片（编号1），当员工走进门时，希望门可以自动为他们打开，这个（编号2）是在数据库中的图片，不需要每次都计算这些特征（编号6），可以提前计算好，当一个新员工走近时，使用上方的卷积网络来计算这些编码（编号5），和预先计算好的编码进行比较，然后输出预测值\hat y 总结：把人脸验证当作一个监督学习，创建一个只有成对图片的训练集，不是三个一组，而是成对的图片，目标标签是1表示一对图片是一个人，目标标签是0表示图片中是不同的人。利用不同的成对图片，使用反向传播算法去训练Siamese神经网络 4.6 什么是深度卷积网络？（What are deep ConvNets learning?）假如训练了一个Alexnet轻量级网络，不同层之间隐藏单元的计算结果如下： 从第一层的隐藏单元开始，将训练集经过神经网络，然后弄明白哪一张图片最大限度地激活特定的单元。在第一层的隐藏单元，只能看到小部分卷积神经，只有一小块图片块是有意义的，因为这就是特定单元所能看到的全部 然后选一个另一个第一层的隐藏单元，重复刚才的步骤： 对其他隐藏单元也进行处理，会发现其他隐藏单元趋向于激活类似于这样的图片： 以此类推，这是9个不同的代表性神经元，每一个不同的图片块都最大化地激活了。可以理解为第一层的隐藏单元通常会找一些简单的特征，比如说边缘或者颜色阴影 在深层部分，一个隐藏单元会看到一张图片更大的部分，在极端的情况下，可以假设每一个像素都会影响到神经网络更深层的输出，靠后的隐藏单元可以看到更大的图片块 第一层，第一个被高度激活的单元： 第二层检测的特征变得更加复杂： 第三层明显检测到更复杂的模式 第四层，检测到的模式和特征更加复杂： 第五层检测到更加复杂的事物： 4.7 代价函数（Cost function）为了实现神经风格迁移，需要定义一个关于G的代价函数J用来评判某个生成图像的好坏，使用梯度下降法去最小化J(G)，以便于生成图像 代价函数定义为两个部分： J_{\text{content}}(C,G)，被称作内容代价，是一个关于内容图片和生成图片的函数，用来度量生成图片G的内容与内容图片C的内容有多相似 然后把结果加上一个风格代价函数J_{\text{style}}(S,G)，用来度量图片G的风格和图片S的风格的相似度 J( G) = \alpha J_{\text{content}}( C,G) + \beta J_{\text{style}}(S,G)最后用两个超参数\alpha和\beta来来确定内容代价和风格代价 对于代价函数J(G)，为了生成一个新图像，要做的是随机初始化生成图像G，可能是100×100×3、500×500×3，或任何想要的尺寸 然后使用之前定义的代价函数J(G)，用梯度下降的方法将其最小化，更新： G:= G - \frac{\partial}{\partial G}J(G)即更新图像G的像素值，也就是100×100×3，比如RGB通道的图片 比如从内容图片（编号1）和风格（编号2）图片开始，当随机初始化G，生成图像就是随机选取像素的白噪声图（编号3）。接下来运行梯度下降算法，最小化代价函数J(G)，逐步处理像素，慢慢得到一个生成图片（编号4、5、6），越来越像用风格图片的风格画出来的内容图片 4.8 内容代价函数（Content cost function）$J(G)$的第一部分J_{content}(C,G)，它表示内容图片C与生成图片G之间的相似度 使用的CNN网络是之前训练好的模型，例如Alex-Net。C，S，G共用相同模型和参数 CNN的每个隐藏层分别提取原始图片的不同深度特征，由简单到复杂。如果l太小，则G与C在像素上会非常接近，没有迁移效果；如果l太深，则G上某个区域将直接会出现C中的物体。所以在实际中，层l在网络中既不会选的太浅也不会选的太深 衡量内容图片和生成图片在内容上的相似度： 令a^{[l][C]}和a^{[l][G]}代表图片C和G的l层的激活函数值。如果这两个激活值相似，意味着两个图片的内容相似 定义： J_{content}(C,G) = \frac{1}{4 \times n_H \times n_W \times n_C}\sum _{ \text{all entries}} (a^{[l][C]} - a^{[l][C]})^2为两个激活值不同或者相似的程度 后面如果对J(G)做梯度下降来找G的值时，整个代价函数会激励这个算法来找到图像G，使得隐含层的激活值和内容图像的相似 4.9 风格代价函数（Style cost function）利用CNN网络模型，图片的风格可以定义成第l层隐藏层不同通道间激活函数的乘积（相关性） 选取第l层隐藏层，各通道使用不同颜色标注。因为每个通道提取图片的特征不同，比如1通道（红色）提取的是图片的垂直纹理特征，2通道（黄色）提取的是图片的橙色背景特征。那么这两个通道的相关性越大，表示原始图片及既包含了垂直纹理也包含了该橙色背景；相关性越小，表示原始图片并没有同时包含这两个特征。即计算不同通道的相关性，反映了原始图片特征间的相互关系，从某种程度上刻画了图片的“风格” 接下来定义图片的风格矩阵（style matrix）为： G_{kk^{'}}^{[l]} = \sum_{i = 1}^{n_{H}^{[l]}}{\sum_{j = 1}^{n_{W}^{[l]}}{a_{i, j,k}^{[l]}a_{i, j, k^{'}}^{[l]}}}a_{i, j, k}^{[l]}$$为隐藏层$$l$$中$$(i,j,k)$$位置的激活项，$$i$$，$$j$$，$$k$$分别代表该位置的高度、宽度以及对应的通道数，k，$$k^{'}$$分别表示不同通道。风格矩阵$$G_{kk^{'}}^{[l]}$$计算第$$l$$层隐藏层不同通道对应的所有激活函数输出和，$$l$$层风格图像的矩阵$$G^{[l]}$$是一个$$n_{c} \times n_{c}$$的矩阵： ![upload successful](http://pnlb0i3oh.bkt.clouddn.com/pasted-155.png) 若两个通道之间相似性高，则对应的$$G_{kk^{'}}^{[l]}$$较大；若两个通道之间相似性低，则对应的$$G_{kk^{'}}^{[l]}$$较小 风格矩阵$$G_{kk'}^{[l](S)}$$表征了风格图片$$S$$第$$l$$层隐藏层的“风格”。生成图片$$G$$也有$$G_{kk'}^{[l](G)}$$，$$G_{kk'}^{[l](S)}$$与$$G_{kk'}^{[l](G)}$$越相近，则表示$$G$$的风格越接近$$S$$。即$$J^{[l]}_{style}(S,G)$$定义为：J_{style}^{[l]}(S,G) = \frac{1}{4 \times {n_C}^2 \times (n_H \times n_W)^2} \sum _{i=1}^{n_C}\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2 然后使用梯度下降算法，不断迭代修正$$G$$的像素值，使$$J^{[l]}_{style}(S,G)$$不断减小 为了提取更多的“风格”，可以使用多层隐藏层，然后相加，表达式为：J_{style}(S,G)=\sum_l\lambda^{[l]}\cdot J^{[l]}_{style}(S,G) $$\lambda^{[l]}$$表示累加过程中各层$$J^{[l]}_{style}(S,G)$$的权重系数，为超参数 最终的cost function为：J(G)=\alpha \cdot J_{content}(C,G)+\beta \cdot J_{style}(S,G) $$ 之后用梯度下降法，或者更复杂的优化算法来找到一个合适的图像G，并计算J(G)的最小值，这样将能够得到非常好看的结果 4.10 一维到三维推广（1D and 3D generalizations of models）1D卷积将2D卷积推广到1D卷积： 二维数据的卷积是将同一个5×5特征检测器应用于图像中不同的位置（编号1所示），最后得到10×10的输出结果。1维过滤器可以在不同的位置中应用类似的方法（编号3，4，5所示） 当对这个1维信号使用卷积，将一个14维的数据与5维数据进行卷积，并产生一个10维输出： 如果有16个过滤器，最后会获得一个10×16的数据： 对于卷积网络的下一层，如果输入一个10×16数据，可以使用一个5维过滤器进行卷积，需要16个通道进行匹配，如果有32个过滤器，另一层的输出结果就是6×32： 3D卷积当进行CT扫描时，人体躯干的不同切片数据本质上是3维的 如果有一个3D对象是14×14×14： 过滤器也是3D的，如果使用5×5×5过滤器进行卷积，将会得到一个10×10×10的结果输出，如果使用16个过滤器，输出将是10×10×10×16 如果下一层卷积使用5×5×5×16维度的过滤器再次卷积，如果有32个过滤器，最终将得到一个6×6×6×32的输出]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第三周 目标检测（Object detection)(Course 4)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%B8%89%E5%91%A8-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88Object-detection-Course-4%2F</url>
    <content type="text"><![CDATA[3.1 目标定位（Object localization）定位分类问题：不仅要用算法判断图片中是不是一辆汽车，还要在图片中标记出它的位置，用边框或红色方框把汽车圈起来，“定位”的意思是判断汽车在图片中的具体位置 定位分类问题通常只有一个较大的对象位于图片中间位置，对它进行识别和定位。对象检测问题中图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象 构建汽车自动驾驶系统，对象可能包括以下几类：行人、汽车、摩托车和背景 定位图片中汽车的位置：让神经网络输出一个边界框，标记为b_{x},b_{y},b_{h}和b_{w}，是被检测对象的边界框的参数化表示 红色方框的中心点表示为(b_{x},b_{y})，边界框的高度为b_{h}，宽度为b_{w}。训练集不仅包含神经网络要预测的对象分类标签，还要包含表示边界框的这四个数字，接着采用监督学习算法，输出一个分类标签，还有四个参数值，从而给出检测对象的边框位置 如何为监督学习任务定义目标标签 y： 目标标签y​的定义： y= \begin{bmatrix} p_{c} \\ b_{x} \\ b_{y}\\ b_{h}\\ b_{w} \\ c_{1}\\ c_{2}\\ c_{3} \end{bmatrix}$p_{c}​$表示是否含有对象，如果对象属于前三类（行人、汽车、摩托车），则p_{c}= 1​，如果是背景，则p_{c} =0​。p_{c}​表示被检测对象属于某一分类的概率，背景分类除外 如果检测到对象，就输出被检测对象的边界框参数b_{x}​、b_{y}​、b_{h}​和b_{w}​。p_{c}=1​，同时输出c_{1}​、c_{2}​和c_{3}​，表示该对象属于行人，汽车还是摩托车 如果图片中没有检测对象: $p_{c} =0$，y的其它参数全部写成问号，表示“毫无意义”的参数 神经网络的损失函数，如果采用平方误差策略： L\left(\hat{y},y \right) = \left( \hat{y_1} - y_{1} \right)^{2} + \left(\hat{y_2} - y_{2}\right)^{2} + \ldots+\left( \hat{y_8} - y_{8}\right)^{2}损失值等于每个元素相应差值的平方和 如果图片中存在定位对象，y_{1} =p_{c}=1，损失值是不同元素的平方和 $y_{1}= p_{c} = 0$，损失值是\left(\hat{y_1} - y_{1}\right)^{2}，只需要关注神经网络输出p_{c}的准确度 这里用平方误差简化了描述过程。实际可以每个分量使用不同的损失函数, 通常做法是对p_{c}应用逻辑回归函数，边界框坐标应用平方差, 分类标签使用softmax损失函数 3.2 特征点检测（Landmark detection）仅对目标的关键特征点坐标进行定位，这些关键点被称为landmarks 选定特征点个数，并生成包含特征点的标签训练集，利用神经网络输出脸部关键特征点的位置 具体做法:准备一个卷积网络和一些特征集，将人脸图片输入卷积网络，输出1表示有人脸，0表示没有人脸，然后输出（l_{1x}，l_{1y}）……直到（l_{64x}，l_{64y}），l代表一个特征，即该网络模型共检测人脸上64处特征点，加上是否为face的标志位，输出label共有64x2+1=129个值，即有129个输出单元，由此实现对图片的人脸检测和定位 检测人体姿势动作： 特征点的特性在所有图片中必须保持一致 3.3 目标检测（Object detection）通过卷积网络进行对象检测，采用的是基于滑动窗口的目标检测算法 构建汽车检测算法步骤： 首先创建一个标签训练集，x和y表示适当剪切的汽车图片样本，一开始可以使用适当剪切的图片，就是整张图片x几乎都被汽车占据，使汽车居于中间位置，并基本占据整张图片 开始训练卷积网络，输入这些适当剪切过的图片（编号6），卷积网络输出y，0或1表示图片中有汽车或没有汽车 训练完这个卷积网络，用它来实现滑动窗口目标检测，具体步骤如下： 1.首先选定一个特定大小的窗口，将红色小方块输入卷积神经网络，卷积网络开始判断红色方框内有没有汽车 [ 2.滑动窗口目标检测算法继续处理第二个图像，红色方框稍向右滑动之后的区域，并输入给卷积网络，再次运行卷积网络，然后处理第三个图像，依次重复操作，直到这个窗口滑过图像的每一个角落 思路是以固定步幅移动窗口，遍历图像的每个区域，把这些剪切后的小图像输入卷积网络，对每个位置按0或1进行分类 3.重复上述操作，选择一个更大的窗口，截取更大的区域，并输入给卷积神经网络处理，输出0或1 4.再以某个固定步幅滑动窗口，重复以上操作，遍历整个图像，输出结果 5.第三次重复操作，选用更大的窗口 这样不论汽车在图片的什么位置，总有一个窗口可以检测到 这种算法叫作滑动窗口目标检测：以某个步幅滑动这些方框窗口遍历整张图片，对这些方形区域进行分类，判断里面有没有汽车 滑动窗口目标检测算法缺点：计算成本 如果选用的步幅很大，会减少输入卷积网络的窗口个数，粗糙间隔尺寸可能会影响性能 如果采用小粒度或小步幅，传递给卷积网络的小窗口会特别多，这意味着超高的计算成本 3.4 卷积的滑动窗口实现（Convolutional implementation of sliding windows）把神经网络的全连接层转化成卷积层 前几层和之前的一样，下一层全连接层用5×5×16的过滤器来实现，数量是400个（编号1），输入图像大小为5×5×16，输出维度是1×1×400，这400个节点中每个节点都是上一层5×5×16激活值经过某个任意线性函数的输出结果 再添加另外一个卷积层（编号2），用1×1卷积，假设有400个1×1的过滤器，在这400个过滤器的作用下，下一层的维度是1×1×400，是上个网络中的这一全连接层经由1×1过滤器的处理，得到一个softmax激活值，通过卷积网络，最终得到1×1×4的输出层，而不是这4个数字（编号3） 以上就是用卷积层代替全连接层的过程，结果这几个单元集变成了1×1×400和1×1×4的维度 通过卷积实现滑动窗口对象检测算法假设向滑动窗口卷积网络输入14×14×3的图片，神经网络最后的输出层，即softmax单元的输出是1×1×4 假设测试集图片是16×16×3，给输入图片加上黄色条块，在最初的滑动窗口算法中，把蓝色区域输入卷积网络（红色笔标记）生成0或1分类。接着滑动窗口，步幅为2个像素，向右滑动2个像素，将绿框区域输入给卷积网络，运行整个卷积网络，得到另外一个标签0或1。继续将这个橘色区域输入给卷积网络，卷积后得到另一个标签，最后对右下方的紫色区域进行最后一次卷积操作。在这个16×16×3的小图像上滑动窗口，卷积网络运行了4次，于是输出了了4个标签 这4次卷积操作中很多计算都是重复的。执行滑动窗口的卷积时使得卷积网络在这4次前向传播过程中共享很多计算，尤其是在编号1，卷积网络运行同样的参数，使用相同的5×5×16过滤器进行卷积操作，得到12×12×16的输出层。然后执行同样的最大池化（编号2），输出结果6×6×16。照旧应用400个5×5的过滤器（编号3），得到一个2×2×400的输出层，现在输出层为2×2×400，应用1×1过滤器（编号4）得到另一个2×2×400的输出层。再做一次全连接的操作（编号5），最终得到2×2×4的输出层，在输出层4个子方块中，蓝色的是图像左上部分14×14的输出（红色箭头标识），右上角方块是图像右上部分（绿色箭头标识）的对应输出，左下角方块是输入层左下角（橘色箭头标识），右下角是卷积网络处理输入层右下角14×14区域(紫色箭头标识)的结果 具体的计算步骤：以绿色方块为例，假设剪切出这块区域（编号1），传递给卷积网络，第一层的激活值就是这块区域（编号2），最大池化后的下一层的激活值是这块区域（编号3），这块区域对应着后面几层输出的右上角方块（编号4，5，6） 该卷积操作的原理是不需要把输入图像分割成四个子集，分别执行前向传播，而是把它们作为一张图片输入给卷积网络进行计算，其中的公共区域可以共享很多计算 假如对一个28×28×3的图片应用滑动窗口操作，以14×14区域滑动窗口，以大小为2的步幅不断地向右移动窗口，直到第8个单元格，得到输出层的第一行。然后向图片下方移动，最终输出8×8×4的结果 总结滑动窗口的实现过程： 在图片上剪切出一块区域，假设大小是14×14，把它输入到卷积网络。继续输入下一块区域，大小同样是14×14，重复操作，直到某个区域识别到汽车 但是不能依靠连续的卷积操作来识别图片中的汽车，可以对大小为28×28的整张图片进行卷积操作，一次得到所有预测值，如果足够幸运，神经网络便可以识别出汽车的位置 在卷积层上应用滑动窗口算法提高了整个算法的效率，缺点是边界框的位置可能不够准确 3.5 Bounding Box预测（Bounding box predictions）滑动窗口法的卷积实现算法效率很高，但不能输出最精准的边界框 输入图像是100×100的，用3×3网格，实际实现时会用更精细的网格（19×19）。使用图像分类和定位算法 编号1什么也没有，左上格子的标签向量y是\begin{bmatrix}0\ ?\ ?\ ?\ ?\ ?\ ?\ ? \end{bmatrix}。其他什么也没有的格子都一样 图中有两个对象，YOLO算法做的是取两个对象的中点，将对象分配给包含对象中点的格子。即使中心格子（编号5）同时有两辆车的一部分，分类标签y也为y= \begin{bmatrix}0\ ?\ ?\ ?\ ?\ ?\ ?\ ? \end{bmatrix}。编号4目标标签y= \begin{bmatrix} 1\ b_{x}\ b_{y}\ b_{h}\ b_{w}\ 0\ 1\ 0 \end{bmatrix}，编号6类似 3×3中9个格子都对应一个8维输出目标向量y，其中一些值可以是dont care-s（即？）所以总的目标输出尺寸就是3×3×8 如果要训练一个输入为100×100×3的神经网络，输入图像通过普通的卷积网络，卷积层，最大池化层等等，最后映射到一个3×3×8输出尺寸。然后用反向传播训练神经网络，将任意输入x映射到输出向量y 这个算法的优点在于神经网络可以输出精确的边界框，测试的时候有要做的是喂入输入图像x，然后跑正向传播，直到得到输出y。然后3×3位置对应的9个输出，只要每个格子中对象数目没有超过1个，这个算法应该是没问题的。但实践中会使用更精细的19×19网格，输出就是19×19×8，多个对象分配到同一个格子得概率就小得多 即使对象可以横跨多个格子，也只会被分配到9个格子其中之一，或者19×19网络的其中一个格子。在19×19网格中，两个对象的中点（图中蓝色点所示）处于同一个格子的概率就会更低。 优点： 显式地输出边界框坐标，可以具有任意宽高比，并且能输出更精确的坐标，不会受到滑动窗口分类器的步长大小限制 并没有在3×3网格上跑9次算法，而是单次卷积实现，但在处理这3×3计算中很多计算步骤是共享的，所以这个算法效率很高 因为是卷积实现，运行速度非常快，可以达到实时识别 如何编码这些边界框b_{x}、b_{y}、b_{h}和b_{w}： 在YOLO算法中，编号1约定左上点是(0,0)，右下点是(1,1)，橙色中点的位置b_{x}大概是0.4，b_{y}大概是0.3，b_{w}是0.9，b_{h}是0.5。b_{x}、b_{y}、b_{h}和b_{w}单位是相对于格子尺寸的比例，所以b_{x}和b_{y}必须在0和1之间，因为从定义上看，橙色点位于对象分配到格子的范围内，如果它不在0和1之间，即它在方块外，那么这个对象就应该分配到另一个格子上。这个值（b_{h}和b_{w}）可能会大于1，特别是如果有一辆汽车的边界框是这样的（编号3所示），那么边界框的宽度和高度有可能大于1 3.6 交并比（Intersection over union）并交比函数可以用来评价对象检测算法 交并比（loU）函数是计算两个边界框交集和并集之比。两个边界框的并集是两个边界框绿色阴影区域，而交集是这个橙色阴影区域，交并比就是交集的大小（橙色阴影面积）除以绿色阴影的并集面积 一般约定，在计算机检测任务中，如果loU≥0.5，就说检测正确，如果预测器和实际边界框完美重叠，loU就是1，因为交集就等于并集 3.7 非极大值抑制（Non-max suppression）对象检测中的一个问题是算法可能对同一个对象做出多次检测，非极大值抑制可以确保算法对每个对象只检测一次 实践中当运行对象分类和定位算法时，对于每个格子都运行一次，编号1、2、3可能会认为这辆车中点应该在格子内部 这个算法做的是： 1.首先看哪个检测结果相关的概率p_{c}（实际上是p_{c}乘以c_{1}、c_{2}或c_{3}）概率最大，右边车辆中是0.9，即最可靠的检测，用高亮标记，之后非极大值抑制逐一审视剩下的矩形，所有和这个最大的边框有很高交并比，高度重叠的其他边界框输出就会被抑制 2.逐一审视剩下的矩形，找出概率p_{c}最高的一个，在这种情况下是0.8，就认为检测出一辆车（左边车辆），然后非极大值抑制算法就会去掉其他loU值很高的矩形。现在每个矩形都会被高亮显示或者变暗，如果直接抛弃变暗的矩形，就剩下高亮显示的那些是最后得到的两个预测结果 非最大值意味着只输出概率最大的分类结果，但抑制很接近，不是最大的其他预测结果 算法的细节： 首先在19×19网格上执行算法，会得到19×19×8的输出尺寸。简化成只做汽车检测，会得到输出预测概率（p_{c}）和边界框参数（b_{x}、b_{y}、b_{h}和b_{w}） 1.将所有的预测值p_{c}小于或等于某个阈值，如p_{c}\le 0.6的边界框去掉 2.剩下的边界框就一直选择概率p_{c}最高的边界框，把它输出成预测结果，取一个边界框，让它高亮显示，就可以确定输出有一辆车的预测 3.去掉所有剩下的边界框 如果同时检测三个对象，比如说行人、汽车、摩托，输出向量就会有三个额外的分量。正确的做法是独立进行三次非极大值抑制，对每个输出类别都做一次 3.8 Anchor Boxes对象检测存在的一个问题是每个格子只能检测出一个对象，如果想让一个格子检测出多个对象，可以使用anchor box 行人的中点和汽车的中点都落入到同一个格子中 anchor box的思路是：预先定义两个不同形状的anchor box，把预测结果和这两个anchor box关联起来 定义类别标签： y= \begin{bmatrix} p_{c} & b_{x} & b_{y} &b_{h} & b_{w} & c_{1} & c_{2} & c_{3} & p_{c} & b_{x} & b_{y} & b_{h} & b_{w} &c_{1} & c_{2} & c_{3} \end{bmatrix}^{T}前面的p_{c},b_{x},b_{y},b_{h},b_{w},c_{1},c_{2},c_{3}（绿色方框标记的参数）是和anchor box 1关联的8个参数，后面的8个参数（橙色方框标记的元素）是和anchor box 2相关联 行人：p_{c}= 1,b_{x},b_{y},b_{h},b_{w},c_{1} = 1,c_{2} = 0,c_{3} = 0 车子的边界框更像anchor box 2，(p_{c}= 1,b_{x},b_{y},b_{h},b_{w},c_{1} = 0,c_{2} = 1,c_{3} = 0) 现在每个对象都分配到对象中点所在的格子中，以及分配到和对象形状交并比最高的anchor box中。然后观察哪个anchor box和实际边界框（编号1，红色框）的交并比更高 编号1对应同时有车和行人，编号3对应只有车： anchor box是为了处理两个对象出现在同一个格子的情况，实践中这种情况很少发生，特别用的是19×19网格 怎么选择anchor box： 一般手工指定anchor box形状，可以选择5到10个anchor box形状，覆盖到想要检测的对象的各种形状 更高级的是使用k-means算法，将两类对象形状聚类，选择最具有代表性的一组anchor box 3.9 YOLO 算法（Putting it together: YOLO algorithm）假设要在图片中检测行人、汽车，同时使用两种不同的Anchor box 训练集： 输入X：同样大小的完整图片 目标Y：使用3\times3网格划分，输出大小3\times3\times2\times8，或者3\times3\times16 对不同格子中的小图，定义目标输出向量Y 编号2目标向量y =\begin{bmatrix} 0 & ? & ? & ? & ? & ? & ? & ? & 1 & b_{x} & b_{y} & b_{h} &b_{w} & 0 & 1 & 0 \end{bmatrix}^{T}，假设训练集中对于车子有一个边界框（编号3），水平方向更长一点，红框和anchor box 2的交并比更高，车子和向量的下半部分相关 模型预测： 输入与训练集中相同大小的图片，然后训练一个卷积网络，遍历9个格子，得到每个格子中不同的输出结果：3\times3\times2\times8 运行非最大值抑制（NMS）： 假设使用了2个Anchor box，每一个网格都会得到预测输出的2个bounding boxes，其中一个P_{c}比较高 抛弃概率P_{c}值低的预测bounding boxes 对每个对象分别使用NMS算法得到最终的预测边界框 如果有三个对象检测类别，希望检测行人，汽车和摩托车：对于每个类别单独运行非极大值抑制，处理预测结果所属类别的边界框，用非极大值抑制来处理行人、车子、摩托车类别，运行三次来得到最终的预测结果 3.10 候选区域（选修）（Region proposals (Optional)）滑动窗法会对原始图片的每个区域都进行扫描，即使是一些空白的或明显没有目标的区域，这样会降低算法运行效率，耗费时间 R-CNN算法，即带区域的卷积网络，或者带区域的CNN。这个算法尝试选出一些区域，在少数窗口上运行卷积网络分类器 选出候选区域的方法是运行图像分割算法，找出各个尺度的色块，然后在色块上运行分类器，即首先得到候选区域，然后再分类 R-CNN算法很慢，基本的R-CNN算法是使用某种算法求出候选区域，然后对每个候选区域运行一下分类器，每个区域会输出一个标签，有没有车子、行人、摩托车？并输出一个边界框，就能在确实存在对象的区域得到一个精确的边界框 R-CNN算法不会直接信任输入的边界框，也会输出一个边界框b_{x}，b_{y}，b_{h}和b_{w}，这样得到的边界框比较精确，比单纯使用图像分割算法给出的色块边界要好 Fast R-CNN算法基本上是R-CNN算法，最初的算法是逐一对区域分类，快速R-CNN用的是滑动窗法的一个卷积实现，和3.4 卷积的滑动窗口实现的相似，显著提升了R-CNN的速度，问题是得到候选区域的聚类步骤仍然非常缓慢 更快的R-CNN算法（Faster R-CNN），使用的是卷积神经网络，而不是更传统的分割算法来获得候选区域色块，比Fast R-CNN算法快得多 不过大多数更快R-CNN的算法实现还是比YOLO算法慢很多]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周 深度卷积网络：实例探究（Deep convolutional models: case studies）(Course 4)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%BA%8C%E5%91%A8-%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%9A%E5%AE%9E%E4%BE%8B%E6%8E%A2%E7%A9%B6%EF%BC%88Deep-convolutional-models-case-studies%EF%BC%89-Course-4%2F</url>
    <content type="text"><![CDATA[2.1 经典网络（Classic networks）LeNet-5LeNet-5可以识别图中的手写数字，是针对灰度图片训练的，所以图片的大小只有32×32×1。该LeNet模型总共包含了大约6万个参数，典型的LeNet-5结构包含CONV layer，POOL layer和FC layer，顺序一般是CONV layer-&gt;POOL layer-&gt;CONV layer-&gt;POOL layer-&gt;FC layer-&gt;FC layer-&gt;OUTPUT layer，即\hat y： 随着网络越来越深，图像的高度和宽度在缩小，从最初的32×32缩小到28×28，再到14×14、10×10，最后只有5×5，通道数量一直在增加，从1增加到6个，再到16个 这个神经网络中还有一种模式就是一个或多个卷积层后面跟着一个池化层，然后又是若干个卷积层再接一个池化层，然后是全连接层，最后是输出 AlexNetAlexNet包含约6000万个参数。当用于训练图像和数据集时，AlexNet能够处理非常相似的基本构造模块，这些模块往往包含着大量的隐藏单元或数据，AlexNet比LeNet表现更为出色的另一个原因是它使用了ReLu激活函数 VGG-16VGG，也叫作VGG-16网络。VGG-16网络没有那么多超参数，是一种只需要专注于构建卷积层的简单网络。首先用3×3，步幅为1的过滤器构建卷积层，padding参数为same卷积中的参数。然后用一个2×2，步幅为2的过滤器构建最大池化层。VGG网络的一大优点是简化了神经网络结构 假设要识别这个图像，在最开始的两层用64个3×3的过滤器对输入图像进行卷积，输出结果是224×224×64，因为使用了same卷积，通道数量也一样 接下来创建一个池化层，池化层将输入图像进行压缩，减少到112×112×64。然后又是若干个卷积层，使用128个过滤器，以及一些same卷积，输出112×112×128。然后进行池化，池化后的结果是56×56×128。再用256个相同的过滤器进行三次卷积操作，然后再池化，然后再卷积三次，再池化。如此进行几轮操作后，将最后得到的7×7×512的特征图进行全连接操作，得到4096个单元，然后进行softmax激活，输出从1000个对象中识别的结果 VGG-16的数字16指在这个网络中有13个卷积层和3个全链接层 总共包含约1.38亿个参数，这种网络结构很规整，都是几个卷积层后面跟着可以压缩图像大小的池化层，池化层缩小图像的高度和宽度。同时，卷积层的过滤器数量变化存在一定的规律，由64翻倍变成128，再到256和512。主要缺点是需要训练的特征数量非常巨大 随着网络的加深，图像的高度和宽度都在以一定的规律不断缩小，每次池化后刚好缩小一半，而通道数量在不断增加，而且刚好也是在每组卷积操作后增加一倍。即图像缩小的比例和通道数增加的比例是有规律的 2.2 残差网络（Residual Networks (ResNets)）人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系，这种神经网络被称为Residual Networks(ResNets) Residual Networks由许多隔层相连的神经元子模块组成，称之为Residual block（残差块）。单个Residual block的结构如下图所示： [ 紫色线是skip connection（跳跃连接），直接建立a^{[l]}与a^{[l+2]}之间的隔层联系。相应的表达式如下： z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]} a^{[l+1]}=g(z^{[l+1]}) z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]} a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$a^{[l]}$直接隔层与下一层的线性输出相连，a^{[l]}插入的时机是在线性激活之后，ReLU激活之前，与z^{[l+2]}共同通过激活函数（ReLU）输出a^{[l+2]} 这种模型结构对于训练非常深的神经网络效果很好。非Residual Networks称为Plain Network [ Residual Network的结构 [ Plain Network 与Plain Network相比，Residual Network能够训练更深层的神经网络，有效避免发生发生梯度消失和梯度爆炸 随着神经网络层数增加，Plain Network实际性能会变差，training error甚至会变大 Residual Network的训练效果却很好，training error一直呈下降趋势 2.3 残差网络为什么有用？（Why ResNets work?） 输入X 经过一个大型神经网络输出激活值a^{[l]}，再给这个网络额外添加两层作为一个ResNets块，输出a^{\left\lbrack l + 2 \right\rbrack}： a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})假设在整个网络中使用ReLU激活函数，所以激活值都大于等于0，包括输入X的非零异常值。因为ReLU激活函数输出的数字要么是0，要么是正数 如果使用L2正则化或权重衰减，会压缩W^{\left\lbrack l + 2\right\rbrack}的值。W是关键项，如果W^{\left\lbrack l + 2 \right\rbrack} = 0，方便起见，假设b^{\left\lbrack l + 2 \right\rbrack} = 0，假定使用ReLU激活函数，并且所有激活值都是非负的，g\left(a^{[l]} \right)是应用于非负数的ReLU函数，所以a^{[l+2]} =a^{[l]} 可以看出，即使发生了梯度消失，W^{[l+2]}\approx0，b^{[l+2]}\approx0，也能直接建立a^{[l+2]}与a^{[l]}的线性关系，且a^{[l+2]}=a^{[l]}，这就是identity function（恒等函数）。a^{[l]}直接连到a^{[l+2]}，相当于直接忽略了a^{[l]}之后的这两层神经层。这样看似很深的神经网络，由于许多Residual blocks的存在，弱化削减了某些神经层之间的联系，实现隔层线性传递，而不是一味追求非线性关系，模型本身也就能“容忍”更深层的神经网络了。从性能上来说，这两层额外的Residual blocks也不会降低Big NN的性能，所以给大型神经网络增加两层，不论是把残差块添加到神经网络的中间还是末端位置，都不会影响网络的表现 如果Residual blocks确实能训练得到非线性关系，那么也会忽略short cut，跟Plain Network起到同样的效果 如果Residual blocks中a^{[l+2]}与a^{[l]}的维度不同，可以引入矩阵W_s与a^{[l]}相乘，使得W_s*a^{[l]}的维度与a^{[l+2]}一致 参数矩阵W_s有来两种方法得到： 将W_s作为学习参数，通过模型训练得到 固定W_s值（类似单位矩阵），不需要训练，W_s与a^{[l]}的乘积仅使得a^{[l]}截断或者补零 CNN中ResNets的结构： ResNets同类型层之间，例如CONV layers，大多使用same类型，这也解释了添加项z^{[l+2]}+a^{[l]}（维度相同所以能够相加）。如果是不同类型层之间的连接，例如CONV layer与POOL layer之间，如果维度不同，则引入矩阵W_s 2.4 网络中的网络以及 1×1 卷积（Network in Network and 1×1 convolutions）如果是一张6×6×32的图片，使用1×1过滤器进行卷积效果更好。1×1卷积所实现的功能是遍历这36个单元格，计算左图中32个数字和过滤器中32个数字的元素积之和，然后应用ReLU非线性函数 1×1×32过滤器中的32个数字可以理解为一个神经元的输入是32个数字，这32个数字具有不同通道，乘以32个权重（将过滤器中的32个数理解为权重），然后应用ReLU非线性函数，输出相应的结果 如果过滤器是多个，就好像有多个输入单元，其输入内容为一个切片上所有数字，输出结果是6×6×#filters 1×1卷积可以从根本上理解为对这32个不同的位置都应用一个全连接层，全连接层的作用是输入32个数字（过滤器数量标记为n\_{C}^{\left\lbrack l + 1\right\rbrack}，在这36个单元上重复此过程）,输出结果是6×6×#filters（过滤器数量），以便在输入层上实施一个非平凡（non-trivial）计算 这种方法通常称为1×1卷积，也被称为Network in Network 假设一个28×28×192的输入层，如果通道数量很大，可以用32个大小为1×1×192的过滤器，使输出层为28×28×32，这就是压缩通道数（n_{C}）的方法 如果想保持通道数192不变，也是可行的，1×1卷积只是添加了非线性函数，也可以让网络学习更复杂的函数 1×1卷积层给神经网络添加了一个非线性函数，从而减少或保持输入层中的通道数量不变，也可以增加通道数量 2.5 谷歌 Inception 网络简介（Inception network motivation）Inception网络或Inception层的作用是代替人工来确定卷积层中的过滤器类型，或者确定是否需要创建卷积层或池化层 基本思想是Inception网络在单层网络上可以使用多个不同尺寸的filters，进行same convolutions，把各filter下得到的输出拼接起来。还可以将CONV layer与POOL layer混合，同时实现各种效果，但是要注意使用same pool。Inception Network不需要人为决定使用哪个过滤器或者是否需要池化，它使用不同尺寸的filters并将CONV和POOL混合起来，将所有功能输出组合拼接，再由神经网络本身去学习参数并选择最好的模块 Inception Network在提升性能的同时，会带来计算量大的问题： 乘法运算的总次数为每个输出值所需要执行的乘法运算次数（5×5×192）乘以输出值个数（28×28×32），结果等于1.2亿。 为此，引入1x1 Convolutions来减少计算量，对于输入层，使用1×1卷积把输入值从192个通道减少到16个通道。然后对这个较小层运行5×5卷积，得到最终输出 把该1x1 Convolution称为“瓶颈层”（bottleneck layer），瓶颈层是网络中最小的部分，即先缩小网络，然后再扩大 引入bottleneck layer之后，第一个卷积层计算成本：1×1×192×输出28×28×16，相乘结果约等于240万，第二个卷积层的计算成本是：28×28×32×5×5×16，计算结果为1000万，总次数是1204万，计算成本从1.2亿下降到了原来的十分之一 总结： 如果在构建神经网络层的时候，不想决定池化层是使用1×1，3×3还是5×5的过滤器，Inception模块是最好的选择。可以应用各种类型的过滤器，只需要把输出连接起来 计算成本问题，通过使用1×1卷积来构建瓶颈层，大大降低计算成本 只要合理构建瓶颈层，既可以显著缩小表示层规模，又不会降低网络性能，从而节省了计算 2.6 Inception 网络（Inception network）引入1x1 Convolution后的Inception module如下图所示： Inception模块会将之前层的激活或者输出作为它的输入，为了能在最后将这些输出都连接起来，会使用same类型的padding来池化，使得输出的高和宽依然是28×28，这样才能将它与其他输出连接起来。如果进行了最大池化，即便用了same padding，3×3的过滤器，stride为1，其输出将会是28×28×192，其通道数与输入（通道数）相同。要做的是再加上一个1×1的卷积层，将通道的数量缩小到28×28×32，避免了最后输出时，池化层占据所有的通道 最后把得到的各个层的通道都加起来，得到一个28×28×256的输出。这就是一个Inception模块 Inception网络只是很多在不同的位置重复组成的网络： 中间隐藏层也可以作为输出层Softmax，确保了即便是隐藏单元和中间层也参与了特征计算，也能预测图片的分类，起到一种调整的效果，有利于防止发生过拟合 2.7 迁移学习（Transfer Learning）训练集很小的情况： 建议：从网上下载一些神经网络开源的实现，不仅把代码下载下来，也把权重下载下来。然后去掉Softmax层，创建自己的Softmax单元，用来输出Tigger、Misty和neither三个类别。把所有的层看作是冻结的，冻结网络中所有层的参数，只需要训练和Softmax层有关的参数。这个Softmax层有三种可能的输出，Tigger、Misty或者Neither。 通过使用其他人预训练的权重，很可能得到很好的性能，即使只有一个小的数据集。大多数深度学习框架会有trainableParameter=0的参数，对于前面的层，可以设置这个参数。为了不训练这些权重，会有freeze=1的参数。只需要训练softmax层的权重，把前面这些层的权重都冻结 由于前面的层都冻结了，相当于一个固定的函数，因此不需要改变和训练它，取输入图像X，然后把它映射到softmax前一层的激活函数。能加速训练的技巧是如果先计算这一层（紫色箭头标记），计算特征或者激活值，然后把它们存到硬盘里。所做的就是用这个固定的函数，在这个神经网络的前半部分（softmax层之前的所有层视为一个固定映射），取任意输入图像X，然后计算它的某个特征向量，这样训练的就是一个很浅的softmax模型，用这个特征向量来做预测。对计算有用的一步就是对训练集中所有样本的这一层的激活值进行预计算，然后存储到硬盘里，在此之上训练softmax分类器。存储到硬盘或者说预计算方法的优点是不需要每次遍历训练集再重新计算这个激活值 更大的训练集：应该冻结更少的层，然后训练后面的层。如果输出层的类别不同，那么需要构建自己的输出单元，Tigger、Misty或者Neither三个类别。可以取后面几层的权重，用作初始化，然后从这里开始梯度下降 也可以直接去掉这几层，换成自己的隐藏单元和softmax输出层，如果有越来越多的数据，那么需要冻结的层数就越少，能够训练的层数就越多。如果有一个更大的数据集，那么不要单单训练一个softmax单元，而是考虑训练中等大小的网络，包含最终要用的网络的后面几层 如果有大量数据：应该做的就是用开源的网络和它的权重，把所有的权重当作初始化，然后训练整个网络 如果有越多的标定的数据，可以训练越多的层。极端情况下，可以用下载的权重只作为初始化，用它们来代替随机初始化，接着用梯度下降训练，更新网络所有层的所有权重 2.8 数据扩充（Data augmentation）当下计算机视觉的主要问题是没有办法得到充足的数据 最简单的数据扩充方法就是垂直镜像对称 另一个经常使用的技巧是随机裁剪，给定一个数据集，然后开始随机裁剪，得到不同的图片放在数据集中，随机裁剪并不是一个完美的数据扩充的方法，如果随机裁剪的那一部分（红色方框标记部分，编号4）看起来不像猫。但在实践中，这个方法还是很实用的，随机裁剪构成了很大一部分的真实图片 也可以使用旋转，剪切（仅水平或垂直坐标发生变化）图像，扭曲变形，引入很多形式的局部弯曲等等，但在实践中太复杂所以使用的很少 彩色转换：给R、G和B三个通道上加上不同的失真值 实践中对R、G和B的变化是基于某些分布，改变可能很小，R、G和B的值是根据某种概率分布来决定，这样会使得学习算法对照片的颜色更改更具鲁棒性 对R、G和B有不同的采样方式，其中一种影响颜色失真的算法是PCA，即主成分分析，PCA颜色增强的大概含义是，如果图片呈现紫色，即主要含有红色和蓝色，绿色很少，然后PCA颜色增强算法就会对红色和蓝色增减很多，绿色变化相对少一点，所以使总体的颜色保持一致 如果有特别大的训练数据，可以使用CPU线程，不停的从硬盘中读取数据，用CPU线程来实现失真变形，可以是随机裁剪、颜色变化，或者是镜像 同时CPU线程持续加载数据，然后实现任意失真变形，从而构成批数据或者最小批数据，这些数据持续的传输给其他线程或者其他的进程，然后开始训练，可以在CPU或者GPU上实现一个大型网络的训练 常用的实现数据扩充的方法是使用一个线程或者是多线程来加载数据，实现变形失真，然后传给其他的线程或者其他进程，来训练编号2和这个编号1，可以并行实现 在数据扩充过程中也有一些超参数，比如说颜色变化了多少，以及随机裁剪的时候使用的参数 2.9 计算机视觉现状（The state of computer vision） 大部分机器学习问题是介于少量数据和大量数据范围之间的。 语音识别有很大数量的数据 虽然现在图像识别或图像分类方面有相当大的数据集，但因为图像识别是一个复杂的问题，通过分析像素并识别出它是什么，即使在线数据集非常大，如超过一百万张图片，仍然希望能有更多的数据 物体检测拥有的数据更少 图像识别是如何看图片的问题，并且告诉你这张图是不是猫，而对象检测则是看一幅图，画一个框，告诉你图片里的物体，比如汽车等等。因为获取边框的成本比标记对象的成本更高，所以进行对象检测的数据往往比图像识别数据要少 当有很多数据时，倾向于使用更简单的算法和更少的手工工程，只要有一个大型的神经网络，甚至一个更简单的架构，就可以去学习它想学习的东西 当没有那么多的数据时，更多的是手工工程 对机器学习应用时，通常学习算法有两种知识来源： 一个来源是被标记的数据，像(x,y)应用在监督学习 第二个来源是手工工程，有很多方法去建立一个手工工程系统，它可以是源于精心设计的特征，手工精心设计的网络体系结构或者是系统的其他组件。当没有太多标签数据时，只需要更多地考虑手工工程 在基准研究和比赛中，下面的tips可能会有较好的表现： 集成，意味着想好了要的神经网络之后，可以独立训练几个神经网络，并平均它们的输出。比如说随机初始化三个、五个或者七个神经网络，然后训练所有这些网络，对输出\hat y进行平均计算，而不要平均权重，可能会在基准上提高1%，2%或者更好。但因为集成意味着要对每张图片进行测试，可能需要在从3到15个不同的网络中运行一个图像，会让运行时间变慢 Multi-crop at test time，Multi-crop是一种将数据扩充应用到测试图像中的一种形式，在测试图片的多种版本上运行分类器，输出平均结果 如把猫的图片复制四遍，包括两个镜像版本。如取中心的crop，然后取四个角落的crop，通过分类器来运行它 编号1和编号3是中心crop，编号2和编号4是四个角落的crop。把这些加起来会有10种不同的图像的crop，命名为10-crop。通过分类器来运行这十张图片，然后对结果进行平均 集成的一个大问题是需要保持所有这些不同的神经网络，占用了更多的计算机内存。multi-crop，只保留一个网络，不会占用太多的内存，但仍然会让运行时间变慢]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周 卷积神经网络（Foundations of Convolutional Neural Networks）(Course 4)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%B8%80%E5%91%A8-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Foundations-of-Convolutional-Neural-Networks%EF%BC%89-Course-4%2F</url>
    <content type="text"><![CDATA[1.1 计算机视觉（Computer vision）图片分类，或图片识别： 目标检测： 神经网络实现图片风格迁移： 使用传统神经网络处理机器视觉的一个主要问题是输入层维度很大。例如一张64x64x3的图片，神经网络输入层的维度为12288。如果图片尺寸较大，例如一张1000x1000x3的图片，神经网络输入层的维度将达到3百万，使得网络权重W非常庞大。这样会造成两个后果，一是神经网络结构复杂，数据量相对不够，容易出现过拟合；二是所需内存、计算量较大。解决这一问题的方法就是使用卷积神经网络（CNN）。 1.2边缘检测示例（Edge detection example）对于CV问题，神经网络由浅层到深层，分别可以检测出图片的边缘特征 、局部特征（例如眼睛、鼻子等）、整体面部轮廓 图片的边缘检测最常检测的图片边缘有两类：一是垂直边缘（vertical edges），二是水平边缘（horizontal edges） 图片的边缘检测可以通过与相应滤波器进行卷积来实现。以垂直边缘检测为例，原始图片尺寸为6x6，滤波器filter尺寸为3x3，卷积后的图片尺寸为4x4，得到结果如下： ∗表示卷积操作。python中，卷积用conv_forward()表示；tensorflow中，卷积用tf.nn.conv2d()表示；keras中，卷积用Conv2D()表示 垂直边缘是一个3×3的区域，左边是明亮的像素，中间的并不需要考虑，右边是深色像素。在这个6×6图像的中间部分，明亮的像素在左边，深色的像素在右边，就被视为一个垂直边缘 ] 1.3 更多边缘检测内容（More edge detection）图片边缘有两种渐变方式，一种是由明变暗，另一种是由暗变明。实际应用中，这两种渐变方式并不影响边缘检测结果，可以对输出图片取绝对值操作，得到同样的结果 由亮向暗 由暗向亮 下图的垂直边缘过滤器是一个3×3的区域，左边相对较亮，右边相对较暗。右图的水平边缘过滤器也是一个3×3的区域，上边相对较亮，而下方相对较暗 30（右边矩阵中绿色方框标记元素）代表了左边这块3×3的区域（左边矩阵绿色方框标记部分），这块区域是上边比较亮，下边比较暗，所以它在这里发现了一条正边缘。而-30（右边矩阵中紫色方框标记元素）代表了左边另一块区域（左边矩阵紫色方框标记部分），这块区域是底部比较亮，而上边则比较暗，所以在这里它是一条负边 10（右边矩阵中黄色方框标记元素）代表的是左边这块区域（左边6×6矩阵中黄色方框标记的部分）。这块区域左边两列是正边，右边一列是负边，正边和负边的值加在一起得到了一个中间值。但假如这是一个非常大的1000×1000大图，就不会出现亮度为10的过渡带了，因为图片尺寸很大，这些中间值就会变得非常小 对于这个3×3的过滤器来说，使用了其中的一种数字组合： 还可以使用这种： \begin{bmatrix}1 & 0 & - 1 \\ 2 & 0 & - 2 \\ 1 & 0 & - 1 \end{bmatrix}叫做Sobel过滤器，优点在于增加了中间一行元素的权重，使得结果的鲁棒性会更高一些 或者： \begin{bmatrix} 3& 0 & - 3 \\ 10 & 0 & - 10 \\ 3 & 0 & - 3 \end{bmatrix}叫做Scharr过滤器，也是一种垂直边缘检测，如果将其翻转90度，就能得到对应水平边缘检测 随着深度学习的发展，如果想检测图片的各种边缘特征，而不仅限于垂直边缘和水平边缘，那么filter的数值一般需要通过模型训练得到，将矩阵的所有数字都设置为参数，通过数据反馈，让神经网络自动去学习它们，类似于标准神经网络中的权重W一样由梯度下降算法反复迭代求得，会发现神经网络可以学习一些低级的特征，例如这些边缘的特征。CNN的主要目的就是计算出这些filter的数值，确定得到了这些filter后，CNN浅层网络也就实现了对图片所有边缘特征的检测 1.4 Padding 如果有一个n\times n的图像，用f\times f的过滤器做卷积，输出的维度就是(n-f+1)\times (n-f+1) 这样的话会有两个缺点: 每次做卷积操作，输出图片尺寸缩小 原始图片边缘信息对输出贡献得少，输出图片丢失边缘信息 角落边缘的像素（绿色阴影标记）只被一个输出所触碰或者使用，中间的像素点（红色方框标记）会有许多3×3的区域与之重叠。角落或者边缘区域的像素点在输出中采用较少，丢掉了图像边缘位置的许多信息 可以在卷积操作之前填充这幅图像。沿着图像边缘再填充一层像素,6×6的图像填充成8×8的图像。就得到了一个尺寸和原始图像6×6的图像。习惯上，可以用0去填充，如果p是填充的数量，输出也就变成了(n+2p-f+1)\times (n+2p-f+1)。涂绿的像素点（左边矩阵）影响了输出中的这些格子（右边矩阵）。这样角落或图像边缘的信息发挥的作用较小的这一缺点就被削弱了 选择填充多少像素，通常有两个选择，分别叫做Valid卷积和Same卷积 Valid卷积意味着不填充，如果有一个n\times n的图像，用一个f\times f的过滤器卷积，会给一个(n-f+1)\times (n-f+1)维的输出 另一个叫做Same卷积，填充后输出大小和输入大小是一样的。由n-f+1，当填充p个像素点，n就变成了n+2p，公式变为： n+2p-f+1即： p=\frac{f-1}{2}当f是一个奇数，只要选择相应的填充尺寸就能确保得到和输入相同尺寸的输出 计算机视觉中，f通常是奇数，有两个原因： 如果f是偶数，只能使用一些不对称填充 当有一个奇数维过滤器，比如3×3或者5×5的，它就有一个中心点，便于指出过滤器的位置 1.5 卷积步长（Strided convolutions）Stride表示filter在原图片中水平方向和垂直方向每次的步进长度。之前默认stride=1。若stride=2，则表示filter每次步进长度为2，即隔一点移动一次 用s表示stride长度，p表示padding长度，如果原始图片尺寸为n x n，filter尺寸为f x f，则卷积后的图片尺寸为： \lfloor\frac{n+2p-f}{s}+1\rfloor\ \times\ \lfloor\frac{n+2p-f}{s}+1\rfloor真正的卷积运算会先将filter绕其中心旋转180度，然后再将旋转后的filter在原始图片上进行滑动计算。filter旋转如下所示： 相关系数的计算过程则不会对filter进行旋转，而是直接在原始图片上进行滑动计算 目前为止介绍的CNN卷积实际上计算的是相关系数，而不是数学意义上的卷积。为了简化计算，一般把CNN中的这种“相关系数”就称作卷积运算。之所以可以这么等效，是因为滤波器算子一般是水平或垂直对称的，180度旋转影响不大；而且最终滤波器算子需要通过CNN网络梯度下降算法计算得到，旋转部分可以看作是包含在CNN模型算法中。忽略旋转运算可以大大提高CNN网络运算速度，而且不影响模型性能。 卷积运算服从分配律： (A*B)*C=A*(B*C)1.6三维卷积（Convolutions over volumes）3通道的RGB图片对应的滤波器算子也是3通道的。例如一个图片是6 x 6 x 3，分别表示图片的高度（height）、宽度（weight）和通道（#channel） 3通道图片的卷积运算与单通道图片的卷积运算基本一致。过程是将每个单通道（R，G，B）与对应的filter进行卷积运算求和，然后再将3通道的和相加，得到输出图片的一个像素值 不同通道的滤波算子可以不相同。例如R通道filter实现垂直边缘检测，G和B通道不进行边缘检测，全部置零，或者将R，G，B三通道filter全部设置为水平边缘检测 为了进行多个卷积运算，实现更多边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。做完卷积，然后把这两个4×4的输出堆叠在一起，第一个放到前面，第二个放到后面，就得到一个4×4×2的输出立方体 不同滤波器组卷积得到不同的输出，个数由滤波器组决定 若输入图片的尺寸为n x n xn_c，filter尺寸为f x f x n_c，则卷积后的图片尺寸为(n-f+1) x (n-f+1) x {n}'_c(默认padding为1）。n_c为图片通道数目，{n}'_c为滤波器组个数 1.7单层卷积网络（One layer of a convolutional network）卷积神经网络的单层结构如下所示： 相比之前的卷积过程，CNN的单层结构多了激活函数ReLU和偏移量b。整个过程与标准的神经网络单层结构非常类似： Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} A^{[l]}=g^{[l]}(Z^{[l]})卷积运算对应着上式中的乘积运算，滤波器组数值对应着权重W^{[l]}，所选的激活函数为ReLU 每个滤波器组有3x3x3=27个参数，还有1个偏移量b，则每个滤波器组有27+1=28个参数，两个滤波器组总共包含28x2=56个参数。选定滤波器组后，参数数目与输入图片尺寸无关。所以不存在由于图片尺寸过大，造成参数过多的情况，这就是卷积神经网络的一个特征，叫作“避免过拟合”。例如一张1000x1000x3的图片，标准神经网络输入层的维度将达到3百万，而在CNN中，参数数目只由滤波器组决定，数目相对来说要少得多，这是CNN的优势之一 设层数为l，CNN单层结构的所有标记符号： f^{[l]}= filter size p^{[l]}= padding s^{[l]}= stride n_c^{[l]}= number of filters 输入维度为：n_H^{[l-1]}\times n_W^{[l-1]}\times n_c^{[l-1]}，因为是上一层的激活值每个滤波器组维度为：f^{[l]}\times f^{[l]}\times n_c^{[l-1]} 权重维度为：f^{[l]}\times f^{[l]}\times n_c^{[l-1]}\times n_c^{[l]} 偏置维度为：1 \times 1\times 1 \times n_c^{[l]} 输出维度为：n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]} 其中： n_H^{[l]}=\lfloor \frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \rfloor n_W^{[l]}=\lfloor \frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \rfloor如果有m个样本，进行向量化运算，相应的输出维度为： m \times n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}1.8 简单卷积网络示例（A simple convolution network example）简单的CNN网络模型： a^{[3]}$$的维度是7 x 7 x 40，将$$a^{[3]}$$排列成1列，维度为1960 x 1，然后连接最后一级输出层。输出层可以是一个神经元，即二元分类（logistic）；也可以是多个神经元，即多元分类（softmax）。最后得到预测输出$$\hat y随着CNN层数增加，n_H^{[l]}和n_W^{[l]}一般逐渐减小，而n_c^{[l]}一般逐渐增大 CNN有三种类型的layer： Convolution层（CONV） Pooling层（POOL） Fully connected层（FC） CONV最为常见也最重要 1.9 池化层（Pooling layers）Pooling layers是CNN中用来减小尺寸，提高运算速度的，同样能减小noise影响，让各特征更具有健壮性 Pooling layers没有卷积运算，仅在滤波器算子滑动区域内取最大值，即max pooling，这是最常用的做法。超参数p很少在pooling layers中使用 Max pooling的好处是只保留区域内的最大值（特征），数字大意味着可能探测到了某些特定的特征，忽略了其它值，降低了noise影响，提高了模型健壮性。max pooling需要的超参数仅为滤波器尺寸f和滤波器步进长度s，没有其他参数需要模型训练得到，计算量很小 如果是多个通道，每个通道单独进行max pooling操作： average pooling是在滤波器算子滑动区域计算平均值： 实际应用中，max pooling比average pooling更为常用，也有例外，深度很深的神经网络可以用平均池化来分解规模为7×7×1000的网络的表示层，在整个空间内求平均值，得到1×1×1000 总结： 池化的超级参数包括过滤器大小f和步幅s，常用的参数值为f=2，s=2，应用频率非常高，其效果相当于高度和宽度缩减一半。最大池化时，往往很少用到超参数padding，p最常用的值是0，即p=0。最大池化的输入就是： n_{H} \times n_{W} \times n_{c}假设没有padding，则输出： \lfloor\frac{n_{H} - f}{s} +1\rfloor \times \lfloor\frac{n_{w} - f}{s} + 1\rfloor \times n_{c}输入通道与输出通道个数相同，因为对每个通道都做了池化。最大池化只是计算神经网络某一层的静态属性，池化过程中没有需要学习的参数。执行反向传播时，反向传播没有参数适用于最大池化 1.10 卷积神经网络示例（Convolutional neural network example）简单的数字识别CNN例子： CONV层后面紧接一个POOL层，CONV1和POOL1构成第一层，CONV2和POOL2构成第二层。FC3和FC4为全连接层FC，跟标准的神经网络结构一致。最后的输出层（softmax）由10个神经元构成 整个网络各层的尺寸和参数如下表格所示： 池化层和最大池化层没有参数；卷积层的参数相对较少，许多参数都存在于神经网络的全连接层。随着神经网络的加深，激活值尺寸会逐渐变小，如果激活值尺寸下降太快，也会影响神经网络性能 尽量不要自己设置超参数，而是查看文献中别人采用了哪些超参数，选一个在别人任务中效果很好的架构，也可能适用于自己的应用程序 在神经网络中，另一种常见模式就是一个或多个卷积后面跟随一个池化层，然后一个或多个卷积层后面再跟一个池化层，然后是几个全连接层，最后是一个softmax 1.11 为什么使用卷积？（Why convolutions?）和只用全连接层相比，卷积层的两个主要优势在于参数共享和稀疏连接 如果这是一张1000×1000的图片，权重矩阵会变得非常大。而卷积层的参数数量：每个过滤器都是5×5，一个过滤器有25个参数，再加上偏差参数，那么每个过滤器就有26个参数，一共有6个过滤器，所以参数共计156个，参数数量很少 卷积网络映射这么少参数有两个原因： 参数共享：一个特征检测器（例如垂直边缘检测）对图片某块区域有用，同时也可能作用在图片其它区域。 特征检测如垂直边缘检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。如果用一个3×3的过滤器检测垂直边缘，那么图片的左上角区域，以及旁边的各个区域（左边矩阵中蓝色方框标记的部分）都可以使用这个3×3的过滤器。每个特征检测器以及输出都可以在输入图片的不同区域中使用同样的参数，以便提取垂直边缘或其它特征。它不仅适用于边缘特征这样的低阶特征，同样适用于高阶特征，例如提取脸上的眼睛，猫或者其他特征对象。即使减少参数个数，这9个参数同样能计算出16个输出。直观感觉是，一个特征检测器，如垂直边缘检测器用于检测图片左上角区域的特征，这个特征很可能也适用于图片的右下角区域。因此在计算图片左上角和右下角区域时，不需要添加其它特征检测器 连接的稀疏性：因为滤波器算子尺寸限制，每一层的每个输出只与输入部分区域内有关 右边输出单元（元素0）仅与36个输入特征中9个相连接。其它像素值都不会对输出产生任何影响，输出（右边矩阵中红色标记的元素 30）仅仅依赖于这9个特征（左边矩阵红色方框标记的区域），只有这9个输入特征与输出相连接，其它像素对输出没有任何影响 神经网络可以通过这两种机制减少参数，以便用更小的训练集来训练它，从而预防过拟合。CNN比较擅长捕捉区域位置偏移，也就是说CNN进行物体检测时，不太受物体所处图片位置的影响，增加检测的准确性和系统的健壮性。通过观察可以发现，向右移动两个像素，图片中的猫依然清晰可见，因为神经网络的卷积结构使得即使移动几个像素，这张图片依然具有非常相似的特征，应该属于同样的输出标记 最后，把这些层整合起来，比如要构建一个猫咪检测器，x表示一张图片，\hat{y}是二进制标记或某个重要标记。选定一个卷积神经网络，输入图片，增加卷积层和池化层，然后添加全连接层，并随机初始化参数w和b，最后输出一个softmax，即\hat{y}，代价函数J等于神经网络对整个训练集的预测的损失总和再除以m（即\text{Cost} J = \frac{1}{m}\sum_{i = 1}^{m}{L(\hat{y}^{(i)},y^{(i)})}）。所以训练神经网络，要做的就是使用梯度下降法，或其它算法，例如Momentum梯度下降法，含RMSProp或其它因子的梯度下降来优化神经网络中所有参数，以减少代价函数J的值]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周：机器学习策略（2）(ML Strategy (2))(Course 3)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%EF%BC%882%EF%BC%89-ML-Strategy-2-Course-2%2F</url>
    <content type="text"><![CDATA[2.1 进行误差分析（Carrying out error analysis）如果希望让学习算法能够胜任人类能做的任务，但学习算法还没有达到人类的表现，那么人工检查一下算法犯的错误可以了解接下来应该做什么，这个过程称为错误分析假设正在调试猫分类器，取得了90%准确率，相当于10%错误，注意到算法将一些狗分类为猫，需要对模型的一些部分做相应调整，才能更好地提升分类的精度 收集错误样例： 在开发集（测试集）中，获取大约100个错误标记的例子，然后手动检查，一次只看一个，看看开发集里有多少错误标记的样本是狗 100个数据中有5个样例是狗，如果对数据集的错误标记做努力去改进模型的精度，可以提升的上限是5%，仅可以达到9.5%的错误率，称为性能上限（ceiling on performance）。这种情况下，这样耗时的努力方向不是很值得的事 100个数据中，有50多个样例是狗，改进数据集的错误标记是一个值得的改进方向，可以将模型的精确度提升至95 并行分析： 修改那些被分类成猫的狗狗图片标签 修改那些被错误分类的大型猫科动物，如：狮子，豹子等 提升模糊图片的质量 为了并行的分析，可以建立表格来进行。在最左边，人工过一遍想分析的图像集，电子表格的每一列对应要评估的想法，如狗的问题，猫科动物的问题，模糊图像的问题，最后一列写评论 在这个步骤做到一半时，可能会发现其他错误类型，比如可能发现有Instagram滤镜，那些花哨的图像滤镜，干扰了分类器。在这种情况下可以在错误分析途中，增加一列多色滤镜 Instagram滤镜和Snapchat滤镜，再过一遍，并确定新的错误类型百分比，这个分析步骤的结果可以给出一个估计，是否值得去处理每个不同的错误类型 可以把团队可以分成两个团队，其中一个改善大猫的识别，另一个改善模糊图片的识别 总结： 进行错误分析，应该找一组错误样本，可能在开发集或者测试集，观察错误标记的样本，看看假阳性（false positives）和假阴性（false negatives），统计属于不同错误类型的错误数量。在这个过程中，可能会得到启发，归纳出新的错误类型，通过统计不同错误标记类型的百分比，可以发现哪些问题需要优先解决 2.2 清楚标注错误的数据（Cleaning up Incorrectly labeled data）监督学习问题的数据由输入x和输出标签y 构成，如果发现有些输出标签 y 是错的，是否值得花时间去修正这些标签？ 倒数第二不是猫，是标记错误的样本。“标记错误的样本”表示学习算法输出了错误的 y 值，如果数据有一些标记错误的样本，该怎么办？ 训练集：深度学习算法对于训练集中的随机错误是相当健壮的（robust）。只要这些错误样本离随机错误不太远，有时可能做标记的人没有注意或者不小心，按错键了，如果错误足够随机，放着这些错误不管可能也没问题，而不要花太多时间修复它们，只要总数据集足够大，实际错误率可能不会太高 深度学习算法对随机误差很健壮，但对系统性的错误没那么健壮。如果做标记的人一直把白色的狗标记成猫，那就成问题。因为分类器学习之后，会把所有白色的狗都分类为猫。但随机错误或近似随机错误，对于大多数深度学习算法来说不成问题 开发集和测试集有标记出错的样本：在错误分析时，添加一个额外的列，统计标签 y=1错误的样本数。统计因为标签错误所占的百分比，解释为什么学习算法做出和数据集的标记不一样的预测1 是否值得修正6%标记出错的样本： 如果标记错误严重影响了在开发集上评估算法的能力，应该去花时间修正错误的标签 如果没有严重影响到用开发集评估成本偏差的能力，不应该花时间去处理 看3个数字来确定是否值得去人工修正标记出错的数据： 看整体的开发集错误率，系统达到了90%整体准确度，10%错误率，应该看错误标记引起的错误的数量或者百分比。6％的错误来自标记出错，10%的6%是0.6%，剩下的占9.4%，是其他原因导致的，比如把狗误认为猫，大猫图片。即有9.4%错误率需要集中精力修正，而标记出错导致的错误是总体错误的一小部分而已，应该看其他原因导致的错误 错误率降到了2％，但总体错误中的0.6%还是标记出错导致的。修正开发集里的错误标签更有价值 开发集的主要目的是从两个分类器A和B中选择一个。当测试两个分类器A和B时，在开发集上一个有2.1%错误率，另一个有1.9%错误率，但是不能再信任开发集，因为它无法告诉你这个分类器是否比这个好，因为0.6%的错误率是标记出错导致的。现在就有很好的理由去修正开发集里的错误标签，因为右边这个样本标记出错对算法错误的整体评估标准有严重的影响，而左边相对较小 如果决定要去修正开发集数据，手动重新检查标签，并尝试修正一些标签，这里还有一些额外的方针和原则需要考虑： 不管用什么修正手段，都要同时作用到开发集和测试集上，开发和测试集必须来自相同的分布。开发集确定了目标，当击中目标后，希望算法能够推广到测试集上，这样能够更高效的在来自同一分布的开发集和测试集上迭代 如果打算修正开发集上的部分数据，最好也对测试集做同样的修正以确保它们继续来自相同的分布。可以让一个人来仔细检查这些标签，但必须同时检查开发集和测试集 要同时检验算法判断正确和判断错误的样本，如果只修正算法出错的样本，算法的偏差估计可能会变大，会让算法有一点不公平的优势 修正训练集中的标签相对没那么重要，如果训练集来自稍微不同的分布，对于这种情况学习算法其实相当健壮，通常是一件很合理的事情 几个建议： 构造实际系统时，需要更多的人工错误分析，更多的人类见解来架构这些系统 搭建机器学习系统时，花时间亲自检查数据非常值得，可以帮你找到需要优先处理的任务，然后确定应该优先尝试哪些想法，或者哪些方向 2.3 快速搭建你的第一个系统，并进行迭代（Build your first system quickly, then iterate）如果正在开发全新的机器学习应用，应该尽快建立第一个系统原型，然后快速迭代 改进语音识别系统特定的技术: 对于几乎所有的机器学习程序可能会有50个不同的方向可以前进，并且每个方向都是相对合理的可以改善系统。但挑战在于如何选择一个方向集中精力处理。如果想搭建全新的机器学习程序，就是快速搭好第一个系统，然后开始迭代。首先快速设立开发集和测试集还有指标，决定目标所在，如果目标定错，之后改也可以。但一定要设立某个目标，然后马上搭好一个机器学习系统原型，找到训练集训练一下，看算法表现如何，在开发集测试集，评估指标表现如何。当建立第一个系统后，就可以马上用到偏差方差分析和错误分析，来确定下一步优先做什么。如果错误分析到大部分的错误来源是说话人远离麦克风，就有很好的理由去集中精力研究这些技术，所谓远场语音识别的技术，就是处理说话人离麦克风很远的情况 建立初始系统所有意义：是一个快速和粗糙的实现（quick and dirty implementation），有一个学习过的系统，有一个训练过的系统，确定偏差方差的范围，知道下一步应该优先做什么，能够进行错误分析，观察一些错误，然后想出所有能走的方向，哪些是实际上最有希望的方向 当这个领域有很多可以借鉴的学术文献，处理的问题和要解决的几乎完全相同，比如人脸识别有很多学术文献，如果搭建一个人脸识别设备，可以从现有大量学术文献为基础出发，一开始就搭建比较复杂的系统。但如果第一次处理某个新问题，还是构建一些快速而粗糙的实现，然后用来找到改善系统要优先处理的方向 2.4 在不同的划分上进行训练并测试（Training and testing on different distributions）猫咪识别假设只收集到10,000张用户上传的照片和超过20万张网上下载的高清猫图： 做法一：将两组数据合并在一起，把这21万张照片随机分配到训练、开发和测试集中。假设已经确定开发集和测试集各包含2500个样本，训练集有205000个样本。 好处：训练集、开发集和测试集都来自同一分布 坏处：开发集的2500个样本中很多图片都来自网页下载的图片，并不是真正关心的数据分布，因为真正要处理的是来自手机的图片 2500个样本有2500\times \frac{200k}{210k} =2381张图来自网页下载，平均只有119张图来自手机上传。设立开发集的目的是告诉团队去瞄准的目标，而瞄准目标的大部分精力却都用在优化来自网页下载的图片 建议：开发集和测试集都是2500张来自应用的图片，训练集包含来自网页的20万张图片还有5000张来自应用的图片，现在瞄准的目标就是想要处理的目标，才是真正关心的图片分布 语音激活后视镜假设有很多不是来自语音激活后视镜的数据 分配： 训练集500k段语音，开发集和测试集各包含10k段语音（从实际的语音激活后视镜收集） 也可以拿一半放训练集里，训练集51万段语音，开发集和测试集各5000 2.5 不匹配数据划分的偏差和方差（Bias and Variance with mismatched data distributions）当训练集和开发集、测试集不同分布时，分析偏差和方差的方式： 分析的问题在于，当看训练误差，再看开发误差，有两件事变了，很难确认这增加的9%误差率有多少是因为： 算法只见过训练集数据，没见过开发集数据（方差） 开发集数据来自不同的分布 为了弄清楚哪个因素影响更大，定义一组新的数据，称之为训练-开发集，是一个新的数据子集。从训练集的分布里分出来，但不会用来训练网络 随机打散训练集，分出一部分训练集作为训练-开发集（training-dev），训练集、训练-开发集来自同一分布 只在训练集训练神经网络，不让神经网络在训练-开发集上跑后向传播。为了进行误差分析，应该看分类器在训练集上的误差、训练-开发集上的误差、开发集上的误差 假设训练误差是1%，训练-开发集上的误差是9%，开发集误差是10%，存在方差，因为训练-开发集的错误率是在和训练集来自同一分布的数据中测得的，尽管神经网络在训练集中表现良好，但无法泛化到来自相同分布的训练-开发集 假设训练误差为1%，训练-开发误差为1.5%，开发集错误率10%。方差很小，当转到开发集时错误率大大上升，是数据不匹配的问题 如果训练集误差是10%，训练-开发误差是11%，开发误差为12%，人类水平对贝叶斯错误率的估计大概是0%，存在可避免偏差问题 如果训练集误差是10%，训练-开发误差是11%，开发误差是20%，有两个问题 可避免偏差问题 数据不匹配问题 如果加入测试集错误率，而开发集表现和测试集表现有很大差距，可能对开发集过拟合，需要一个更大的开发集 如果人类的表现是4%，训练错误率是7%，训练-开发错误率是10%。开发集是6%。可能开发测试集分布比实际处理的数据容易得多，错误率可能会下降 Human level 4%和Training error 7%衡量了可避免偏差大小，Training error 7%和Training-dev error 10%衡量了方差大小，Training-dev error 10%和Dev/Test dev 6%衡量了数据不匹配问题的大小 rearview mirror speech data 6%和Error on examples trained on 6%：获得这个数字的方式是让一些人标记他们的后视镜语音识别数据，看看人类在这个任务里能做多好，然后收集一些后视镜语音识别数据，放在训练集中，让神经网络去学习，测量那个数据子集上的错误率，如果得到rearview mirror speech data 6%和Error on examples trained on 6%，说明在后视镜语音数据上达到人类水平 General speech recognition Human level 4%和rearview mirror speech data 6%：说明后视镜的语音数据比一般语音识别更难，因为人类都有6%的错误，而不是4%的错误 总结： 开发集、测试集不同分布： 可以提供更多训练数据，有助于提高学习算法的性能 潜在问题不只是偏差和方差问题，还有数据不匹配 2.6 定位数据不匹配（Addressing data mismatch）解决train set与dev/test set样本分布不一致的两条建议： 为了让训练数据更接近开发集，可以人工合成数据（artificial data synthesis）。例如说话人识别问题，实际应用场合（dev/test set）是包含背景噪声的，而训练样本train set很可能没有背景噪声。为了让train set与dev/test set分布一致，可以在train set上人工添加背景噪声，合成类似实际场景的声音。这样会让模型训练的效果更准确。但是不能给每段语音都增加同一段背景噪声，会出现对背景噪音过拟合，这就是人工数据合成需要注意的地方 研发无人驾驶汽车，用计算机合成图像 如果只合成这些车中很小的子集，学习算法可能会对合成的这一个小子集过拟合 2.7 迁移学习（Transfer learning）将已经训练好的模型的一部分知识（网络结构）直接应用到另一个类似模型中去。比如已经训练好一个猫类识别的神经网络模型，直接把该模型中的一部分网络结构应用到使用X光片预测疾病的模型中去，这种学习方法被称为迁移学习（Transfer Learning） 如果已经有一个训练好的神经网络用来做图像识别。想要构建另一个X光片进行诊断的模型。迁移学习的做法是无需重新构建新的模型，而是利用之前的神经网络模型，只改变样本输入、输出以及输出层的权重系数W^{[L]},\ b^{[L]}，即对新的样本(X,Y)，重新训练输出层权重系数W^{[L]},\ b^{[L]}，其它层所有的权重系数W^{[L]},\ b^{[L]}保持不变 如果需要构建新模型的样本数量较少，可以只训练输出层的权重系数W^{[L]},\ b^{[L]}，保持其它层所有的权重系数W^{[l]},\ b^{[l]}不变 如果样本数量足够多，可以只保留网络结构，重新训练所有层的权重系数。这种做法使得模型更加精确，因为样本对模型的影响最大 择哪种方法通常由数据量决定 如果重新训练所有权重系数，初始W^{[l]},\ b^{[l]}由之前的模型训练得到，这一过程称为pre-training。之后，不断调试、优化W^{[l]},\ b^{[l]}的过程称为fine-tuning。pre-training和fine-tuning分别对应上图中的黑色箭头和红色箭头 迁移学习能这么做的原因是神经网络浅层部分能够检测出许多图片固有特征，例如图像边缘、曲线等。使用之前训练好的神经网络部分结果有助于更快更准确地提取X光片特征。二者处理的都是图片，而图片处理是有相同的地方，第一个训练好的神经网络已经实现如何提取图片有用特征。即便是即将训练的第二个神经网络样本数目少，仍然可以根据第一个神经网络结构和权重系数得到健壮性好的模型 迁移学习可以保留原神经网络的一部分，再添加新的网络层，可以去掉输出层后再增加额外一些神经层 迁移学习的应用场合主要包括三点： Task A and B have the same input x. You have a lot more data for Task A than Task B. Low level features from A could be helpful for learning B. 2.8 多任务学习（Multi-task learning）在迁移学习中，步骤是串行的，从任务A里学习然后只是迁移到任务B。在多任务学习中是同时开始学习的，试图让单个神经网络同时做几件事情，希望每个任务都能帮到其他所有任务 假设无人驾驶需要同时检测行人、车辆、停车标志，还有交通灯各种其他东西 如果输入图像x^{(i)}，那么 y^{(i)}不再是一个标签，而是有4个标签。在这个例子中，没有行人，有一辆车，有一个停车标志，没有交通灯。所以 y^{(i)}是个4×1向量。将训练集的标签水平堆叠起来，从y^{(1)}一直到y^{(m)}： \begin{bmatrix} \vdots & \vdots & \vdots & \vdots & \vdots\\ y^{(1)} & y^{(2)} & y^{(3)} & \cdots & y^{(m)}\\ \vdots & \vdots & \vdots & \vdots & \vdots \end{bmatrix}矩阵Y变成4\times m矩阵 输出四个节点，第一个节点是预测图中有没有行人，第二个预测有没有车，第三预测有没有停车标志，第四预测有没有交通灯，所以\hat y是四维 整个训练集的平均损失： \frac{1}{m}\sum_{i = 1}^{m}{\sum_{j = 1}^{4}{L(\hat y_{j}^{(i)},y_{j}^{(i)})}}$\sum_{j = 1}^{4}{L(\hat y_{j}^{(i)},y_{j}^{(i)})}$是单个预测的损失，所以这是对四个分量的求和，行人、车、停车标志、交通灯，标志L指的是logistic损失： L(\hat y_{j}^{(i)},y_{j}^{(i)}) = - y_{j}^{(i)}\log\hat y_{j}^{(i)} - (1 - y_{j}^{(i)})log(1 - \hat y_{j}^{(i)})Multi-task learning与Softmax regression的区别在于： Multi-task learning是multiple labels的，即输出向量y可以有多个元素为1 Softmax regression是single label的，即输出向量y只有一个元素为1 神经网络一些早期特征，在识别不同物体时都会用到，训练一个神经网络做四件事情会比训练四个完全独立的神经网络分别做四件事性能要更好 多任务学习也可以处理图像只有部分物体被标记的情况。比如没有标记是否有停车标志，或者是否有交通灯。也许有些样本都有标记，有些样本只标记了有没有车，然后还有一些是问号 即使是这样的数据集，也可以在上面训练算法，同时做四个任务，即使一些图像只有一小部分标签，其他是问号。训练算法的方式是对j从1到4只对带0和1标签的j值求和，当有问号就在求和时忽略那个项 多任务学习当三件事为真时有意义的： 训练的一组任务，可以共用低层次特征。对于无人驾驶的例子，同时识别交通灯、汽车和行人是有道理的，这些物体有相似的特征 如果每个任务的数据量很接近，这个准则没那么绝对，不一定对 想要从多任务学习得到很大性能提升，其他任务加起来必须要有比单个任务大得多的数据量 多任务学习会降低性能的唯一情况是神经网络还不够大。但如果可以训练一个足够大的神经网络，多任务学习肯定不会或者很少会降低性能 在实践中，多任务学习的使用频率要低于迁移学习。因为很难找到那么多相似且数据量对等的任务可以用单一神经网络训练。不过在计算机视觉领域，物体检测这个例子是最显著的例外情况 2.9 什么是端到端的深度学习？（What is end-to-end deep learning?）以前有一些数据处理系统或者学习系统需要多个阶段的处理。端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它 语音识别目标是输入x，比如说一段音频，然后把它映射到一个输出y，就是这段音频的听写文本: 传统上语音识别需要很多阶段的处理。首先提取一些特征，一些手工设计的音频特征，比如MFCC，这种算法是用来从音频中提取一组特定的人工设计的特征。在提取出一些低层次特征之后，应用机器学习算法在音频片段中找到音位（声音的基本单位），比如“Cat”这个词是三个音节构成的，Cu-、Ah-和Tu-，算法把这三个音位提取出来，然后将音位串在一起构成独立的词，然后将词串起来构成音频片段的听写文本 ) 端到端深度学习是训练一个巨大的神经网络，输入一段音频，输出直接是听写文本。只需要把训练集拿过来，直接学到了x和y之间的函数映射，绕过了其中很多步骤 端到端深度学习的挑战之一是需要大量数据才能让系统表现良好，比如只有3000小时数据去训练语音识别系统，那传统的流水线效果很好。但当有非常大的数据集时，比如10,000小时数据或者100,000小时数据，端到端方法突然开始很厉害。所以当数据集较小时，传统流水线方法效果更好。如果数据量适中，也可以用中间件方法，如输入还是音频，然后绕过特征提取，直接尝试从神经网络输出音位 门禁识别系统 最好的方法是一个多步方法，首先运行一个软件来检测人脸，然后放大图像并裁剪图像，使人脸居中显示，然后红线框起来的照片再喂到神经网络里，让网络去学习，或估计那人的身份 比起一步到位，一步学习，把这个问题分解成两个更简单的步骤更好： 首先弄清楚脸在哪里 第二步是看着脸，弄清楚这是谁 这种方法让两个学习算法分别解决两个更简单的任务，并在整体上得到更好的表现 训练第二步的方式：输入两张图片，网络将两张图比较一下，判断是否是同一个人。比如记录了10,000个员工ID，可以把红色框起来的图像快速比较，看看红线内的照片是不是那10000个员工之一，判断是否应该允许其进入 为什么两步法更好： 解决的两个问题，每个问题实际上要简单得多 两个子任务的训练数据都很多 机器翻译 传统上机器翻译系统也有一个很复杂的流水线，比如英语机翻得到文本，然后做文本分析，基本上要从文本中提取一些特征之类的，经过很多步骤，最后会将英文文本翻译成法文。因为对于机器翻译来说有很多(英文,法文)的数据对，端到端深度学习在机器翻译领域非常好用 2.10 是否要使用端到端的深度学习？（Whether to use end-to-end learning?）端到端学习的优点 端到端学习只是让数据说话。如果有足够多的(x,y)数据，不管从x到y最适合的函数映射是什么，如果训练一个足够大的神经网络，希望这个神经网络能自己搞清楚。使用纯机器学习方法，直接从x到y输入去训练神经网络，可能更能够捕获数据中的任何统计信息，而不是被迫引入人类的成见。例如在语音识别领域，早期的识别系统有这个音位概念，如果让学习算法学习它想学习的任意表示方式，而不是强迫使用音位作为表示方式，其整体表现可能会更好 所需手工设计的组件更少，能够简化设计工作流程，不需要花太多时间去手工设计功能，手工设计中间表示方式 端到端学习的缺点 直接学到x到y的映射，需要大量(x,y)数据 排除了可能有用的手工设计组件。当有大量数据时，手工设计不太重要，当没有太多的数据时，构造一个精心设计的系统，可以将人类对这个问题的很多认识直接注入到问题里，对算法很有帮助 端到端深度学习的弊端之一是它把可能有用的人工设计的组件排除在外，精心设计的人工组件可能非常有用，但也可能真的影响算法表现。例如，强制算法以音位为单位思考，也许让算法自己找到更好的表示方法更好。但往往好处更多，手工设计的组件往往在训练集更小的时候帮助更大 决定是否使用端到端深度学习，关键的问题是是否有足够的数据能够直接学到从x映射到y足够复杂的函数。识别图中骨头位置是相对简单的问题，系统不需要那么多数据。但把手的X射线照片直接映射到孩子的年龄，直接去找这种函数，就是更为复杂的问题。如果用纯端到端方法，需要很多数据去学习]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第 三 周 超 参 数 调 试 、 Batch 正 则 化 和 程 序 框 架 （Hyperparameter tuning）(Course 2)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC-%E4%B8%89-%E5%91%A8-%E8%B6%85-%E5%8F%82-%E6%95%B0-%E8%B0%83-%E8%AF%95-%E3%80%81-Batch-%E6%AD%A3-%E5%88%99-%E5%8C%96-%E5%92%8C-%E7%A8%8B-%E5%BA%8F-%E6%A1%86-%E6%9E%B6-%EF%BC%88Hyperparameter-tuning%EF%BC%89-Course-2%2F</url>
    <content type="text"><![CDATA[3.1 调试处理（Tuning process）深度神经网络需要调试的超参数（Hyperparameters）包括： \alpha：学习因子 \beta：动量梯度下降因子 \beta_1,\beta_2,\varepsilon：Adam算法参数 #layers：神经网络层数 #hidden units：各隐藏层神经元个数 learning rate decay：学习因子下降参数 mini-batch size：批量训练样本包含的样本个数 学习因子\alpha是最重要的超参数，也是需要重点调试的超参数。动量梯度下降因子\beta、各隐藏层神经元个数#hidden units和mini-batch size的重要性仅次于,然后就是神经网络层数#layers和学习因子下降参数learning rate decay。最后，Adam算法的三个参数\beta_1,\beta_2,\varepsilon一般常设置为0.9，0.999和10^{-8} 传统的机器学习中，对每个参数等距离选取任意个数的点，分别使用不同点对应的参数组合进行训练，最后根据验证集上的表现好坏来选定最佳的参数。例如有两个待调试的参数，分别在每个参数上选取5个点，这样构成了5x5=25中参数组合： 这种做法在参数比较少的时候效果较好 深度神经网络模型中是使用随机选择。随机选择25个点，作为待调试的超参数： ) 随机化选择参数是为了尽可能地得到更多种参数组合。如果使用均匀采样，每个参数只有5种情况；而使用随机采样的话，每个参数有25种可能的情况，更可能得到最佳的参数组合 另外一个好处是对重要性不同的参数之间的选择效果更好。假设hyperparameter1为\alpha，hyperparameter2为\varepsilon，显然二者的重要性是不一样的。如果使用第一种均匀采样的方法，\varepsilon的影响很小，相当于只选择了5个\alpha值。而如果使用第二种随机采样的方法，\varepsilon和\alpha都有可能选择25种不同值。这大大增加了\alpha调试的个数，更有可能选择到最优值 在实际应用中完全不知道哪个参数更加重要的情况下，随机采样的方式能有效解决这一问题，但是均匀采样做不到这点 随机采样之后，可能得到某些区域模型的表现较好。为了得到更精确的最佳参数，继续对选定的区域进行由粗到细的采样（coarse to fine sampling scheme）。就是放大表现较好的区域，对此区域做更密集的随机采样 如对下图中右下角的方形区域再做25点的随机采样，以获得最佳参数： 3.2 为超参数选择合适的范围（Using an appropriate scale to pick hyperparameters）随机取值并不是在有效范围内的随机均匀取值，而是选择合适的标尺，用于探究这些超参数 对于超参数#layers和#hidden units，都是正整数，是可以进行均匀随机采样的，即超参数每次变化的尺度都是一致 对于某些超参数，可能需要非均匀随机采样（即非均匀刻度尺）。例如超参数\alpha，待调范围是[0.0001, 1]。如果使用均匀随机采样，90%的采样点分布在[0.1, 1]之间，只有10%分布在[0.0001, 0.1]之间。而最佳的\alpha值可能主要分布在[0.0001, 0.1]之间，因此更应在区间[0.0001, 0.1]内细分更多刻度 通常的做法是将linear scale转换为log scale，将均匀尺度转化为非均匀尺度，然后再在log scale下进行均匀采样。这样，[0.0001, 0.001]，[0.001, 0.01]，[0.01, 0.1]，[0.1, 1]各个区间内随机采样的超参数个数基本一致，扩大了之前[0.0001, 0.1]区间内采样值个数 如果线性区间为[a, b]，令m=log(a)，n=log(b)，则对应的log区间为[m,n]。对log区间的[m,n]进行随机均匀采样，得到的采样值r，最后反推到线性区间，即10^r.10^r是最终采样的超参数。代码为： 12345m = np.log10(a)n = np.log10(b)r = np.random.rand()r = m + (n-m)*rr = np.power(10,r) 动量梯度因子\beta在超参数调试也需要进行非均匀采样。一般\beta的取值范围在[0.9, 0.999]之间，1−\beta的取值范围在[0.001, 0.1]。那么直接对1−\beta在[0.001, 0.1]区间内进行log变换 为什么\beta也需要向\alpha那样做非均匀采样： 假设\beta从0.9000变化为0.9005，那么\frac{1}{1-\beta}基本没有变化。但假设β从0.9990变化为0.9995，那么\frac{1}{1-\beta}前后差别1000。\beta越接近1，指数加权平均的个数越多，变化越大。所以对\beta接近1的区间，应该采集得更密集一些 3.3 超参数训练的实践： Pandas VS Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）经过调试选择完最佳的超参数不是一成不变的，一段时间之后（例如一个月），需要根据新的数据和实际情况，再次调试超参数，以获得实时的最佳模型 在训练深度神经网络时，一种情况是有庞大的数据组，但没有许多计算资源或足够的 CPU 和GPU 的前提下，只能对一个模型进行训练，调试不同的超参数，使得这个模型有最佳的表现。称之为Babysitting one model。另外一种情况是可以对多个模型同时进行训练，每个模型上调试不同的超参数，根据表现情况，选择最佳的模型。称之为Training many models in parallel 第一种情况只使用一个模型，类比做Panda approach；第二种情况同时训练多个模型，类比做Caviar approach。使用哪种模型是由计算资源、计算能力所决定的。一般来说，对于非常复杂或者数据量很大的模型，使用Panda approach更多一些 3.4 归一化网络的激活函数（ Normalizing activations in a network）在神经网络中，第l层隐藏层的输入就是第l-1层隐藏层的输出A^{[l-1]}。对A^{[l-1]}进行标准化处理，从原理上来说可以提高W^{[l]}和b^{[l]}的训练速度和准确度。这种对各隐藏层的标准化处理就是Batch Normalization。一般是对Z^{[l-1]}进行标准化处理而不是A^{[l-1]} Batch Normalization对第l层隐藏层的输入Z^{[l-1]}做如下标准化处理，忽略上标[l-1]： \mu=\frac1m\sum_iz^{(i)} \sigma^2=\frac1m\sum_i(z_i-\mu)^2 z^{(i)}_{norm}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\varepsilon}}m是单个mini-batch包含样本个数，ε是为了防止分母为零，可取值10^{-8}。使得该隐藏层的所有输入z^{(i)}均值为0，方差为1 大部分情况下并不希望所有的z^{(i)}均值都为0，方差都为1，也不太合理。通常需要对z^{(i)}进行进一步处理： \tilde z^{(i)}=\gamma\cdot z^{(i)}_{norm}+\beta$\gamma$和\beta是learnable parameters，可以通过梯度下降等算法求得。\gamma和\beta是让\tilde z^{(i)}的均值和方差为任意值，只需调整其值。如： \gamma=\sqrt{\sigma^2+\varepsilon},\ \ \beta=u则\tilde z^{(i)}=z^{(i)}，即identity function。设置\gamma和\beta为不同的值，可以得到任意的均值和方差 通过Batch Normalization，对隐藏层的各个z^{[l](i)}进行标准化处理，得到\tilde z^{[l](i)}，替代z^{[l](i)} 输入的标准化处理Normalizing inputs和隐藏层的标准化处理Batch Normalization是有区别的。Normalizing inputs使所有输入的均值为0，方差为1。而Batch Normalization可使各隐藏层输入的均值和方差为任意值。从激活函数的角度来说，如果各隐藏层的输入均值在靠近0的区域即处于激活函数的线性区域，这样不利于训练好的非线性神经网络，得到的模型效果也不会太好 3.5 将 Batch Norm 拟合进神经网络（Fitting Batch Norm into a neural network）前向传播的计算流程： 实现梯度下降： for t = 1 … num （这里num 为Mini Batch 的数量）： 在每一个X^t 上进行前向传播（forward prop）的计算： 在每个隐藏层都用 Batch Norm 将z^{[l]}替换为 \widetilde{z}^{[l]} 使用反向传播（Back prop）计算各个参数的梯度：dw^{[l]},d\gamma^{[l]},d\beta^{[l]} 更新参数： w^{[l]}:=w^{[l]}-\alpha dw^{[l]} \gamma^{[l]}:=\gamma^{[l]}-\alpha d\gamma^{[l]} \beta^{[l]}:=\beta^{[l]}-\alpha d\beta^{[l]} 经过Batch Norm的作用，整体流程如下： ) Batch Norm对各隐藏层Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}有去均值的操作，Batch Norm 要做的就是将z^{[l]}归一化，结果成为均值为0，标准差为1的分布，再由\beta 和\gamma 进行重新的分布缩放，意味着无论b^{[l]} 值为多少，在这个过程中都会被减去，不会再起作用。所以常数项b^{[l]}可以消去，其数值效果完全可以由\widetilde{z}^{[l]}中的\beta来实现。在使用Batch Norm的时候，可以忽略各隐藏层的常数项b^{[l]}。在使用梯度下降算法时，分别对W^{[l]},\beta^{[l]}和\gamma^{[l]}进行迭代更新 除了传统的梯度下降算法之外，还可以使用动量梯度下降、RMSprop或者Adam等优化算法 3.6 Batch Norm 为什么奏效？（Why does Batch Norm work?）Batch Norm 可以加速神经网络训练的原因： 和输入层的输入特征进行归一化，从而改变Cost function的形状，使得每一次梯度下降都可以更快的接近函数的最小值点，从而加速模型训练过程的原理有相同的道理，只是Batch Norm是将各个隐藏层的激活函数的激活值进行的归一化，并调整到另外的分布 Batch Norm 可以使权重比网络更滞后或者更深层 判别是否是猫的分类问题：假设第一训练样本的集合中的猫均是黑猫，而第二个训练样本集合中的猫是各种颜色的猫。如果将第二个训练样本直接输入到用第一个训练样本集合训练出的模型进行分类判别，在很大程度上无法保证能够得到很好的判别结果 因为训练样本不具有一般性（即不是所有的猫都是黑猫），第一个训练集合中均是黑猫，而第二个训练集合中各色猫均有，虽然都是猫，但是很大程度上样本的分布情况是不同的，无法保证模型可以仅仅通过黑色猫的样本就可以完美的找到完整的决策边界 这种训练样本（黑猫）和测试样本（猫）分布的变化称之为covariate shift。如下图所示： 深度神经网络中，covariate shift会导致模型预测效果变差，重新训练的模型各隐藏层的W^{[l]}和B^{[l]}均产生偏移、变化。而Batch Norm的作用恰恰是减小covariate shift的影响，让模型变得更加健壮，鲁棒性更强 使用深层神经网络，使用Batch Norm，该模型对花猫的识别能力应该也是不错 Batch Norm 解决Covariate shift的问题) 网络的目的是通过不断的训练，最后输出一个更加接近于真实值的\hat y，以第2个隐藏层为输入来看： 对于后面的神经网络，是以第二层隐层的输出值a^{[2]}作为输入特征的，通过前向传播得到最终的\hat y，但是网络还有前面两层，由于训练过程，参数w^{[1]},w^{[2]}是不断变化的，对于后面的网络，a^{[2]}的值也是处于不断变化之中，所以就有了Covariate shift的问题 如果对z^{[2]}使用了Batch Norm，即使其值不断的变化，其均值和方差却会保持。Batch Norm的作用是限制前层的参数更新导致对后面网络数值分布程度的影响，使得输入后层的数值变得更加稳定。Batch Norm减少了各层W^{[l]},B^{[l]}之间的耦合性，让各层更加独立，实现自我训练学习的效果。如果输入发生covariate shift，Batch Norm的作用是对个隐藏层输出Z^{[l]}进行均值和方差的归一化处理，让W^{[l]},B^{[l]}更加稳定，使得原来的模型也有不错的表现 Batch Norm 削弱了前层参数与后层参数之间的联系，使得网络的每层都可以自己进行学习，相对其他层有一定的独立性，有助于加速整个网络的学习 Batch Norm 正则化效果 使用Mini-batch梯度下降，每次计算均值和偏差都是在一个Mini-batch上进行计算，而不是在整个数据样集上。这样在均值和偏差上带来一些比较小的噪声。那么用均值和偏差计算得到的\widetilde{z}^{[l]}也将会加入一定的噪声 和Dropout相似，其在每个隐藏层的激活值上加入了一些噪声，（Dropout以一定的概率给神经元乘上0或者1）。Batch Norm 也有轻微的正则化效果 如果使用Batch Norm ，使用大的Mini-batch如256，相比使用小的Mini-batch如64，会引入更少的噪声，会减少正则化的效果 Batch Norm的正则化效果比较微弱，正则化不是Batch Norm的主要功能 3.7 测试时的 Batch Norm（Batch Norm at test time）训练过程中Batch Norm的主要过程： \mu=\frac1m\sum_iz^{(i)} \sigma^2=\frac1m\sum_i(z^{(i)}-\mu)^2 z_{norm}^{(i)}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\varepsilon}} \tilde z^{(i)}=\gamma\cdot z^{(i)}_{norm}+\beta$\mu$和\sigma^2是对单个mini-batch中所有m个样本求得的。在测试过程中，如果只有一个样本，求其均值和方差是没有意义的，就需要对\mu和\sigma^2进行估计。实际应用是使用指数加权平均（exponentially weighted average）的方法来预测测试过程单个样本的\mu和\sigma^2 对于第l层隐藏层，在训练的过程中, ，对于训练集的Mini-batch，考虑所有mini-batch在该隐藏层下的\mu^{[l]}和{\sigma^{2}}^{[l]}，使用指数加权平均，当训练结束的时候，得到指数加权平均后当前单个样本的\mu^{[l]}和{\sigma^{2}}^{[l]},这些值直接用于Batch Norm公式的计算，用以对测试样本进行预测，再利用训练过程得到的\gamma和\beta计算出各层的\tilde z^{(i)}值 3.8 Softmax 回归（Softmax regression）Softmax 回归，能在识别多种分类中的一个时做出预测，对于多分类问题，用C表示种类个数，神经网络中输出层就有C个神经元，即n^{[L]}=C，每个神经元的输出依次对应属于该类的概率，即P(y=c|x)，处理多分类问题一般使用Softmax回归模型 把猫做类 1，狗为类 2，小鸡 是类 3， 如果不属于以上任何一类， 就分到“其它”或者“以上均不符合”这一类，叫 做类 0 用大写C表示输入会被分入的类别总个数,当有 4 个分类时，指示类别的数字，就是从 0 到C − 1( 0、 1、 2、 3) Softmax回归模型输出层的激活函数： z^{[L]}=W^{[L]}a^{[L-1]}+b^{[L]} a^{[L]}_i=\frac{e^{z^{[L]}_i}}{\sum_{i=1}^Ce^{z^{[L]}_i}}输出层每个神经元的输出a^{[L]}_i对应属于该类的概率，满足： \sum_{i=1}^Ca^{[L]}_i=1所有的a^{[L]}_i，即\hat y，维度为(C, 1) 在没有隐藏隐藏层的时候，直接对Softmax层输入样本的特点，则在不同数量的类别下，Sotfmax层的作用： 图中的颜色显示了 Softmax 分类器的输出的阈值，输入的着色是基于三种输出中概率最高的那种，任何两个分类之间的决策边界都是线性的 如果使用神经网络，特别是深层神经网络，可以得到更复杂、更精确的非线性模型 3.9 训练一个 Softmax 分类器（Training a Softmax classifier）C=4，某个样本的预测输出\hat y和真实输出y： \hat y=\left[ \begin{matrix} 0.3 \\ 0.2 \\ 0.1 \\ 0.4 \end{matrix} \right] y=\left[ \begin{matrix} 0 \\ 1 \\ 0 \\ 0 \end{matrix} \right]从\hat y值来看，P(y=4|x)=0.4，概率最大，而真实样本属于第2类，该预测效果不佳 定义softmax classifier的loss function为： L(\hat y,y)=-\sum_{j=1}^4y_j\cdot log\ \hat y_j$L(\hat y,y)$简化为： L(\hat y,y)=-y_2\cdot log\ \hat y_2=-log\ \hat y_2让L(\hat y,y)更小，就应该让\hat y_2越大越好。\hat y_2反映的是概率 m个样本的cost function为： J=\frac{1}{m}\sum_{i=1}^mL(\hat y,y)预测输出向量A^{[L]}即\hat Y的维度为(4, m) softmax classifier的反向传播过程: 先推导dZ^{[L]}： da^{[L]}=-\frac{1}{a^{[L]}} \frac{\partial a^{[L]}}{\partial z^{[L]}}=\frac{\partial}{\partial z^{[L]}}\cdot (\frac{e^{z^{[L]}_i}}{\sum_{i=1}^Ce^{z^{[L]}_i}})=a^{[L]}\cdot (1-a^{[L]}) 所有m个训练样本： dZ^{[L]}=A^{[L]}-Y]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第四周：深层神经网络(Deep Neural Networks)(Course 1)]]></title>
    <url>%2F2019%2F02%2F28%2F%E7%AC%AC%E5%9B%9B%E5%91%A8%EF%BC%9A%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Deep-Neural-Networks%2F</url>
    <content type="text"><![CDATA[4.1 深层神经网络（Deep L-layer neural network） $L-layer\quad NN$，则包含了L-1个隐藏层，最后的L层是输出层 $a^{[l]}$和W^{[l]}中的上标l都是从1开始的，l=1,\cdots,L 输入x记为a^{[0]}​​，把输出层\hat y记为a^{[L]} $X$：(12288, 209)(with m=209 examples) Shape of W Shape of b Activation Shape of Activation Layer 1 (n^{[1]},12288) (n^{[1]},1) Z^{[1]} = W^{[1]} X + b^{[1]} (n^{[1]},209) Layer 2 (n^{[2]}, n^{[1]}) (n^{[2]},1) Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} (n^{[2]},209) \vdots \vdots \vdots \vdots \vdots Layer L-1 (n^{[L-1]}, n^{[L-2]}) (n^{[L-1]}, 1) Z^{[L-1]} = W^{[L-1]} A^{[L-2]} + b^{[L-1]} (n^{[L-1]}, 209) Layer L (n^{[L]}, n^{[L-1]}) (n^{[L]}, 1) Z^{[L]} = W^{[L]} A^{[L-1]} + b^{[L]} (n^{[L]}, 209) 4.2 前向传播和反向传播（Forward and backward propagation）正向传播过程 z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]} a^{[l]}=g^{[l]}(z^{[l]})$m$个训练样本，向量化形式为： Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} A^{[l]}=g^{[l]}(Z^{[l]})反向传播过程 dz^{[l]}=da^{[l]}\ast g^{[l]'}(z^{[l]}) dW^{[l]}=dz^{[l]}\cdot {a^{[l-1]}}^T db^{[l]}=dz^{[l]} da^{[l-1]}=W^{[l]T}\cdot dz^{[l]}得到： dz^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}\ast g^{[l]'}(z^{[l]})$m$个训练样本，向量化形式为： dZ^{[l]}=dA^{[l]}\ast g^{[l]'}(Z^{[l]}) dW^{[l]}=\frac1mdZ^{[l]}\cdot A^{[l-1]T} db^{[l]}=\frac1mnp.sum(dZ^{[l]},axis=1,keepdim=True) dA^{[l-1]}=W^{[l]T}\cdot dZ^{[l]} dZ^{[l]}=W^{[l+1]T}\cdot dZ^{[l+1]}\ast g^{[l]'}(Z^{[l]}) 4.3 深层网络中的前向传播（Forward propagation in a Deep Network ）对于第l层，其正向传播过程的Z^{[l]}和A^{[l]}可以表示为： Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} A^{[l]}=g^{[l]}(Z^{[l]})其中l=1,\cdots,L 4.4 为什么使用深层表示？（Why deep representations?）人脸识别经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。 随着层数由浅到深，神经网络提取的特征也是从边缘到局部特征到整体，由简单到复杂。如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。 语音识别模型浅层的神经元能够检测一些简单的音调，较深的神经元能够检测出基本的音素，更深的神经元就能够检测出单词信息。如果网络够深，还能对短语、句子进行检测。 神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大 深层网络另外一个优点:减少神经元个数，从而减少计算量 使用电路理论，计算逻辑输出： y=x_1\oplus x_2\oplus x_3\oplus\cdots\oplus x_n对于这个逻辑运算，深度网络的结构是每层将前一层的两两单元进行异或，最后得到一个输出 整个深度网络的层数是log_2(n)，不包含输入层。总共使用的神经元个数为： 1+2+\cdots+2^{log_2(n)-1}=1\cdot\frac{1-2^{log_2(n)}}{1-2}=2^{log_2(n)}-1=n-1输入个数是n，这种深层网络所需的神经元个数仅仅是n-1个 如果不用深层网络，使用单个隐藏层，需要的神经元个数将是指数级别那么大。由于包含了所有的逻辑位（0和1），则需要2^{n-1}个神经元 处理同一逻辑问题，深层网络所需的神经元个数比浅层网络要少很多 4.5 搭建神经网络块（Building blocks of deep neural networks）第l层的流程块图 对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示： 4.6 参数 VS 超参数（Parameters vs Hyperparameters）神经网络中的参数是W^{[l]}和b^{[l]} 超参数则是例如学习速率\alpha，训练迭代次数N，神经网络层数L，各层神经元个数n^{[l]}，激活函数g(z)等 叫做超参数的原因是它们决定了参数W^{[l]}和b^{[l]}的值 如何设置最优的超参数： 通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周 机器学习策略（1）（ML strategy（1））(Course 3)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%B8%80%E5%91%A8-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5%EF%BC%881%EF%BC%89%EF%BC%88ML-strategy%EF%BC%881%EF%BC%89%EF%BC%89-Course-3%2F</url>
    <content type="text"><![CDATA[1.1 为什么是 ML 策略？（Why ML Strategy?）当最初得到一个深度神经网络模型时，希望从很多方面来对它进行优化，例如： Collect more data Collect more diverse training set Train algorithm longer with gradient descent Try Adam instead of gradient descent Try bigger network Try smaller network Try dropout Add L2 regularization Network architecture: Activation functions, #hidden units… 可选择的方法很多、很复杂、繁琐。盲目选择、尝试不仅耗费时间而且可能收效甚微。因此，使用快速、有效的策略来优化机器学习模型是非常必要的。 1.2 正交化（Orthogonalization）每次只调试一个参数，保持其它参数不变，使得到的模型某一性能改变是一种最常用的调参策略，称之为正交化方法（Orthogonalization） Orthogonalization的核心在于每次调试一个参数只会影响模型的某一个性能 机器学习监督式学习模型大致分成四个独立的“功能”： Fit training set well on cost function ，优化训练集可以通过使用更复杂NN，使用Adam等优化算法来实现 Fit dev set well on cost function，优化验证集可以通过正则化，采用更多训练样本来实现 Fit test set well on cost function，优化测试集可以通过使用更多的验证集样本来实现 Performs well in real world，提升实际应用模型可以通过更换验证集，使用新的cost function来实现 每一种“功能”对应不同的调节方法，是正交的 early stopping在模型功能调试中并不推荐使用。因为early stopping在提升验证集性能的同时降低了训练集的性能。即early stopping同时影响两个“功能”，不具有独立性、正交性 1.3 单一数字评估指标（Single number evaluation metric）A和B模型的准确率（Precision）和召回率（Recall）分别如下： 使用单值评价指标F1 Score来评价模型的好坏。F1 Score综合了Precision和Recall的大小： F1=\frac{2\cdot P\cdot R}{P+R} 还可以使用平均值作为单值评价指标： 不同国家样本的错误率，计算平均性能，选择平均错误率最小的模型（C模型） 1.4 满足和优化指标（Satisficing and optimizing metrics）当把所有的性能指标都综合在一起，构成单值评价指标比较困难时：可以把某些性能作为优化指标（Optimizing metic），寻求最优化值；而某些性能作为满意指标（Satisficing metic），只要满足阈值就行 Accuracy和Running time这两个性能不太合适综合成单值评价指标。可以将Accuracy作为优化指标（Optimizing metic），Running time作为满意指标（Satisficing metic）。给Running time设定一个阈值，在其满足阈值的情况下，选择Accuracy最大的模型。如果设定Running time必须在100ms以内，模型C不满足阈值条件，剔除；模型B相比较模型A而言，Accuracy更高，性能更好 如果要考虑N个指标，则选择一个指标为优化指标，其他N-1个指标都是满足指标： N_{metric}:\left\{ \begin{array}{l} 1\qquad \qquad \qquad Optimizing\ metric\\ N_{metric}-1\qquad Satisificing\ metric \end{array} \right.性能指标（Optimizing metic）需要优化，越优越好；满意指标（Satisficing metic）只要满足设定的阈值 1.5 训练/开发/测试集划分（Train/dev/test distributions）训练、开发、测试集选择设置的一些规则和意见： 训练、开发、测试集的设置会对产品带来非常大的影响； 在选择开发集和测试集时要使二者来自同一分布，且从所有数据中随机选取； 所选择的开发集和测试集中的数据，要与未来想要或者能够得到的数据类似，即模型数据和未来数据要具有相似性； 设置的测试集只要足够大，使其能够在过拟合的系统中给出高方差的结果就可以，也许10000左右的数目足够； 设置开发集只要足够使其能够检测不同算法、不同模型之间的优劣差异就可以，百万大数据中1%的大小就足够； 尽量保证dev sets和test sets来源于同一分布且都反映了实际样本的情况。如果dev sets和test sets不来自同一分布，从dev sets上选择的“最佳”模型往往不能够在test sets上表现得很好。好比在dev sets上找到最接近一个靶的靶心的箭，但是test sets提供的靶心却远远偏离dev sets上的靶心，结果肯定无法射中test sets上的靶心位置 1.6 开发集和测试集的大小（Size of dev and test sets） 样本数量不多（小于一万）的时候，通常将Train/dev/test sets的比例设为60%/20%/20% 没有dev sets的情况下，Train/test sets的比例设为70%/30% 样本数量很大（百万级别）的时候，通常将相应的比例设为98%/1%/1%或者99%/1% dev sets数量的设置，遵循的准则是通过dev sets能够检测不同算法或模型的区别，以便选择出更好的模型 test sets数量的设置，遵循的准则是通过test sets能够反映出模型在实际中的表现 实际应用中，可能只有train/dev sets，而没有test sets。这种情况也是允许的，只要算法模型没有对dev sets过拟合。但条件允许的话，最好有test sets，实现无偏估计 1.7 什么时候该改变开发/测试集和指标？（When to change dev/test sets and metrics）算法模型的评价标准有时候需要根据实际情况进行动态调整，目的是让算法模型在实际应用中有更好的效果 example1假设有两个猫的图片的分类器： 评估指标：分类错误率 算法A：3%错误率 算法B：5%错误率 初始的评价标准是错误率，A更好一些。实际使用时发现算法A会通过一些色情图片，但是B没有。从用户的角度来说，更倾向选择B模型，虽然B的错误率高一些。这时候需要改变之前只使用错误率作为评价标准，考虑新的情况进行改变。如增加色情图片的权重，增加其代价 假设开始的评估指标如下： Error = \dfrac{1}{m_{dev}}\sum\limits_{i=1}^{m_{dev}}I\{y^{(i)}_{pred}\neq y^{(i)}\}该评估指标对色情图片和非色情图片一视同仁 修改的方法，在其中加入权重w^{(i)}： Error = \dfrac{1}{\sum w^{(i)}}\sum\limits_{i=1}^{m_{dev}} w^{(i)}I\{y^{(i)}_{pred}\neq y^{(i)}\} w^{(i)}=\begin{cases} 1, & x^{(i)}\ is\ non-porn\\ 10 \ or\ 100, & x^{(i)}\ is\ porn \end{cases}通过设置权重，当算法将色情图片分类为猫时，误差项会快速变大 概括来说，机器学习可分为两个过程： Define a metric to evaluate classifiers How to do well on this metric 第一步是找靶心，第二步是通过训练，射中靶心。但是在训练的过程中可能会根据实际情况改变算法模型的评价标准，进行动态调整,如果评估指标无法正确评估算法的排名，则需要重新定义一个新的评估指标 example2对example1中的两个不同的猫图片的分类器A和B： 实际情况是一直使用网上下载的高质量的图片进行训练；当部署到手机上时，由于图片的清晰度及拍照水平的原因，当实际测试算法时，会发现算法B的表现其实更好 如果在训练开发测试的过程中得到的模型效果比较好，但是在实际应用中所真正关心的问题效果却不好的时候，就需要改变开发、测试集或者评估指标 Guideline： 定义正确的评估指标来更好的给分类器的好坏进行排序 优化评估指标 1.8 为什么是人的表现？（ Why human-level performance?）机器学习模型的表现通常会跟人类水平表现作比较： 当开始往人类水平努力时，进展很快，机器学习模型经过训练会不断接近human-level performance甚至超过它。超过之后，准确性会上升得比较缓慢，当继续训练算法时，可能模型越来越大，数据越来越多，但是性能无法超过某个理论上限，这就是所谓的贝叶斯最优错误率（Bayes optimal error）。理论上任何模型都不能超过它，即没有任何办法设计出一个x到y的函数，让它能够超过一定的准确度，bayes optimal error代表了最佳表现 对于语音识别来说，如果x是音频片段，有些音频很嘈杂，基本不可能知道说的是什么，所以完美的准确率可能不是100%。对于猫图识别来说，也许一些图像非常模糊，不管是人类还是机器，都无法判断该图片中是否有猫。所以完美的准确度可能不是100 贝叶斯最优错误率有时写作Bayesian，即省略optimal，就是从x到y映射的理论最优函数，永远不会被超越。，无论在一个问题上工作多少年，紫色线永远不会超越贝叶斯错误率，贝叶斯最佳错误率 机器学习的进展直到超越人类的表现之前一直很快，当超越时，有时进展会变慢。有两个原因： 人类水平在很多任务中离贝叶斯最优错误率已经不远 只要表现比人类的表现更差，可以使用某些工具来提高性能。一旦超越了人类的表现，这些工具就没那么好用 只要人类的表现比任何其他算法都要好，就可以让人类看看算法处理的例子，知道错误出在哪里，并尝试了解为什么人能做对，算法做错 1.9 可避免偏差（Avoidable bias）猫分类器: 人类具有近乎完美的准确度，人类水平的错误是1%,如果学习算法达到8%的训练错误率和10%的开发错误率，算法在训练集上的表现和人类水平的表现有很大差距，说明算法对训练集的拟合并不好。从减少偏差和方差这个角度看，把重点放在减少偏差上。比如训练更大的神经网络，跑久一点梯度下降，试试能不能在训练集上做得更好 同样的训练错误率和开发错误率，假设人类水平错误实际上是7.5%，系统在训练集上的表现还好，只比人类的表现差一点。在第二个例子中，应专注减少学习算法的方差，可以试试正则化，让开发错误率更接近训练错误率 用人类水平的错误率估计或代替贝叶斯错误率或贝叶斯最优错误率，对于计算机视觉任务而言，这样替代相当合理，因为人类非常擅长计算机视觉任务，人类能做到的水平和贝叶斯错误率相差不远 左边的例子8%的训练错误率真的很高，可以把它降到1%，减少偏差的手段可能有效。右边的例子中，如果认为贝叶斯错误率是7.5%，这里使用人类水平错误率来替代贝叶斯错误率，就知道没有太多改善的空间了，不能继续减少训练错误率，训练误差和开发误差之间有更多的改进空间，可以将这个2%的差距缩小一点，使用减少方差的手段，比如正则化，或者收集更多的训练数据 贝叶斯错误率或者对贝叶斯错误率的估计和训练错误率之间的差值称为可避免偏差 理论上是不可能超过贝叶斯错误率的，除非过拟合 训练错误率和开发错误率之前的差值，说明算法在方差问题上还有多少改善空间 1.10 理解人的表现（Understanding human-level performance）医学图像识别的例子： 在减小误诊率的背景下，人类水平误差在这种情形下应定义为：0.5% error。但是实际应用中，不同人可能选择的human-level performance基准是不同的，这会带来一些影响 如果在为了部署系统或者做研究分析的背景下，也许超过一名普通医生即可，即人类水平误差在这种情形下应定义为：1% error 假如该模型training error为0.7%，dev error为0.8。如果选择Team of experienced doctors，即human-level error为0.5%，则bias比variance更加突出。如果选择Experienced doctor，即human-level error为0.7%，则variance更加突出。选择什么样的human-level error，有时候会影响bias和variance值的相对变化。当然这种情况一般只会在模型表现很好，接近bayes optimal error的时候出现。越接近bayes optimal error，模型越难继续优化，因为这时候的human-level performance可能是比较模糊难以准确定义的 1.11 超过人的表现（Surpassing human- level performance）对于自然感知类问题，例如视觉、听觉等，机器学习的表现不及人类。但是在很多其它方面，机器学习模型的表现已经超过人类了，包括： Online advertising Product recommendations Logistics(predicting transit time) Loan approvals 机器学习模型超过human-level performance是比较困难的。但是只要提供足够多的样本数据，训练复杂的神经网络，模型预测准确性会大大提高，很有可能接近甚至超过human-level performance。值得一提的是当算法模型的表现超过human-level performance时，很难再通过人的直觉来解决如何继续提高算法模型性能的问题 1.12 改善你的模型的表现（Improving your model performance）提高机器学习模型性能主要要解决两个问题：avoidable bias和variance。training error与human-level error之间的差值反映的是avoidable bias，dev error与training error之间的差值反映的是variance 基本假设： 模型在训练集上有很好的表现； 模型推广到开发和测试集啥也有很好的表现 减少可避免偏差 训练更大的模型 训练更长时间、训练更好的优化算法（Momentum、RMSprop、Adam） 寻找更好的网络架构（RNN、CNN）、寻找更好的超参数 减少方差 收集更多的数据 正则化（L2、dropout、数据增强） 寻找更好的网络架构（RNN、CNN）、寻找更好的超参数]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周：优化算法 (Optimization algorithms)(Course 2)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%9A%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-Optimization-algorithms-Course-2%2F</url>
    <content type="text"><![CDATA[2.1 Mini-batch 梯度下降（Mini-batch gradient descent）神经网络训练过程是同时对所有m个样本（称为batch）通过向量化计算方式进行的。如果m很大，训练速度会很慢，因为每次迭代都要对所有样本进进行求和运算和矩阵运算。这种梯度下降算法称为Batch Gradient Descent解决： 把m个训练样本分成若干个子集，称为mini-batches，然后每次在单一子集上进行神经网络训练，这种梯度下降算法叫做Mini-batch Gradient Descent 假设总的训练样本个数m=5000000，其维度为(n_x,m)。将其分成5000个子集，每个mini-batch含有1000个样本。将每个mini-batch记为X^{\{t\}}，其维度为(n_x,1000)。相应的每个mini-batch的输出记为Y^{\{t\}}，其维度为(1,1000)，且t=1,2,\cdots,5000 X^{(i)}：第i个样本 Z^{[l]}：神经网络第l层网络的线性输出 X^{\{t\}},Y^{\{t\}}：第t组mini-batch Mini-batches Gradient Descent是先将总的训练样本分成T个子集（mini-batches），然后对每个mini-batch进行神经网络训练，包括Forward Propagation，Compute Cost Function，Backward Propagation，循环至T个mini-batch都训练完毕 for\ \ t=1,\cdots,T\ \ \{ \ \ \ \ Forward\ Propagation \ \ \ \ Compute\ Cost\ Function \ \ \ \ Backward\ Propagation \ \ \ \ W:=W-\alpha\cdot dW \ \ \ \ b:=b-\alpha\cdot db \}经过T次循环之后，所有m个训练样本都进行了梯度下降计算。这个过程称之为经历了一个epoch。对于Batch Gradient Descent而言，一个epoch只进行一次梯度下降算法；而Mini-Batches Gradient Descent，一个epoch会进行T次梯度下降算法 对于Mini-Batches Gradient Descent，可以进行多次epoch训练。每次epoch，最好是将总体训练数据打乱、重新分成T组mini-batches，这样有利于训练出最佳的神经网络模型 2.2 理解 mini-batch 梯度下降法（Understanding mini-batch gradient descent）Batch gradient descent和Mini-batch gradient descent的cost曲线： 对于一般的神经网络模型，使用Batch gradient descent，随着迭代次数增加，cost是不断减小的。而使用Mini-batch gradient descent，随着在不同的mini-batch上迭代训练，其cost不是单调下降，而是受类似noise的影响，出现振荡。但整体的趋势是下降的，最终也能得到较低的cost值 出现细微振荡的原因是不同的mini-batch之间是有差异的。可能第一个子集(X^{\{1\}},Y^{\{1\}})是好的子集，而第二个子集(X^{\{2\}},Y^{\{2\}})包含了一些噪声noise。出现细微振荡是正常的 如果mini-batch size=m，即为Batch gradient descent，只包含一个子集为(X^{\{1\}},Y^{\{1\}})=(X,Y)； 如果mini-batch size=1，即为Stachastic gradient descent，每个样本就是一个子集(X^{\{1\}},Y^{\{1\}})=(x^{(i)},y^{(i)})，共有m个子集 蓝色的线代表Batch gradient descent，紫色的线代表Stachastic gradient descent。Batch gradient descent会比较平稳地接近全局最小值，但因为使用了所有m个样本，每次前进的速度有些慢。Stachastic gradient descent每次前进速度很快，但路线曲折，有较大的振荡，最终会在最小值附近来回波动，难达到最小值。而且在数值处理上不能使用向量化的方法来提高运算速度 mini-batch size不能设置得太大（Batch gradient descent），也不能设置得太小（Stachastic gradient descent）。相当于结合了Batch gradient descent和Stachastic gradient descent各自的优点，既能使用向量化优化算法，又能较快速地找到最小值。mini-batch gradient descent的梯度下降曲线如下图绿色所示，每次前进速度较快，且振荡较小，基本能接近全局最小值。 总体样本数量m不太大时，例如m\leq2000，建议直接使用Batch gradient descent 总体样本数量m很大时，建议将样本分成许多mini-batches。推荐常用的mini-batch size为64,128,256,512。都是2的幂。原因是计算机存储数据一般是2的幂，这样设置可以提高运算速度 mini-batch 中确保 X{\{t\}} 和Y{\{t\}}要符合 CPU/GPU 内存，取决于应用方向以及训练集的大小。如果处理的 mini-batch 和 CPU/GPU 内存不相符，不管用什么方法处理数据，算法的表现都急转直下变得惨不忍睹 从训练集（X，Y）中构建小批量 随机洗牌（Shuffle）：创建训练集（X，Y）的混洗版本，X和Y的每一列代表一个训练示例。随机混洗是在X和Y之间同步完成的。这样在混洗之后第i列的X对应的例子就是Y第i列中的标签。混洗步骤可确保将示例随机分成不同的小批次 分区（Partition）：将混洗（X，Y）分区为小批量mini_batch_size（此处为64）。训练示例的数量并非总是可以被mini_batch_size整除。最后一个小批量可能会更小 2.3 指数加权平均数（Exponentially weighted averages）半年内伦敦市的气温变化： 温度数据有noise，抖动较大 如果希望看到半年内气温的整体变化趋势，可以通过移动平均（moving average）的方法来对每天气温进行平滑处理 设V_0=0，当成第0天的气温值 第一天的气温与第0天的气温有关： V_1=0.9V_0+0.1\theta_1第二天的气温与第一天的气温有关： \begin{aligned}V_2 =&0.9V_1+0.1\theta_2\\ =&0.9(0.9V_0+0.1\theta_1)+0.1\theta_2\\ =&0.9^2V_0+0.9\cdot0.1\theta_1+0.1\theta_2 \end{aligned}第三天的气温与第二天的气温有关： \begin{aligned}V_3 =&0.9V_2+0.1\theta_3\\ =&0.9(0.9^2V_0+0.9\cdot0.1\theta_1+0.1\theta_2)+0.1\theta_3\\ =&0.9^3V_0+0.9^2\cdot 0.1\theta_1+0.9\cdot 0.1\theta_2+0.1\theta_3 \end{aligned}第t天与第t-1天的气温迭代关系为： \begin{aligned}V_t =&0.9V_{t-1}+0.1\theta_t\\ =&0.9^tV_0+0.9^{t-1}\cdot0.1\theta_1+0.9^{t-2}\cdot 0.1\theta_2+\cdots+0.9\cdot0.1\theta_{t-1}+0.1\theta_t \end{aligned}经过移动平均处理得到的气温如下图红色曲线所示： 这种滑动平均算法称为指数加权平均（exponentially weighted average）。一般形式为： V_t=\beta V_{t-1}+(1-\beta)\theta_t$\beta$值决定了指数加权平均的天数，近似表示为： \frac{1}{1-\beta}当\beta=0.9，则\frac{1}{1-\beta}=10，表示将前10天进行指数加权平均。当\beta=0.98，则\frac{1}{1-\beta}=50，表示将前50天进行指数加权平均。\beta值越大，则指数加权平均的天数越多，平均后的趋势线就越平缓，但是同时也会向右平移 绿色曲线和黄色曲线分别表示了\beta=0.98和\beta=0.5时，指数加权平均的结果 2.4 理解指数加权平均数（Understanding exponentially weighted averages ）指数加权平均公式的一般形式： \begin{aligned}V_t =&\beta V_{t-1}+(1-\beta)\theta_t\\ =&(1-\beta)\theta_t+(1-\beta)\cdot\beta\cdot\theta_{t-1}+(1-\beta)\cdot \beta^2\cdot\theta_{t-2}+\cdots +(1-\beta)\cdot \beta^{t-1}\cdot \theta_1+\beta^t\cdot V_0 \end{aligned}$\theta_t,\theta_{t-1},\theta_{t-2},\cdots,\theta_1$是原始数据值，(1-\beta),(1-\beta)\beta,(1-\beta)\beta^2,\cdots,(1-\beta)\beta^{t-1}是类似指数曲线，从右向左，呈指数下降的。V_t 的值是这两个子式的点乘，将原始数据值与衰减指数点乘，相当于做了指数衰减，离得越近，影响越大，离得越远，影响越小，衰减越厉害 为了减少内存的使用，使用这样的语句来实现指数加权平均算法： V_{\theta}=0 Repeat\ \{ \ \ \ \ Get\ next\ \theta_t \ \ \ \ V_{\theta}:=\beta V_{\theta}+(1-\beta)\theta_t \}2.5 指 数 加 权 平 均 的 偏 差 修 正 （ Bias correction inexponentially weighted averages ）当\beta=0.98时，指数加权平均结果如绿色曲线。但实际上真实曲线如紫色曲线 紫色曲线与绿色曲线的区别是，紫色曲线开始的时候相对较低一些。因为开始时设置V_0=0，所以初始值会相对小一些，直到后面受前面的影响渐渐变小，趋于正常 修正这种问题的方法是进行偏移校正（bias correction），即在每次计算完V_t后，对V_t进行下式处理： \frac{V_t}{1-\beta^t}刚开始的时候，t比较小，(1-\beta^t)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周：深度学习的实用层面(Practical aspects of Deep Learning)(Course 2)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%B8%80%E5%91%A8%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2-Practical-aspects-of-Deep-Learning-Course-2%2F</url>
    <content type="text"><![CDATA[1.1 训练，验证，测试集（Train / Dev / Test sets）在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助创建高效的神经网络。训练神经网络时，需要做出很多决策，例如： 神经网络分多少层 每层含有多少个隐藏单元 学习速率是多少 各层采用哪些激活函数 循环迭代的过程是这样的： 先有个想法Idea，先选择初始的参数值，构建神经网络模型结构 然后通过代码Code的形式，实现这个神经网络； 通过实验Experiment验证这些参数对应的神经网络的表现性能。 根据验证结果，对参数进行适当的调整优化，再进行下一次的Idea-&gt;Code-&gt;Experiment循环。通过很多次的循环，不断调整参数，选定最佳的参数值，从而让神经网络性能最优化 在机器学习发展的小数据量时代，常见做法是将所有数据三七分，就是的70%训练集，30%测试集，如果没有明确设置验证集，也可以按照60%训练，20%验证和20%测试集来划分 在大数据时代，数据量可能是百万级别，验证集和测试集占数据总量的比例会趋向于变得更小。因为验证集的目的就是验证不同的算法，检验哪种算法更有效，因此，验证集要足够大才能评估，比如2个甚至10个不同算法，并迅速判断出哪种算法更有效。可能不需要拿出20%的数据作为验证集 数据量过百万的应用，训练集可以占到99.5%，验证和测试集各占0.25%，或者验证集占0.4%，测试集占0.1% 确保验证集和测试集的数据来自同一分布 没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。所以如果只有验证集，没有测试集，要做的就是在训练集上训练，尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。因为验证集中已经涵盖测试集数据，其不再提供无偏性能评估 搭建训练验证集和测试集能够加速神经网络的集成，也可以更有效地衡量算法的偏差和方差，从而更高效地选择合适方法来优化算法 1.2 偏差，方差（Bias /Variance）在一个只有x_1和x_2两个特征的二维数据集中，可以绘制数据，将偏差和方差可视化。 在多维空间数据中，绘制数据和可视化分割边界无法实现，但可以通过几个指标，来研究偏差和方差 理解偏差和方差的两个关键数据是训练集误差（Train set error）和验证集误差（Dev set error） 假定训练集误差是1%，验证集误差是11%，可以看出训练集设置得非常好，而验证集设置相对较差，可能过度拟合了训练集，验证集并没有充分利用交叉验证集的作用，这种情况称之为“高方差”。 假设训练集误差是15%，验证集误差是16%，该案例中人的错误率几乎为0%，算法并没有在训练集中得到很好训练，如果训练数据的拟合度不高，就是数据欠拟合，这种算法偏差比较高。对于验证集产生的结果却是合理的，验证集中的错误率只比训练集的多了1%，这种算法偏差高，因为它甚至不能拟合训练集 训练集误差是15%，偏差相当高，验证集的评估结果更糟糕，错误率达到30%，这种算法偏差高，因为它在训练集上结果不理想，而且方差也很高，这是方差偏差都很糟糕的情况 训练集误差是0.5%，验证集误差是1%，猫咪分类器只有1%的错误率，偏差和方差都很低 以上分析都是基于假设预测的，训练集和验证集数据来自相同分布，假设人眼辨别的错误率接近0%，一般来说，最优误差也被称为贝叶斯误差，最优误差接近0%，如果最优误差或贝叶斯误差非常高，比如15%。再看看这个分类器（训练误差15%，验证误差16%），15%的错误率对训练集来说也是非常合理的，偏差不高，方差也非常低 偏差和方差都高： 这条曲线中间部分灵活性非常高，却过度拟合了这两个样本，这类分类器偏差很高，因为它几乎是线性的 采用曲线函数或二次元函数会产生高方差，因为曲线灵活性太高以致拟合了这两个错误样本和中间这些活跃数据。但对于高维数据，有些数据区域偏差高，有些数据区域方差高，所以在高维数据中采用这种分类器看起来就不会那么牵强 1.3 机器学习基础（Basic Recipe for Machine Learning）初始模型训练完成后，首先要知道算法的偏差高不高，如果偏差较高，试着评估训练集或训练数据的性能。如果偏差的确很高，甚至无法拟合训练集，要做的就是增加神经网络的隐藏层个数、神经元个数，训练时间延长，选择其它更复杂的NN模型等 如果网络足够大，通常可以很好的拟合训练集，一旦偏差降低到可以接受的数值，检查一下方差有没有问题，为了评估方差，要查看验证集性能，从一个性能理想的训练集推断出验证集的性能是否也理想，如果方差高，最好的解决办法就是增加训练样本数据，进行正则化Regularization，选择其他更复杂的NN模型 两点需要注意： 第一点，高偏差和高方差是两种不同的情况，通常用训练验证集来诊断算法是否存在偏差或方差问题，然后根据结果选择尝试部分方法。如果算法存在高偏差问题，准备更多训练数据没什么用处 第二点，在当前的深度学习和大数据时代，只要持续训练一个更大的网络，只要正则适度，通常构建一个更大的网络便可以在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。 这两步实际要做的工作是：使用更复杂的神经网络和海量的训练样本，一般能够同时有效减小Bias和Variance 1.4 正则化（Regularization）深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据 $\frac{\lambda}{2m}$乘以w范数的平方，欧几里德范数的平方等于w_j（ j值从1到n_x）平方的和，也可表示为ww^T，也就是向量参数w的欧几里德范数（2范数）的平方，此方法称为L2正则化。因为这里用了欧几里德法线，被称为向量参数w的L2范数。 J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||_2^2 ||w||_2^2=\sum_{j=1}^{n_x}w_j^2=w^Tw为什么不再加上参数b呢？因为通常w是一个高维参数矢量，几乎涵盖所有参数，已经可以表达高偏差问题，所以参数很大程度上由w决定，而b只是众多参数中的一个，改变b值对整体模型影响较小,所以通常省略不计,如果加了参数b，也没太大影响 $L2$正则化是最常见的正则化类型，L1正则化是正则项\frac{\lambda}{m}乘以\sum_{j=1}^{n^x}|w|，\sum_{j=1}^{n^x}|w|也被称为参数向量w的L1范数无论分母是，m还是2m，它都是一个比例常量 J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||_1 ||w||_1=\sum_{j=1}^{n_x}|w_j|如果用的是L1正则化，w最终会是稀疏的，也就是说w向量中有很多0，虽然L1正则化使模型变得稀疏，却没有降低太多存储内存,实际上L1 regularization在解决high variance方面比L2 regularization并不更具优势。而且，L1的在微分求导方面比较复杂 $\lambda$是正则化参数，可以设置\lambda为不同的值，在Dev set中进行验证，选择最佳的\lambda,通常使用验证集或交叉验证集来配置这个参数 在深度学习模型中，L2 regularization的表达式为： J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L||w^{[l]}||^2 ||w^{[l]}||^2=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^2$||w^{[l]}||^2$称为Frobenius范数，记为||w^{[l]}||_F^2。一个矩阵的Frobenius范数就是计算所有元素平方和再开方，如下所示： ||A||_F=\sqrt {\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2}由于加入了正则化项，梯度下降算法中的dw^{[l]}计算表达式需要做如下修改： dw^{[l]}=dw^{[l]}_{before}+\frac{\lambda}{m}w^{[l]} w^{[l]}:=w^{[l]}-\alpha\cdot dw^{[l]}L2 regularization也被称做weight decay。这是因为，由于加上了正则项，dw^{[l]}有个增量，在更新w^{[l]}的时候，会多减去这个增量，使得w^{[l]}比没有正则项的值要小一些。不断迭代更新，不断地减小 \begin{aligned}w^{[l]} &:=w^{[l]}-\alpha\cdot dw^{[l]}\\ &=w^{[l]}-\alpha\cdot(dw^{[l]}_{before}+\frac{\lambda}{m}w^{[l]})\\ &=(1-\alpha\frac{\lambda}{m})w^{[l]}-\alpha\cdot dw^{[l]}_{before} \end{aligned}其中，(1-\alpha\frac{\lambda}{m})]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第三周：浅层神经网络(Shallow neural networks)(Course 1)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%B8%89%E5%91%A8%EF%BC%9A%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Shallow-neural-networks%2F</url>
    <content type="text"><![CDATA[3.1 神经网络概述（Neural Network Overview） 3.2 神经网络的表示（Neural Network Representation ）单隐藏层神经网络就是典型的浅层（shallow）神经网络 单隐藏层神经网络也被称为两层神经网络（2 layer NN） 第l层的权重W^{[l]}维度的行等于l层神经元的个数，列等于l-1层神经元的个数；第i层常数项b^{[l]}维度的行等于l层神经元的个数，列始终为1 3.3 计算一个神经网络的输出（Computing a Neural Network’s output ）两层神经网络可以看成是逻辑回归再重复计算一次 逻辑回归的正向计算可以分解成计算z和a的两部分： z=w^Tx+b a=\sigma(z) 两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算 z^{[1]}=W^{[1]}x+b^{[1]} a^{[1]}=\sigma(z^{[1]}) z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} a^{[2]}=\sigma(z^{[2]}) 3.4 多样本向量化（Vectorizing across multiple examples ）for循环来求解其正向输出： for i = 1 to m: \begin{aligned}&z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]}\\&a^{[1](i)}=\sigma(z^{[1](i)})\\&z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]} \\&a^{[2](i)}=\sigma(z^{[2](i)})\end{aligned}矩阵运算的形式： Z^{[1]}=W^{[1]}X+b^{[1]} A^{[1]}=\sigma(Z^{[1]}) Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]} A^{[2]}=\sigma(Z^{[2]})行表示神经元个数，列表示样本数目m 3.5 激活函数（Activation functions） sigmoid函数 tanh函数 ReLU函数 Leaky ReLU函数 对于隐藏层的激活函数，tanh函数要比sigmoid函数表现更好一些。因为tanh函数的取值范围在[-1,+1]之间，隐藏层的输出被限定在[-1,+1]之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化（均值为0）的效果 对于输出层的激活函数，因为二分类问题的输出取值为\{0,+1\}，所以一般会选择sigmoid作为激活函数 选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点 Leaky$$ $$ReLU$$激活函数，能够保证$$z$$小于零时梯度不为$$03.6 为什么需要（ 非线性激活函数？（why need a nonlinear activation function?）假设所有的激活函数都是线性的，直接令激活函数g(z)=z，即a=z z^{[1]}=W^{[1]}x+b^{[1]} a^{[1]}=z^{[1]} z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} a^{[2]}=z^{[2]} a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}=(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})=W'x+b'多层隐藏层的神经网络，如果使用线性函数作为激活函数，最终的输出仍然是输入x的线性模型。这样的话神经网络就没有任何作用了。因此，隐藏层的激活函数必须要是非线性的 如果是预测问题而不是分类问题，输出y是连续的情况下，输出层的激活函数可以使用线性函数。如果输出y恒为正值，则也可以使用ReLU激活函数 3.7 激活函数的导数（Derivatives of activation functions ）$sigmoid$函数的导数： g(z)=\frac{1}{1+e^{(-z)}} g'(z)=\frac{d}{dz}g(z)=g(z)(1-g(z))=a(1-a)$tanh$函数的导数： g(z)=\frac{e^{(z)}-e^{(-z)}}{e^{(z)}+e^{(-z)}} g'(z)=\frac{d}{dz}g(z)=1-(g(z))^2=1-a^2$ReLU$函数的导数： g(z)=max(0,z) x = \begin{cases} 0 &\text{if } z < 0 \\ 1 &\text{if } z \geq 0 \end{cases}$Leaky ReLU$函数： g(z)=max(0.01z,z) g'(z) = \begin{cases} 0.01 &\text{if } z < 0 \\ 1 &\text{if } z \geq 0 \end{cases}3.8 神经网络的梯度下降（Gradient descent for neural networks） dZ^{[2]}=A^{[2]}-Y dW^{[2]}=\frac1mdZ^{[2]}A^{[1]T} db^{[2]}=\frac1mnp.sum(dZ^{[2]},axis=1,keepdim=True) dZ^{[1]}=W^{[2]T}dZ^{[2]}\ast g'(Z^{[1]}) dW^{[1]}=\frac1mdZ^{[1]}X^T db^{[1]}=\frac1mnp.sum(dZ^{[1]},axis=1,keepdim=True)3.9 （选修）直观理解反向传播（Backpropagation intuition ）单个训练样本反向过程可以根据梯度计算方法逐一推导： dz^{[2]}=a^{[2]}-y dW^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial W^{[2]}}=dz^{[2]}a^{[1]T} db^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial b^{[2]}}=dz^{[2]}\cdot 1=dz^{[2]} dz^{[1]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial a^{[1]}}\cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}=W^{[2]T}dz^{[2]}\ast g^{[1]'}(z^{[1]}) dW^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial W^{[1]}}=dz^{[1]}x^T db^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}=dz^{[1]}\cdot 1=dz^{[1]} 浅层神经网络（包含一个隐藏层），m个训练样本的正向传播过程和反向传播过程分别包含了6个表达式，其向量化矩阵形式如下图所示： 3.10 随机初始化（Random Initialization）神经网络模型中的参数权重W不能全部初始化为零 如果权重W^{[1]}和W^{[2]}都初始化为零，即： W^{[1]}= \left[ \begin{matrix} 0 & 0 \\ 0 & 0 \end{matrix} \right] W^{[2]}= \left[ \begin{matrix} 0 & 0 \end{matrix} \right]这样使得隐藏层第一个神经元的输出等于第二个神经元的输出，即a_1^{[1]}=a_2^{[1]}。经过推导得到dz_1^{[1]}=dz_2^{[1]}，dW_1^{[1]}=dW_2^{[1]}，这样的结果是隐藏层两个神经元对应的权重行向量W_1^{[1]}和W_2^{[1]}每次迭代更新都会得到完全相同的结果，W_1^{[1]}始终等于W_2^{[1]}，完全对称。这样隐藏层设置多个神经元就没有任何意义 权重W全部初始化为零带来的问题称为symmetry breaking problem 随机初始化： 1234W_1 = np.random.randn((2,2))*0.01b_1 = np.zero((2,1))W_2 = np.random.randn((1,2))*0.01b_2 = 0 让W比较小，是因为如果使用sigmoid函数或者tanh函数作为激活函数的话，W比较小，得到的|z|也比较小（靠近零点），而零点区域的梯度比较大，这样能大大提高梯度下降算法的更新速度，尽快找到全局最优解 如果W较大，得到的|z|也比较大，附近曲线平缓，梯度较小，训练过程会慢很多 如果激活函数是ReLU或者Leaky ReLU函数，则不需要考虑这个问题 如果输出层是sigmoid函数，则对应的权重W最好初始化到比较小的值]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning.ai深度学习课程笔记]]></title>
    <url>%2F2019%2F02%2F27%2FDeepLearning-ai%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[笔记中没有涵盖所有的视频内容，主要是我不懂或者觉得比较重要的内容，学生我水平有限，如笔记中有知识点、公式、代码错误，还烦请指出 Andrew Ng（吴恩达）的公开信： 朋友们， 我在做三个全新的AI项目。现在，我十分兴奋地宣布其中的第一个：deeplearning.ai，一个立志于扩散AI知识的项目。该项目在Coursera上发布了一系列深度学习课程，这些课程将帮助你掌握深度学习、对它高效地应用，并打造属于你自己的AI事业。 AI是新一轮电力革命 就像一百年前电力改造了每个主流行业，当今的AI技术在做着相同的事。好几个大型科技公司都设立了AI部门，用AI革新他们的业务。接下来的几年里，各个行业、规模大小各不相同的公司也都会意识到——-在由AI驱动的未来，他们必须成为其中的一份子。 创建由AI驱动的社会 我希望，我们可以建立一个由AI驱动的社会：让每个人看得起病，给每个孩子个性化的教育，让所有人都能坐上价格亲民的自动驾驶汽车，并向男人和女人提供有意义的工作。总而言之，是一个让每个人的生活变得更好的社会。 但是，任何一个公司都不可能单独完成这些任务。就像现在每一个计算机专业的毕业生都知道怎么用云，将来，每个程序员也必须懂得怎么用AI。用深度学习改善人类生活的方法有数百万种，社会也需要数百万个人——即来自世界各国的你们，来创造出了不起的AI系统。不管你是加州的一个软件工程师，一名中国的研究员，还是印度的ML工程师，我希望都能用深度学习来解决世界上的各种挑战。 你会学到什么 任何一个掌握了机器学习基础知识的人，都可以学习这五门系列课程，它们组成了Coursera的全新深度学习专业。 你会学到深度学习的基础，理解如何创建神经网络，学习怎么成功地领导机器学习项目。你会学习卷积神经网络、RNNs、LSTM、Adam、Dropout、BatchNorm、Xavier/He initialization以及更多。学习过程中，你会接触到医疗、自动驾驶、读手语、音乐生成、自然语言处理的案例。 你不仅会掌握深度学习理论，还会看到它是怎样在行业应用落地的。你会在Python和TensorFlow里试验这些想法，你还会听到各位深度学习领袖人物的意见，他们会分享各自的学习经历，并提供职业规划建议。 当你拿到Coursera的深度学习专业证书，就可以自信得把“深度学习”四个字写进你的简历。 加入我，建立一个由AI驱动的社会 从2011年到现在，已经有180万人加入了我的机器学习课程。当时，我和四名斯坦福的学生发布了这门课程，它随即成为了Coursera的第一门公开课。那之后，我受到你们之中许多人的启发——当我看到你们是如何努力地理解机器学习，开发优秀的AI系统，并开启令人惊艳的事业。 我希望深度学习专业能帮助你们实现更了不起的事，让你们为社会贡献更多，在职业道路上走得更远。 我希望大家和我一道，建立一个由AI驱动的社会。 我会通知大家另外两个项目的进展，并不断探索，为全世界AI社区的每一个人提供更多支持的途径。 Sincerely， 吴恩达 吴恩达与deeplearning.ai团队]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二周：神经网络的编程基础(Basics of Neural Network programming)(Course 1)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%BA%8C%E5%91%A8%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80-Basics-of-Neural-Network-programming%2F</url>
    <content type="text"><![CDATA[2.1 二分类(Binary Classification)逻辑回归模型一般用来解决二分类（Binary Classification）问题 二分类就是输出y只有{0,1}两个离散值（也有{-1,1}的情况） 彩色图片包含RGB三个通道。例如该cat图片的尺寸为（64，64，3） 在神经网络模型中，首先要将图片输入x（维度是（64，64，3））转化为一维的特征向量（featurevector）。方法是每个通道一行一行取，再连接起来。则转化后的输入特征向量维度为（12288，1）。此特征向量x是列向量，维度一般记为n_x 如果训练样本共有m张图片，那么整个训练样本X组成了矩阵，维度是（n_x,m), n_x代表了每个样本X^{(i)}特征个数，列m代表了样本个数,输出Y组成了一维的行向量，维度是（1，m） ) 2.2 逻辑回归(Logistic Regression)逻辑回归中，预测值\hat y=P(y=1 | x)表示为1的概率，取值范围在[0,1]之间 使用线性模型，引入参数w和b。权重w的维度是（n_x，1），b是一个常数项 逻辑回归的预测输出可以完整写成： \hat y = Sigmoid(w^Tx+b)=\sigma(w^Tx+b)Sigmoid函数的一阶导数可以用其自身表示： \sigma'(z)=\sigma(z)(1-\sigma(z)) 2.3 逻辑回归的代价函数（Logistic Regression Cost Function）单个样本的cost function用Loss function来表示，使用平方误差（squared error）： L(\hat y,y)=\frac12(\hat y-y)^2逻辑回归一般不使用平方误差来作为Loss function。原因是这种Loss function一般是non-convex的。 non-convex函数在使用梯度下降算法时，容易得到局部最小值（localminimum），即局部最优化。而最优化的目标是计算得到全局最优化（Global optimization），因此一般选择的Loss function应该是convex的 构建另外一种Loss function(针对单个样本)，且是convex的： L(\hat y,y)=-(ylog\ \hat y+(1-y)log\ (1-\hat y))当y=1时，L(\hat y,y)=-\log \hat y，如果\hat y越接近1，L(\hat y,y)\approx 0，表示预测效果越好；如果\hat y越接近0，L(\hat y,y)\approx +\infty，表示预测效果越差 当y=0时，L(\hat y,y)=-\log(1- \hat y)，如果\hat y越接近0，L(\hat y,y)\approx 0，表示预测效果越好；如果\hat y越接近1，L(\hat y,y)\approx +\infty，表示预测效果越差 Cost function是m个样本的Loss function的平均值，反映了m个样本的预测输出\hat y与真实样本输出y的平均接近程度: J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac1m\sum_{i=1}^m[y^{(i)}log\ \hat y^{(i)}+(1-y^{(i)})log\ (1-\hat y^{(i)})] 2.4 逻辑回归的梯度下降（Logistic Regression Gradient Descent）对单个样本而言，逻辑回归Loss function表达式如下： z=w^Tx+b \hat y=a=\sigma(z) L(a,y)=-(y\log(a)+(1-y)\log(1-a)) 计算该逻辑回归的反向传播过程: da=\frac{\partial L}{\partial a}=-\frac ya+\frac{1-y}{1-a} dz=\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a}\cdot \frac{\partial a}{\partial z}=(-\frac ya+\frac{1-y}{1-a})\cdot a(1-a)=a-y dw_1=\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w_1}=x_1\cdot dz=x_1(a-y) dw_2=\frac{\partial L}{\partial w_2}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w_2}=x_2\cdot dz=x_2(a-y) db=\frac{\partial L}{\partial b}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial b}=1\cdot dz=a-y则梯度下降算法可表示为： w_1:=w_1-\alpha\ dw_1 w_2:=w_2-\alpha\ dw_2 b:=b-\alpha\ db 2.5 梯度下降的例子(Gradient Descent on m Examples)m个样本的Cost function表达式如下： z^{(i)}=w^Tx^{(i)}+b \hat y^{(i)}=a^{(i)}=\sigma(z^{(i)}) J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac1m\sum_{i=1}^m[y^{(i)}log\ \hat y^{(i)}+(1-y^{(i)})log\ (1-\hat y^{(i)})]Cost function关于w和b的偏导数可以写成和平均的形式： dw_1=\frac1m\sum_{i=1}^mx_1^{(i)}(a^{(i)}-y^{(i)}) dw_2=\frac1m\sum_{i=1}^mx_2^{(i)}(a^{(i)}-y^{(i)}) dw_m=\frac1m\sum_{i=1}^mx_m^{(i)}(a^{(i)}-y^{(i)}) db=\frac1m\sum_{i=1}^m(a^{(i)}-y^{(i)})算法流程如下所示： 12345678910111213J=0; dw1=0; dw2=0; db=0;for i = 1 to mz(i) = wx(i)+b;a(i) = sigmoid(z(i));J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i));dz(i) = a(i)-y(i);dw1 += x1(i)dz(i);dw2 += x2(i)dz(i);db += dz(i);J /= m;dw1 /= m;dw2 /= m;db /= m; 经过每次迭代后，根据梯度下降算法，w和b都进行更新： w_1:=w_1-\alpha\ dw_1 w_2:=w_2-\alpha\ dw_2 w_m:=w_m-\alpha\ dw_m b:=b-\alpha\ db在深度学习中，样本数量m通常很大，使用for循环会让神经网络程序运行得很慢。应该尽量避免使用for循环操作，而使用矩阵运算，能够大大提高程序运行速度 2.6 向量化 logistic 回归的梯度输出（Vectorizing Logistic Regression’s Gradient Output）db可表示为： db=\frac1m \sum_{i=1}^mdz^{(i)}dw可表示为： dw=\frac1m X\cdot dZ^T单次迭代，梯度下降算法流程如下所示： 12345678Z = np.dot(w.T,X) + bA = sigmoid(Z)dZ = A-Ydw = 1/m*np.dot(X,dZ.T)db = 1/m*np.sum(dZ)w = w - alpha*dwb = b - alpha*db 2.7 （选修）logistic 损失函数的解释（Explanation of logistic regression cost function ）$\hat y$可以看成是预测输出为正类（+1）的概率： \hat y=P(y=1|x)当y=1时： p(y|x)=\hat y当y=0时： p(y|x)=1-\hat y整合到一个式子: P(y|x)=\hat y^y(1-\hat y)^{(1-y)}进行log处理： log\ P(y|x)=log\ \hat y^y(1-\hat y)^{(1-y)}=y\ log\ \hat y+(1-y)log(1-\hat y)上述概率P(y|x)越大越好，加上负号，则转化成了单个样本的Loss function，越小越好: L=-(y\ log\ \hat y+(1-y)log(1-\hat y))对于所有m个训练样本，假设样本之间是独立同分布的，总的概率越大越好： max\ \prod_{i=1}^m\ P(y^{(i)}|x^{(i)})引入log函数，加上负号，将上式转化为Cost function： J(w,b)=-\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac 1m\sum_{i=1}^m[y^{(i)}\ log\ \hat y^{(i)}+(1-y^{(i)})log(1-\hat y^{(i)})]]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一周：深度学习引言(Introduction to Deep Learning)(Course 1)]]></title>
    <url>%2F2019%2F02%2F27%2F%E7%AC%AC%E4%B8%80%E9%97%A8%E8%AF%BE-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Neural-Networks-and-Deep-Learning%2F</url>
    <content type="text"><![CDATA[1.1 神经网络的监督学习(Supervised Learning with Neural Networks) 一般的监督式学习（房价预测和线上广告问题），只要使用标准的神经网络模型就可以图像识别处理问题，则要使用卷积神经网络（Convolution Neural Network），即CNN 处理类似语音这样的序列信号时，则要使用循环神经网络（Recurrent Neural Network），即RNN 自动驾驶这样的复杂问题则需要更加复杂的混合神经网络模型 CNN一般处理图像问题，RNN一般处理语音信号 数据类型一般分为两种：Structured Data和Unstructured Data Structured Data通常指的是有实际意义的数据，例如房价预测中的size，#bedrooms，price等；例如在线广告中的User Age，Ad ID等 Unstructured Data通常指的是比较抽象的数据，例如Audio，Image或者Text 1.2Why is Deep Learning taking off？ 红色曲线代表了传统机器学习算法的表现，例如是SVM，logistic regression，decision tree等。当数据量比较小的时候，传统学习模型的表现是比较好的。当数据量很大的时候，其性能基本趋于水平 构建一个深度学习的流程是首先产生Idea，然后将Idea转化为Code，最后进行Experiment。接着根据结果修改Idea，继续这种Idea-&gt;Code-&gt;Experiment的循环，直到最终训练得到表现不错的深度学习网络模型 ![]]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning]]></title>
    <url>%2F2019%2F02%2F23%2Fmachine%20learning%2F</url>
    <content type="text"><![CDATA[A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience EA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
